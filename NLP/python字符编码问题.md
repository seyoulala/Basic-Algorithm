

关于字符编码的概念太多太杂，当**ASCII、GB2312、Unicode、UTF-8、UTF-16、编码、解码**等诸多名词一股脑堆上来时，确实容易让人迷糊。

为了把这些问题讲清楚，我们先换一种讲法，**不讲编程，只讲故事**，从历史维度，在时间轴上梳理计算机在不同语言国家不断发展的过程，以此来彻底搞清楚这些概念。

**字符编码与解码是什么？**

计算机自己能理解的“语言”是二进制数，最小的信息标识是二进制位，8个二进制位表示一个字节；而我们人类所能理解的语言文字则是一套由英文字母、汉语汉字、标点符号字符、阿拉伯数字等等很多的字符构成的**字符集**。如果要让计算机来按照人类的意愿进行工作，则必须把人类所使用的这些字符集转换为计算机所能理解的二级制码，这个过程就是**编码**，他的逆过程称为**解码。**

**ASCII到Unicode的演进过程**

计算机最开始在美国被发明使用，需要编码的**字符集**并不是很大，无外乎英文字母、数字和一些简单的标点符号，因此采用了一种单字节编码系统。在这套编码规则中，人们将所需字符集中的字符一一映射到128个二进制数上，这128个二进制数最高位为0，利用剩余低7位组成00000000~01111111（0X00~0X7F）。

0X00到0X1F这32个二进制数来对控制字符或通信专用字符（如LF换行、DEL删除、BS退格）进行编码，0X20到0X7F共96个二进制数用来对阿拉伯数字、英文字母大小写和下划线、括号等符号进行编码。**将这套字符集映射到0X00~0X7F二进制码的过程就称为基础ASCII编码**，通过这个编码过程，计算机就将人类的语言转化为自己的语言存储起来；反之，从磁盘中读取二级制数并转化为字母数字等字符以供显示的过程就是**解码**了。

随着计算机被迅速推广使用，欧洲非英语国家的人们发现这套由美国人设计的字符集不够用了，比如一些带重音的字符、希腊字母等都不在这个字符集中**，于是扩充了ASCII编码规则，将原本为0的最高位改为1，因此扩展出了10000000~11111111（0X80~0XFF）这128个二进制数。**这其中，最优秀的扩展方案是ISO 8859-1，通常称之为**Latin-1**。Latin-1利用128~255这128个二进制数，包括了足够的附加字符集来涵盖基本的西欧语言，同时在0~127的范围内兼容ASCII编码规则。

随着使用计算机的国家越来越多，自然而然需要编码的字符集就越来越庞大，早先的ASCII编码字符集由于受到单字节的限制，其容量就远远不够了，比方说面对成千上万的汉字，其压力可想而知。**因此中国国家标准总局发布了一套《信息交换用汉字编码字符集》的国家标准，其标准号就是[GB 2312](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/GB%202312)—1980。**这个字符集共收入汉字6763个和非汉字图形字符682个，采用两个字节对字符集进行编码，并向下兼容ASCII编码方式。简言之，整个字符集分成94个区，每区有94个位，分别用一个字节对应表示相应的区和位。每个区位对应一个字符，因此可用所在的区和位来对汉字进行两字节编码。**再后来生僻字、繁体字及日韩汉字也被纳入字符集，就又有了后来的GBK字符集及相应的编码规范，GBK编码规范也是向下兼容GBK2312的。**

在中国发展的同时，计算机在全世界各个国家不断普及，不同的国家地区都会开发出自己的一套编码系统，因此编码系统五花八门，这时候问题就开始凸显了，特别是在互联网通信的大环境下，装有不同编码系统的计算机之间通信就会彼此不知道对方在“说”些什么，按照A编码系统的编码方式将所需字符转换成二进制码后，在B编码系统的计算机上解码是无法得到原始字符的，相反会出现一些出人意料的古怪字符，这就是所谓的**乱码。**

那么统一字符编码的需求就迫切摆在了大家眼前，**为了实现跨语言、跨平台的文本转换和处理需求，ISO国际标准化组织提出了Unicode的新标准**，这套标准中包含了Unicode字符集和一套编码规范。**Unicode字符集涵盖了世界上所有的文字和符号字符，Unicode编码方案为字符集中的每一个字符指定了统一且唯一的二进制编码**，这就能彻底解决之前不同编码系统的冲突和乱码问题。这套编码方案简单来说是这样的：编码规范中含有17个组（称为平面），每一个组含有65536个码位（例如组0就是0X0000~0XFFFF），每一个码位就唯一对应一个字符，大部分的字符都位于字符集平面0的码位中，少量位于其他平面。

**字符编码和字符代码的概念区分**

**既然提到了Unicode编码，那么常常与之相伴的UTF-8，UTF-16编码方案又是什么？**其实到目前为止我们都一直混淆了两个概念，即**字符代码**和**字符编码**，字符代码是特定字符在某个字符集中的序号，而字符编码是在传输、存储过程当中用于表示字符的以字节为单位的二进制序列。ASCII编码系统中，字符代码和字符编码是一致的，比如字符A，在ASCII字符集中的序号，也就是所谓的字符代码是65，存储在磁盘中的二进制比特序列是01000001（0X41，十进制也是65），另外的，如在GB2312编码系统中字符代码和字符编码的值也是一致的，所以无形之中我们就忽略了二者的差异性。

而在Unicode标准中，我们目前使用的是UCS-4，即字符集中每一个字符的字符代码都是用4个字节来表示的，其中字符代码0~127兼容ASCII字符集，一般的通用汉字的字符代码也都集中在65535之前，使用大于65535的字符代码，即需要超过两个字节来表示的字符代码是比较少的。因此，如果仍然依旧采用字符代码和字符编码相一致的编码方式，那么英语字母、数字原本仅需一个字节编码，目前就需要4个字节进行编码，汉字原本仅需两个字节进行编码，目前也需要4个字节进行编码，这对于存储或传输资源而言是很不划算的。

**因此就需要在字符代码和字符编码间进行再编码，这样就引出了UTF-8、UTF-16等编码方式**。基于上述需求，UTF-8就是针对位于不同范围的字符代码转化成不同长度的字符编码，同时这种编码方式是以字节为单位，并且完全兼容ASCII编码，即0X00-0X7F的字符代码和字符编码完全一致，也是用一个字节来编码ASCII字符集，而常用汉字在Unicode中的字符代码是4E00-9FA5，在**文末的对应关系**中我们看到是用三个字节来进行汉字字符的编码。UTF-16同理，就是以16位二进制数为基本单位对Unicode字符集中的字符代码进行再编码，原理和UTF-8一致。

**因此，我们可以看出，在目前全球互联的大背景下，Unicode字符集和编码方式解决了跨语言、跨平台的交流问题，同时UTF-8等编码方式又有效的节约了存储空间和传输带宽，因而受到了极大的推广应用。**

**讲清楚了字符编码的发展历史，容易让人明白为什么某一时刻就出现了某种编码方式，搞清楚了来龙去脉，对字符编码就好理解了。**

## 附表：

Unicode字符代码与UTF-8编码的对应关系

—————————————————————–

```python3
0000 0000-0000 007F | 0xxxxxxx
0000 0080-0000 07FF | 110xxxxx 10xxxxxx
0000 0800-0000 FFFF | 1110xxxx 10xxxxxx 10xxxxxx
0001 0000-0010 FFFF | 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx
```

讲清楚字符编码的基础概念后，我相信再来介绍python中的字符编码就会容易的多。

通过上一集我们知道，ASCII码（包括其最常见的超集Latin-1）依赖这样的一个假设，即每一个字符与一个字节相匹配，由于存在太多的字符，因此不可避免的会出现问题，Unicode字符集通过使用4个字节来表示1个字符，则解决了该问题。

**首先来介绍一下Python中的两种字符串：**

**Python中有两种字符串：文本字符串和字节字符串。**其中文本字符串类型被命名为str，内部采用Unicode字符集（兼容ASCII码），而字节字符串则直接用来表示原始的字节序列（用print函数来打印字节字符串时，若字节在ascii码范围内，则显示为ascii码对应的字符，其余的则直接显示为16进制数），该类型被命名为bytes。

看一个简单的例子：

```python3
s = 'apple'
b = b'apple'
print(b)
print(type(b))
print(s)
print(type(s))

b'apple'
<class 'bytes'>
apple
<class 'str'>
```

**再近距离的看看bytes类型字节字符串，本质上它就是一串单字节16进制数**

```python3
b = b'apple'
print(b[0])
print(b[1:])
print(list(b))

97
b'pple'
[97, 112, 112, 108, 101]
```

**那这和编码、解码有何关联呢？**

**从本质上来说，编码和解码就是str和bytes这两种字符串类型之间的互相转换。**

str包含一个encode方法，使用特定编码将该字符串其转换为一个bytes，**这称之为编码**。bytes类包含了一个decode方法，也接受一个编码作为单个必要参数，并返回一个str，**这称之为解码。**这种转换操作是显式的操作，且必须根据数据被编码时采用的编码类型进行解码。

**首先说说编码**，即将unicode的str文本字符串转换为bytes的字节字符串，可以显式的传入指定编码（一般来说采用utf-8编码），或使用平台的默认编码。

```python3
s = 'π排球の'
b1 = s.encode('utf-8')
b2 = s.encode()
print(b1)
print(b2)

b'\xcf\x80\xe6\x8e\x92\xe7\x90\x83\xe3\x81\xae'
b'\xcf\x80\xe6\x8e\x92\xe7\x90\x83\xe3\x81\xae'
```

那么我们看看，在不写编码的时候，平台默认的编码方式到底是什么

```python3
import sys
print(sys.platform)
print(sys.getdefaultencoding())

win32
utf-8
```

可以看出我这个平台默认选择的是utf-8编码方式。

**接下来我们来比较一下unicode、latin-1、ASCII编码方式的兼容性问题:**

**首先，非ASCII字符无法使用ASCII编码转换成字节字符串**

```python3
s = 'π排球の'
b = s.encode('ascii')

Traceback (most recent call last):
 File "E:/12homework/12homework.py", line 2, in <module>
   b = s.encode('ascii')
UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-3:
 ordinal not in range(128)
```

**其次，Latin-1和unicode编码方式不兼容。**

例如，重音字符会在latin-1字符集和unicode字符集中同时存在，但是通过latin-1和unicode编码方式编出来的字节流是不一样的，注意，虽然unicode字符集是包含了latin-1字符集，但是不代表utf-8编码方式兼容latin-1编码方式。因为unicode字符集中除了ascii字符集外，都是采用多字节的编码方式，而latin-1一律采用单字节的方式

```python3
s = 'Äè'
print(s.encode('utf-8'))
print(s.encode('latin-1'))

b'\xc3\x84\xc3\xa8'
b'\xc4\xe8'
```

**只有ascii字符集中的字符，三种编码方式得到的结果才完全一致。**对unicode进行编码的时候，针对常规的7位ASCII文本，由于utf-8以及latin-1编码方式都是兼容ASCII的，所以结果都是一样的。

```python3
s = 'abc'
print(s.encode('utf-8'))
print(s.encode('latin-1'))
print(s.encode('ascii'))

b'abc'
b'abc'
b'abc'
```

**那对应的，再来谈谈decode解码方法吧。**

将bytes类型字符串转换成str类型的unicode文本字符串也是一样，要么指定编码参数，要么使用平台的默认参数。这个例子中，我们要操作的字节字符串b是通过utf-8编码方式对文本字符串'π排球の'编码而形成的。

```python3
b = b'\xe6\x8e\x92\xe7\x90\x83'
s1 = b.decode(encoding='utf-8')
s2 = b.decode()
s3 = b.decode(encoding='latin-1')

print(s1)
print(s2)
print(s3)

排球
排球
æŽ’çƒ
```

值得注意的是，**最后一行代码想通过latin-1解码字节字符串，由于字节字符串是通过utf-8编码形成，因此这样解码形成得到的只能是乱码。**

Utf-8编码是用两个字节来表示非ASCII的高128字符，而latin-1则是用一个字节来一一对应

**计算机用二进制来存储信息，而却能在各种应用中显示我们需要的文字，这应该是字符编、解码的应用吧。**

**很对，下面我们来说说文本文件读取时的编、解码问题**

当一个文件以文本模式打开的时候，被读取的二进制存储数据（也就是存储的字节字符串）会自动被解码（依据显式提供的编码名称或平台默认的编码名称），并且将其返回为一个str。写入文件时，会接受一个str，并且将其传输到文件之前自动编码成字节字符串以供磁盘存储。

当一个文件以二进制模式打开时，需要在open方法的模式字符串参数里添加一个b，此时读取的数据不会以任何方式解码，而是直接返回其原始内容，即一个bytes对象；写入文件时，接受一个bytes对象，并且将其传送到文件中且不进行修改。

在读取文本文件的时候，如果open函数没有声明他们如何编码，python3会因其所运行的系统而选取默认的编码方式，默认情况下，python3 期望文件使用 utf-8进行编码。但由于文件并不总是在同一个系统中被保存和打开，因此会带来乱码的风险，所以我们需要显式的指定编码。

补充的说明一下，可以很简单的进行一个分类：处理图像文件、设备数据流等，可以使用bytes和二进制模式文件处理；而如果要处理的内容实质是文本的内容，例如程序输出、HTML、国际化文本或CSV或XML文件，则可能要使用str和文本模式文件

**例如，我们先把AÄBèC用UTF-8编码后存入utf-8data文件，再来读取他，具体看看这里是如何实现的。**

```python3
s = 'AÄBèC'

with open('utf-8data','w',encoding='utf-8') as f:
   f.write(s)

with open('utf-8data','r',encoding='utf-8') as f:
   u_str = f.read()
print(u_str)


AÄBèC
```

这里用到的文件读写的方法后面的章节会详细介绍，现在知道他是什么就好了。

**以二进制的形式读取文件。**

还有一种我们之前介绍过的用法，文本字符串在存储到磁盘的时候会编码成字节字符，因此我们也可以先以字节字符串的形式从文件中将其读取，然后再进行解码。

这样做的原因有二，一种是所接收的可能是非文本数据，如一个图像文件；另一个潜在原因是无法确定所读取文本文件的编码，可能需要依据其他信息再确定：

```python3
with open('utf-8data', 'rb') as f:
   byte_str = f.read()

print(byte_str)
print(byte_str.decode(encoding='utf-8'))

b'A\xc3\x84B\xc3\xa8C'
AÄBèC
```