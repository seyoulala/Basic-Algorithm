### mask language model 的一些问题



**为什么要使用mask？**

首先我们思考一下使用language model去学习一个蕴含上下文信息的context vector 是否是最合适的。语言模型其实是在学习一个联合概率，比如一句话出现的概率通过联合概率可以表示为$p(x_1,x_2,..,x_n)$，语言模型的目标是希望最大化这个联合概率,将联合概率通过chain rule我们可以将其分解成条件概率的乘积$p(x_1,x_2,...,x_n)=p(x_1|<sos>).p(x_2|x_1).p(x_3|x_1,x_2)..p(x_n|x_1,..x_n-1)$

通过上述式子我们可以看到，语言模型是一个单向的模型，天然不太适合这种需要建模双向信息的场景。那么怎么办呢？怎么引入双向的信息？ELMO通过构建bidirectional LSTM模型来引入双向信息，但是LSTM本质也是一个单向的语言模型，两个反方向的LSTM模型是独立训练的，还是无法满足同时看到上下文信息。同时这种双向的方式可能会存在一定的信息泄露问题，如下图

<img src="/Users/eason/Library/Application%20Support/typora-user-images/image-20210325151645494.png" alt="image-20210325151645494" style="zoom:50%;" />

我们在使用双向LSTM的时候，在w1位置的隐藏状态是包含正向的隐藏向量h_forward，同时还包含反向的隐藏向量h_backward的。h_backward中是已经编码了w2的信息的。在w1这个单词所在时刻我们希望我们网络的输出是w2，因此我们在预测w2的时候其实已经使用了一部分w2的信息了。可以认为信息有点泄露咯。

那么如何处理上述这个问题？**通过mask language model**

<img src="/Users/eason/Library/Application%20Support/typora-user-images/image-20210325104526387.png" alt="image-20210325104526387" style="zoom:50%;" />

比如上图，我们将天气mask掉，然后我们的目标是使用天气的周围的单词输入模型，希望模型能够输出天气，通过这种方式我们能够一定程度上解决上述问题。

**mask language model 存在的一些问题**

在mask language model 的pretraning过程中，我们需要将一些词mask掉，然后使用周围的词来对其预测。

但是仅仅把一个词替换成一个特征的mask符合存在一些问题。首先第一个问题就是我们在最后计算loss的时候只会针对mask掉的单词进行loss的计算，这可能会使得文本中其它非mask单词无法很好的学习，因为我们通过mask单词周围的单词来预测mask掉的词是什么等同于一个忘形填空。第二，也就是训练和测试的数据分布不一致问题。在训练的时候我们将一些词进行mask掉了但是实际上测试数据中并不会存在mask单词。同时在pre-trainning中被mask掉的真实单词可能会存在学习不充分的情况。

**那么如何解决这种问题呢?**

mask language model 本质上是属于autoencoder模型，我们将输入喂给模型，然后经过一系列网络层的非线性变换，到最后输出的时候我们希望输出能够接近我们的输入，学习的目标也就是最小化重构误差。它和一般的语言模型不同，比如N元语言模型。N元语言模型第 i 个字的概率和它前 i-1 个字有关，也就是要预测第 i 个字，那么模型就得先从头到尾依次预测出第1个到第 i-1 个字，再来预测 第 i 个字；这样的模型一般称为自回归模型（Autoregressive LM）

同时在autoencoder模型中，我们通常会对输入添加一系列的噪声来增加模型学习的难度，实践证明这样学出来的模型会更加robust。在Bert 的 MASK 机制是这样的：它以 token 为单位随机选择句子中 15% 的 token，然后将其中 80% 的 token 使用 [MASK] 符号进行替换，将 10% 使用随机的其他 token 进行替换，剩下的 10% 保持不变。

