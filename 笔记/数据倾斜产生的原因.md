


### 数据倾斜产生的原因

> 数据的key分布不平衡，有部分key的数据非常多，其它key的数据量相对少的情况，这样就会造成某个节点处理的数据量很多，就有可能出现job失败或者阻塞的情况

1. 数据倾斜和业务逻辑以及数据量有关

   本质上还是单台节点在执行数据量很大那部分reduce任务，由于数据量太大了，机器跑不动了。

2. 业务逻辑造成的数据倾斜的原因

   > 分组 注：group by 优于distinct group
   > 情形：group by 维度过小，某值的数量过多
   > 后果：处理某值的reduce非常耗时
   > 去重 distinct count(distinct xx)
   > 情形：某特殊值过多
   > 后果：处理此特殊值的reduce耗时
   > 连接 join
   > 情形1：其中一个表较小，但是key集中
   > 后果1：分发到某一个或几个Reduce上的数据远高于平均值
   > 情形2：大表与大表，但是分桶的判断字段0值或空值过多
   > 后果2：这些空值都由一个reduce处理，非常慢
   >

## 如何处理数据倾斜

​	1.调优参数

> 1. 在加个combiner函数，加上combiner相当于提前进行reduce,就会把一个mapper中的相同key进行了聚合，减少shuffle过程中数据量，以及reduce端的计算量。这种方法可以有效的缓解数据倾斜问题，但是如果导致数据倾斜的key 大量分布在不同的mapper的时候，这种方法就不是很有效了。
>
> 2. 局部聚合加全局聚合。第二种方法进行两次mapreduce，第一次在map阶段对那些导致了数据倾斜的key 加上1-n的随机前缀，这样之前相同的key 也会被分到不同的reduce中，进行聚合，这样的话就有那些倾斜的key进行局部聚合，数量就会大大降低。然后再进行第二次mapreduce这样的话就去掉随机前缀，进行全局聚合。这样就可以有效地降低mapreduce了。不过进行两次mapreduce，性能稍微比一次的差些。



**spark中数据倾斜的处理**

> spark中，同一个stage中不同partition之间是可以并行处理的，而具有依赖关系的stage则是串行处理的，假设某个job有两个stage，stage0和stage1，stage1依赖于stage0，stage0之间假设有N个task，N-1个task处理都很快，但是某个task处理非常慢，那么这个task就会拖累这个stage。一旦你机器资源不够了可能任务就崩了。由于同一个stage中rdd中task处理的任务都是一样的，不同task之间耗时的差异主要由该task处理的数据量决定。
>
> 而stage的数据来源可以分为两类：
>
> 1. 直接从数据源读取，比如直接读hdfs上的文件
> 2. 读上个stage的shuffle数据
>
> 所以要避免数据倾斜就可以从这两方面入手
>
> **1. 首先从数据源入手**
>
> 如果读取的文件是不可切分的，可以将这个文件变为可切分文件，使得每个文件的数据量大致保持相同
>
> 
>
> **2调整并行度，分散同一个task中的不同key**
>
> spark在做shuffle时，默认使用hashpartition对数据进行partition，如果分区数设置的太小了，可能会使得某个task处理过多的数据，这时候可以调整partition的大小。比如大量不同的key被分配到了相同的task造成了某个task数据量过大。
>
> **3.为数据量特别大的key增加随机的前缀以及后缀**
>
> 通过为数据量特别大的key来增加随机的前缀/后缀，使得原来key相同的数据变为key不相同的数据，从而使倾斜的数据集分散到不同的task中，













3. hadoop和spark都是并行计算的，那么区别是什么？

>  两者都是用mr模型来进行并行计算，hadoop的一个作业称为job，job里面分为map task 和reduce task，每个task都是在自己的进程中运行的，当task结束时，进程也会结束。

>  spark用户提交的任务称为application，一个application对应一个sparkcontext，application中有多个job，这些job可以并行或串行执行，每个job中有多个stage，stage是shuffle过程中通过RDD之间的依赖关系划分job而来的，每个stage里面有多个task，组成taskset有TaskSchaduler分发到各个executor中执行，executor的生命周期是和app一样的，即使没有job运行也是存在的，所以task可以快速启动读取内存进行计算



**spark相对于hadoop的优势**

> hadoop缺点：
>
> 1. 表达能力有限，计算都必须要转换为map和reduce两个操作，但是这并不适合所有的情况，难以描述复杂的数据处理流程
> 2. I/O开销大，每次执行时都需要从磁盘中读取数据，并且在计算完成后需要将中间结果写入到磁盘中
> 3. 延迟高，一次计算可能将job分解为一系列的mapreduce任务，下一个任务必须在前一个任务完成后才能开始。
>
> spark优势：
>
> 1. spark计算模式也是属于MapReduce，但不局限于map和reduce两种操作，还有其他操作类型。
> 2. spark提供基于内存的计算，中间结果直接放在内存中，更适用用一些对迭代需求比较高的场景
> 3. spark基于DAG的任务调度执行机制，要优于MR的迭代执行机制



**spark的一些基本概念**

> RDD:弹性分布式数据集，是分布式内存的一种抽象概念
>
> DAG：反应RDD之间的依赖关系
>
> Executor：运行在工作节点上的一个进程，负责运行任务，并为应用程序存储数据
>
> 任务(task)：运行在exector上的工作单元
>
> 作业(job)：一个作业包含多个RDD及作用于RDD上的各种操作
>
> 阶段：是作业的基本调度单位，一个job会被分为多个stage

一个application就是一个程序代码，application会被分为多个job，划分的标准是：是否要得到一个scala的集合或者对象。

**spark为什么不需要讲中间结果多次写入hdfs上面**

> worker中的executor中有一个bolckmanager存储模块，类似于键值存储系统(把内存和磁盘共同作为存储设备)，在处理迭代任务时，不需要把中间结果存到hdfs上面，而是直接放在这歌存储模块中，后续有需要时就可以直接读取。

**Rdd之间的依赖关系**

> RDD中的依赖分为宽依赖和窄依赖，窄依赖表现为一个父Rdd的分区对应于一个子Rdd的分区，或多个父RDD的分区对应一个子RDD的分区。宽依赖则表现为一个父RDD的一个分区对应一个子rdd分区或者多个RDD分区。总体而言只要一个父RDD的分区只被一个子RDD分区使用，那么就是窄依赖，否则就是宽依赖。窄依赖典型的操作包括map，filter，union等操作，宽依赖则包括groupbykey，sortbykey。
>
> ![](http://dblab.xmu.edu.cn/blog/wp-content/uploads/2016/11/%E5%9B%BE9-10-%E7%AA%84%E4%BE%9D%E8%B5%96%E4%B8%8E%E5%AE%BD%E4%BE%9D%E8%B5%96%E7%9A%84%E5%8C%BA%E5%88%AB.jpg)



**spark依赖设计的好处**

> spark这种依赖设计，使得其容错性很好，加快了spark的执行速度。因为，RDD数据集通过血缘关系记住了某个rdd是如何通过其它rdd转换来的，当某个rdd部分分区数据丢失的时候，它可以通过血缘关系获取足够的信息来重新恢复丢失的数据分区。

**阶段的划分**

>  spark通过分析rdd之间的依赖来生成DAG，然后通过Dagscheduler来解析rdd之间的依赖关系来决定如何划分stage，具体的划分stage的方式是：在DAG中进行反向解析，遇到宽依赖那么就断开，遇到窄依赖就把当前的rdd加入到当前的stage中，尽量将窄依赖都划分到一个stage中，可以实现流水线计算。
>
> ![](http://dblab.xmu.edu.cn/blog/wp-content/uploads/2016/11/%E5%9B%BE9-11-%E6%A0%B9%E6%8D%AERDD%E5%88%86%E5%8C%BA%E7%9A%84%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB%E5%88%92%E5%88%86%E9%98%B6%E6%AE%B5.jpg)
>
> 把一个DAG图划分为多个stage以后，每个stage都代表了一组没有关联的、相互之间没有shuffle依赖关系的任务集合。每个任务集合都会被提交给任务调度器进行执行，由任务调度器将任务分发给executor运行。

![](https://img-blog.csdn.net/20170206094636482?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYTEwNDM0OTg3NzY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

在spark中，task类型分为两种，shufflemaptask以及resulttask，每个stage里面task数量是由这阶段最后一个rdd中的partition数量决定的。图2中的stage1和stage2相当于mapreduce中的mapper，resulttask所代表的stag3相当于mapreduce中的reduce



