# BN为什么效果好

## 1. 什么是BN

batch normalization 就是批规范化，在每次的SGD之后，对mini-batch来对相应的activation做规范化操作。使得输出信号的各个维度均值为0，方差为1.然后通过缩放和转换操作，将BN后的输出能还原到最初的输入.

![](https://pic2.zhimg.com/80/9ad70be49c408d464c71b8e9a006d141_hd.jpg)

## 2.为什么要使用BN

BN说到底还是为了防止**梯度的弥散**

在BN中，通过将activation规范为均值和方差一致的手段使得原来会减小的activation的scale变大。

## 3. 什么时候使用BN

在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决，一般情况下也可以加入BN来加快训练速度，提高模型精度。

## 4. 优势

- 可以使用更高的学习率，使用BN后，BN使得每一层每一个维度的scale都保持一致，这样就可以使用更大的学习率了，这样就可以加快网络的训练速度
- 可以缓解梯度弥散。比如说使用sigmoid时候，如果input太大或者太小的话，这时候梯度就会很小。但是通过batchnormation后，使得网络的输入在0附近，那么此时x处的梯度也就不会落在梯度饱和的区域，有利于缓解梯度弥散。
- 对初始化的权重不太敏感，比如你对W乘上K倍，但是在normation的时候，对于activation的input是没有影响的。