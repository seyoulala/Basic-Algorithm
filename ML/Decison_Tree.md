### Decision Tree


决策树算法简介：

决策树是一种基本的分类和回归算法，对于分类问题，是根据对特征的划分对样本分类的过程，学习时利用训练数据，**根据最小化损失函数为学习策略**来建立树模型，预测时对新的数据利用以及生成的数来进行预测。决策树的学习算法可分为**特征的选择**，**决策树的生成**以及**树的剪枝**。


1. 决策树学习为什么使用启发式方法？
答： 决策树的本质是从训练数据中学习一些了if-then的规则。从而达到将训练数据正确分类的目的；但是能将数据正确分类的规则有很多种，我们需要的是一种拟合能力强但是泛化能力也强的模型，当我们的损失函数确定以后，我们就需要从这些规则里选择最优的一种。但是这很难选择，是一个NP难问题，所以在实际情况下通过启发式的搜索来近似求解。

2. 讲讲**特征选择***的过程
 答：对于ID3来说，使用信息增益来选择特征。具体步骤：开始，将所有训练的数据都放在根节点，依次选择一个特征，对于离散型特征，对其每一个取值都划分一个分支，对于连续值，可以直接取中位点作为划分点将其分开，大于切分点的分到右边，小于切分点的分到左边。每一次划分后都计算信息增益，选择划分后信息增益最大的特征作为划分点。若划分后，一些子集已经分类正确了，那么就标记为叶子节点，没能完全分开的子集继续按上述方法继续递归划分，直到子集中实例能被正确分开，或者划分的特征都用完了为止，最后每一个样本都落到了叶子节点上了。

3. 讲讲**熵**是啥
答：熵是表示随机变量不确定性的度量。熵越大 随机变量的不确定性越大。其公式如下：
$$ P(X=x_i) =p_i, i=1,2,3,...n$$

$$
H(X)=\sum_{i=1}^n {p_ilogp_i}
$$

P(X)是随机变量X的概率分布。

4. 什么是**互信息**
答：表示得知特征X的信息而使得类Y的信息不确定性减少的程度。

5. **信息增益**（ID3）
集合D的经验熵与给定特征A条件下的条件熵之差。$ H(D)$表示对数据集D进行分类的不确定性，$H(D|A)$表示在给定特征A条件下对数据集D 分类的不确定性。那么信息增益就表示由于特征A使得对集合D分类不确定性减少的程度。 

6. **信息增益比**
信息增益比定义为信息增益与集合的经验熵之比
$$
g_R= \frac{g(D,A)}{H(D)}
$$

7. **C4.5算法**

8. 什么是剪枝
答：剪枝是将生成的树根据损失函数简化的过程。具体是从已生成的树上减掉一些叶子节点或者子树，回退到父节点，将父节点作为新的叶子节点。

## 2. CART
CART（分类回归树）既可以用与分类也可以用于回归。对回归树用平方损失最小原则，对分类树用基尼指数最小化准则。

#### 最小二乘回归树算法

输入：训练数据集D
输出： 回归树$f(x)$
采用启发式方法，找到一个变量J的最优切分点，遍历所有变量。找到了最优的变量和最优的切分点。将输入空间划分为两个空间，然后对两个空间递归。
（1）选择最优切分变量$j$与切分点$s$,求解
$$
\min_{j,s} \left[\min_{c1}\sum_{xi\in R_1(j,s)}(y_i-c_1)^2+\min_{c2}\sum_{xi\in R_2(j,s)}(y_i-c_2)^2 \right]   (1)
$$

 遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，选择使（1）达到最小的对（j,s）
 
 （2）用选定的对（j,s）划分区域并决定相应的输出值：
 
$$
R_1(j,s) =\lbrace x|x^{j} \leq s \rbrace,  R_2(j,s) =\lbrace x|x^{j} \gt s \rbrace
$$

公式：
$$
\hat{c}_m = \frac{1}{N_m}\sum_{xi\in R_m(j,s)}y_i , x \in R_m,m=1,2
$$

(3) 继续对两个子区域调用步骤（1），（2）直达满足停止条件
(4) 将输入空间划分为M个区域$R_1, R_2,....R_m$,生成决策树：

$$
f(x) = \sum_{m=1}^M{\hat{c}_mI(x \in R_m)}
$$



