### Decision Tree


决策树算法简介：

决策树是一种基本的分类和回归算法，对于分类问题，是根据对特征的划分对样本分类的过程，学习时利用训练数据，**根据最小化损失函数为学习策略**来建立树模型，预测时对新的数据利用以及生成的数来进行预测。决策树的学习算法可分为**特征的选择**，**决策树的生成**以及**树的剪枝**。


1. 决策树学习为什么使用启发式方法？
答： 决策树的本质是从训练数据中学习一些了if-then的规则。从而达到将训练数据正确分类的目的；但是能将数据正确分类的规则有很多种，我们需要的是一种拟合能力强但是泛化能力也强的模型，当我们的损失函数确定以后，我们就需要从这些规则里选择最优的一种。但是这很难选择，是一个NP难问题，所以在实际情况下通过启发式的搜索来近似求解。

2. 讲讲**特征选择***的过程
 答：对于ID3来说，使用信息增益来选择特征。具体步骤：开始，将所有训练的数据都放在根节点，依次选择一个特征，对于离散型特征，对其每一个取值都划分一个分支，对于连续值，可以直接取中位点作为划分点将其分开，大于切分点的分到右边，小于切分点的分到左边。每一次划分后都计算信息增益，选择划分后信息增益最大的特征作为划分点。若划分后，一些子集已经分类正确了，那么就标记为叶子节点，没能完全分开的子集继续按上述方法继续递归划分，直到子集中实例能被正确分开，或者划分的特征都用完了为止，最后每一个样本都落到了叶子节点上了。

3. 讲讲**熵**是啥
答：熵是表示随机变量不确定性的度量。熵越大 随机变量的不确定性越大。其公式如下：
$$ P(X=x_i) =p_i, i=1,2,3,...n$$

$$
H(X)=-\sum_{i=1}^n {p_ilogp_i}
$$

P(X)是随机变量X的概率分布。

4. 什么是**互信息**
答：表示得知特征X的信息而使得类Y的信息不确定性减少的程度。

5. **信息增益**（ID3）
集合D的经验熵与给定特征A条件下的条件熵之差。$ H(D)$表示对数据集D进行分类的不确定性，$H(D|A)$表示在给定特征A条件下对数据集D 分类的不确定性。那么信息增益就表示由于特征A使得对集合D分类不确定性减少的程度。 

6. **信息增益比**
信息增益比定义为信息增益与集合的经验熵之比
$$
g_R= \frac{g(D,A)}{H(D)}
$$

7. **C4.5算法**

8. 什么是剪枝
答：剪枝是将生成的树根据损失函数简化的过程。具体是从已生成的树上减掉一些叶子节点或者子树，回退到父节点，将父节点作为新的叶子节点。

## 2. CART
CART（分类回归树）既可以用与分类也可以用于回归。对回归树用平方损失最小原则，对分类树用基尼指数最小化准则。

#### 最小二乘回归树算法

输入：训练数据集D
输出： 回归树$f(x)$
采用启发式方法，找到一个变量J的最优切分点，遍历所有变量。找到了最优的变量和最优的切分点。将输入空间划分为两个空间，然后对两个空间递归。
（1）选择最优切分变量$j$与切分点$s$,求解
$$
\min_{j,s} \left[\min_{c1}\sum_{xi\in R_1(j,s)}(y_i-c_1)^2+\min_{c2}\sum_{xi\in R_2(j,s)}(y_i-c_2)^2 \right]  
$$

 遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，选择使（1）达到最小的对（j,s）

 （2）用选定的对（j,s）划分区域并决定相应的输出值：

$$
R_1(j,s) =\lbrace x|x^{j} \leq s \rbrace,  R_2(j,s) =\lbrace x|x^{j} \gt s \rbrace
$$
公式如下：

$$
\hat{c}_m = \frac{1}{N_m}\sum_{x_i \in R_m(j,s)}y_i ,x\in R_m,m=1,2,..m
$$

(3) 继续对两个子区域调用步骤（1），（2）直达满足停止条件
(4) 将输入空间划分为M个区域$R_1, R_2,....R_m$,生成决策树：
$$
f(x) = \sum_{m=1}^M{\hat{c}_mI(x \in R_m)}
$$

####  分类树
分类树用基尼指数选择最优特征，同时决定改特征的最优二值切分点。

**7.基尼指数定义**
分类问题中，假设有K个类，样本点属于第K个类的概率为$p_k$,则概率分布的基尼指数定义为
$$
Gini(p) = \sum_{k=1}^K{p_k(1-p_k)} = 1-\sum_{k=1}^K{p_k^2}
$$

如果样本集D根据特征A是否取某一可能值a被分割成$D_1$ 和 $D_2$两部分，即
$$
D_1 = \lbrace (x,y) \in D| A(x)=a\rbrace,D_2 = D-D_1
$$
则在特征A的条件下，集合D的基尼指数定义为
$$
Gini(D,A) = \frac{|D_1|}{|D|}Gini(D_1) +\frac{|D_2|}{|D|}Gini(D_2)
$$
基尼指数同信息熵一样，越大表示集合D越不确定。

**8.特征选择的总结**
特征选择的目的是选择对分类任务或回归任务有帮助的特征。特征选择的关键是其准则，常用的准则如下：
（1）样本集合$D$对特征$A$ 的信息增益（$ID3$）
$$
\begin{align}
g(D,A) =H(D) - H(D|A) \\
H(D) = -\sum_{k=1}^K\frac{C_k}{D}log_2{\frac{C_k}{D}} \\
H(D|A) = \sum_{i=1}^n\frac{D_i}{D}H(D_i) \\
\end{align}
$$

(2)样本集$D$的信息增益比（C4.5）
$$
g_R(D,A) = \frac{g(D,A)}{H(D)}
$$

(3)样本集合$D$的基尼指数（CART）
$$
Gini(D) = 1- \sum_{k=1}^K{p_k^2}
$$
特征A条件下集合D的基尼指数
$$
Gini(D,A) = \frac{|D_1|}{|D|}Gini(D_1) +\frac{|D_2|}{|D|}Gini(D_2)
$$

$$
\hat{c}_m = \frac{1}{N_m}\sum_{x_i \in R_m(j,s)}y_i ,x\in R_m,m=1,2,..m
$$

####  剪枝

剪枝是决策树学习算法**对付过拟合**的主要手段。决策树剪枝的基本策略有**预剪枝**和**后剪枝**。
- 预剪枝是在决策树生成的过程中，对每个节点划分前进行估计，若当前节点的划分不能提高决策树的泛化性能，那么就停止划分，将当前节点标记为叶子节点。
- 后剪枝是先生成一颗完整的树，自底向上的对非叶子节点进行考察：若是将该节点对应的字树替换为叶子节点后能提高决策树的泛化性能，则将该字树替换为叶子节点。

1. 预剪枝的优欠缺点
- 预剪枝有欠拟合的风险，但是减少了决策树的训练时间。
2. 后剪枝的优缺点
- 后剪枝欠拟合的风险小，泛化性能比预剪枝更好，但训练时间开销比预剪枝的更大。 

#### 一些基本问题
1. 决策树是如何处理连续值的
答：在处理连续值时，决策树是取相邻元素的平均值最为分割点，然后将连续值作为离散值来处理。

2. 决策树是如何处理缺失值的
答：决策树在处理缺失值是有两个问题要解决：（1）如何在属性值缺失的情况下选择划分属性。（2）在确定划分属性后，样本在该属性上缺失如何处理。

    （1）选择在该特征上没有缺失的样本子集作为集合$\hat{D}$,根据信息增益或者基尼指数来选择划分属性
    （2）确定划分特征后，如样本在该特征上取值缺失，则将该样本以不同的概率划分到每个叶子节点。



