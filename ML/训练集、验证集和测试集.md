训练集、验证集和测试集

## 模型评估

### 1.信息泄露

每次基于模型在验证集上的性能来调节模型超参数，都会有一些关于验证数据的信息泄露到模型中，如果只对每个参数只调节一次，那么泄露的数据很少，验证集任然能很好的评估模型。但是如果多次重复这一过程，那么就会有越来越多的关于验证集的信息泄露到模型中。

### 2.带有打乱数据的重复K折验证

具体做法是多次使用 K 折验证，在每次将数据划分为 K 个分区之前都先将数据打乱。
最终分数是每次 K 折验证分数的平均值。注意，这种方法一共要训练和评估 P×K 个模型（P是重复次数），计算代价很大

### 3. 模型评估的注意事项

1. 数据代表性
    你希望训练集和测试集都能代表当前数据，因此那在将数据划分为训练集和测试集之前，应该随机打乱数据.
2. 时间箭头
    如果想要根据过去预测未来（比如明天的天气），那么在划分数据之前**不能**随机打乱数据，因为这样会造成时间泄露。要确保训练集中所有数据的时间都要晚于训练集数据
3. 数据冗余
    保证训练集和验证集没有交集（防止数据泄露）


### 奥卡姆剃刀

原理：如果一件事有两种解释，那么最有可能正确的解释就是最简单的一个，即假设更少的那个。

### 机器学习的通用流程

1. 定义问题,收集数据集
    * 你的输入数据是什么?你要预测什么?
    * 你面对的是什么类型的问题?是二分类问题、多分类问题、还是其它回归，聚类等问题，确定问题类型**有助于选择模型的架构已经损失函数**
    * 假设输出是可以根据输入预测的
    * 假设可用数据包含足够多的信息，足以学习输入和输出之间的关系.
2. 选择评价指标
    评价指标将指引你选择损失函数，既模型要优化的东西，它应该与你的目标（业务指标）保持一致
3. 模型正则化与调节超参数
   添加 dropout。
   尝试不同的架构：增加或减少层数。
   添加 L1 和 / 或 L2 正则化。
   尝试不同的超参数（比如每层的单元个数或优化器的学习率），以找到最佳配置。
   （可选）反复做特征工程：添加新特征或删除没有信息量的特征

