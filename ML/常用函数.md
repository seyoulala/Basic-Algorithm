### 构建[min,max,median,sum]等统计特征

```python
def agg_numeric(df, group_var, df_name):
    """Aggregates the numeric values in a dataframe. This can
 be used to create features for each instance of the grouping variable.

 Parameters
 --------
 df (dataframe): 
 the dataframe to calculate the statistics on
 group_var (string): 
 the variable by which to group df
 df_name (string): 
 the variable used to rename the columns

 Return
 --------
 agg (dataframe): 
 a dataframe with the statistics aggregated for 
 all numeric columns. Each instance of the grouping variable will have 
 the statistics (mean, min, max, sum; currently supported) calculated. 
 The columns are also renamed to keep track of features created.

 """
    # Remove id variables other than grouping variable
    for col in df:
        if col != group_var and 'SK_ID' in col:
            df = df.drop(columns = col)

    group_ids = df[group_var]
    numeric_df = df.select_dtypes('number')
    numeric_df[group_var] = group_ids

    # Group by the specified variable and calculate the statistics
    agg = numeric_df.groupby(group_var).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()

    # Need to create new column names
    columns = [group_var]

    # Iterate through the variables names
    for var in agg.columns.levels[0]:
        # Skip the grouping variable
        if var != group_var:
            # Iterate through the stat names
            for stat in agg.columns.levels[1][:-1]:
                # Make a new column name for the variable and stat
                columns.append('%s_%s_%s' % (df_name, var, stat))

    agg.columns = columns
    return agg
```

### 计算变量和目标变量之间的相关性
```python
def target_corrs(df):

    # List of correlations
    corrs = []

    # Iterate through the columns 
    for col in df.columns:
        print(col)
        # Skip the target column
        if col != 'TARGET':
            # Calculate correlation with the target
            corr = df['TARGET'].corr(df[col])

            # Append the list as a tuple
            corrs.append((col, corr))

    # Sort by absolute magnitude of correlations
    corrs = sorted(corrs, key = lambda x: abs(x[1]), reverse = True)

    return corrs
```

### 对类别型变量的一些衍生
```python
def count_categorical(df, group_var, df_name):
    """Computes counts and normalized counts for each observation
 of `group_var` of each unique category in every categorical variable

 Parameters
 --------
 df : dataframe 
 The dataframe to calculate the value counts for.

 group_var : string
 The variable by which to group the dataframe. For each unique
 value of this variable, the final dataframe will have one row

 df_name : string
 Variable added to the front of column names to keep track of columns

 Return
 --------
 categorical : dataframe
 A dataframe with counts and normalized counts of each unique category in every categorical variable
 with one row for every unique value of the `group_var`.

 """

    # Select the categorical columns
    categorical = pd.get_dummies(df.select_dtypes('object'))

    # Make sure to put the identifying id on the column
    categorical[group_var] = df[group_var]

    # Groupby the group var and calculate the sum and mean
    categorical = categorical.groupby(group_var).agg(['sum', 'mean'])

    column_names = []

    # Iterate through the columns in level 0
    for var in categorical.columns.levels[0]:
        # Iterate through the stats in level 1
        for stat in ['count', 'count_norm']:
            # Make a new column name
            column_names.append('%s_%s_%s' % (df_name, var, stat))

    categorical.columns = column_names

    return categorica
```
### 特征重要性
```python
def plot_feature_importances(df):
    """
 Plot importances returned by a model. This can work with any measure of
 feature importance provided that higher importance is better. 

 Args:
 df (dataframe): feature importances. Must have the features in a column
 called `features` and the importances in a column called `importance

 Returns:
 shows a plot of the 15 most importance features

 df (dataframe): feature importances sorted by importance (highest to lowest) 
 with a column for normalized importance
 """

    # Sort features according to importance
    df = df.sort_values('importance', ascending = False).reset_index()

    # Normalize the feature importances to add up to one
    df['importance_normalized'] = df['importance'] / df['importance'].sum()

    # Make a horizontal bar chart of feature importances
    plt.figure(figsize = (10, 6))
    ax = plt.subplot()

    # Need to reverse the index to plot most important on top
    ax.barh(list(reversed(list(df.index[:15]))), 
            df['importance_normalized'].head(15), 
            align = 'center', edgecolor = 'k')

    # Set the yticks and labels
    ax.set_yticks(list(reversed(list(df.index[:15]))))
    ax.set_yticklabels(df['feature'].head(15))

    # Plot labeling
    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')
    plt.show()

    return df
```
### 删除共性特征
```python
def identify_collinear(df,correlation_threshold):
    corr_matrix = df.corr()
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))
    to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]
    record_collinear = pd.DataFrame(columns = ['drop_feature', 'corr_feature', 'corr_value'])
    for column in to_drop:
     # Find the correlated features
        corr_features = list(upper.index[upper[column].abs() > correlation_threshold])

        # Find the correlated values
        corr_values = list(upper[column][upper[column].abs() > correlation_threshold])
        drop_features = [column for _ in range(len(corr_features))]    

        # Record the information (need a temp df for now)
        temp_df = pd.DataFrame.from_dict({'drop_feature': drop_features,
                                         'corr_feature': corr_features,
                                         'corr_value': corr_values})

        # Add to dataframe
        record_collinear = record_collinear.append(temp_df, ignore_index = True)
    print('%d features with a correlation magnitude greater than %0.2f.\n' % (len(to_drop),correlation_threshold)
    return record_collinear,to_drop

```



```python
#按照主键groupby
def group(df_to_agg, prefix, aggregations, aggregate_by= 'sk_id_curr'):
    """
     对每个表按照主键groupby
    """
    agg_df = df_to_agg.groupby(aggregate_by).agg(aggregations)
    agg_df.columns = pd.Index(['{}{}_{}'.format(prefix, e[0], e[1].lower())
                               for e in agg_df.columns.tolist()])
    return agg_df.reset_index()
#合并groupby之后的数据集
def group_and_merge(df_to_agg, df_to_merge, prefix, aggregations, aggregate_by= 'sk_id_curr'):
    """
    合并groupby后的数据集
    """
    agg_df = group(df_to_agg, prefix, aggregations, aggregate_by= aggregate_by)
    return df_to_merge.merge(agg_df, how='left', on= aggregate_by)


#减少内存的使用
def reduce_memory(df):
    """Reduce memory usage of a dataframe by setting data types. """
    start_mem = df.memory_usage().sum() / 1024 ** 2
    print('Initial df memory usage is {:.2f} MB for {} columns'
          .format(start_mem, len(df.columns)))

    for col in df.columns:
        col_type = df[col].dtypes
        if col_type != object:
            cmin = df[col].min()
            cmax = df[col].max()
            if str(col_type)[:3] == 'int':
                # Can use unsigned int here too
                if cmin > np.iinfo(np.int8).min and cmax < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif cmin > np.iinfo(np.int16).min and cmax < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif cmin > np.iinfo(np.int32).min and cmax < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif cmin > np.iinfo(np.int64).min and cmax < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)
            else:
                if cmin > np.finfo(np.float16).min and cmax < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif cmin > np.finfo(np.float32).min and cmax < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
    end_mem = df.memory_usage().sum() / 1024 ** 2
    memory_reduction = 100 * (start_mem - end_mem) / start_mem
    print('Final memory usage is: {:.2f} MB - decreased by {:.1f}%'.format(end_mem, memory_reduction))
    return df

```

**混淆矩阵**

```python
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt 
from sklearn.utils.multiclass import unique_labels

def plot_confusion_matrix(y_true, y_pred, classes=[0,1],
                          normalize=False,
                          title=None,
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    # Only use the labels that appear in the data
#     classes = classes[unique_labels(y_true, y_pred)]
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    fig, ax = plt.subplots()
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    # We want to show all ticks...
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           # ... and label them with the respective list entries
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')

    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")

    # Loop over data dimensions and create text annotations.
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()
    return ax


```
## 贝叶斯优化

```python

#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Date    : 2019-08-04 15:11:05
# @Author  : xuyinghao (xyh650209@163.com)
# @Link    : https://github.com/seyoulala/Basic-Algorithm
# @Version : $Id$

import numpy as np
import pandas as pd
import warnings
import hyperopt
from hyperopt import hp
from hyperopt.pyll.stochastic import sample
from hyperopt import tpe
from hyperopt import Trials
from hyperopt import fmin
from hyperopt import STATUS_OK
from hyperopt import partial
import gc
import csv
import os
import sys
import lightgbm as lgb
from lightgbm import LGBMClassifier, LGBMRegressor
from timeit import default_timer as timer
from sklearn.metrics import roc_auc_score, mean_squared_error



warnings.filterwarnings('ignore')



class AutoHyperparameter():

    def __init__(self, data, labels, max_iter=100, task='classifier', eval_set=None, out_of_file=None):
    	"""
		max_iter:搜索次数
		task:学习任务
		eval_set:额外指定的验证集
		out_of_file:日志文件

    	"""
        self.data = data
        self.labels = labels
        if self.labels is None:
            raise ValueError("labels is None please input lables")

        self.eval_set = eval_set
        if self.eval_set:
            if not isinstance(eval_set, tuple):
                raise ValueError("eval_set should be a tuple")
        # 交叉验证次数
        self.N_fold = 5
        self.ITERATION = 0
        self.eval_set = eval_set
        self.out_of_file = out_of_file
        self.task = task
        self.MAX_EVALS = max_iter

    def objective(self, hyperparameters):
        """
        :param hyperparameters:
        :return:

        建立需要优化的目标函数
        """
        self.ITERATION += 1

        if 'n_estimators' in hyperparameters:
            del hyperparameters['n_estimators']

        subsample = hyperparameters['boosting_type'].get('subsample', 1.0)
        hyperparameters['boosting_type'] = hyperparameters['boosting_type']['boosting_type']
        hyperparameters['subsample'] = subsample

        for parameter_name in ['num_leaves', 'min_child_samples']:
            hyperparameters[parameter_name] = int(
                hyperparameters[parameter_name])

        n_estimators = None
        start = timer()
        if not self.eval_set:
            train_set = lgb.Dataset(self.data, self.labels)
            if self.task == 'classifier':
                cv_result = lgb.cv(hyperparameters, train_set=train_set, num_boost_round=10000, stratified=True, nfold=self.N_fold,
                                   shuffle=True, early_stopping_rounds=50, verbose_eval=50,metrics='auc', seed=42)
                best_score = np.max(cv_result['auc-mean'])
                n_estimators = int(np.argmax(cv_result['auc-mean']) + 1)
                hyperparameters['n_estimators'] = n_estimators

            else:
                cv_result = lgb.cv(hyperparameters, train_set=train_set, num_boost_round=10000, stratified=False, nfold=self.N_fold,
                                   shuffle=True, early_stopping_rounds=50, verbose_eval=50,metrics='rmse', seed=42)
                best_score = np.min(cv_result['rmse-mean'])
                n_estimators = int(np.argmin(cv_result['rmse-mean']) + 1)
                hyperparameters['n_estimators'] = n_estimators
        else:
            if self.task == 'classifier':
                hyperparameters['n_estimators'] = 10000
                clf = LGBMClassifier(**hyperparameters)
                clf.fit(self.data, self.labels, eval_set=[(self.data,self.labels),self.eval_set],verbose=50,early_stopping_rounds=50)
                valid_pro = clf.predict_proba(
                    self.eval_set[0], num_iteration=clf.best_iteration_)[:, 1]
                best_score = roc_auc_score(self.eval_set[1], valid_pro)
                hyperparameters['n_estimators'] = clf.best_iteration_
                print(hyperparameters)
            else:
                hyperparameters['n_estimators'] = 10000
                reg = LGBMRegressor(**hyperparameters)
                reg.fit(self.data, self.labels, eval_set=[(self.data,self.labels),self.eval_set],verbose=50,
                        early_stopping_rounds=50, eval_metric='rmse')
                vaild_ = reg.predict(
                    self.eval_set[0], num_iteration=reg.best_iteration_)
                best_score = np.sqrt(mean_squared_error(eval_set[1], vaild_))
                hyperparameters['n_estimators'] = reg.best_iteration_

        runtime = timer() - start

        # 需要最小化的东西
        if self.task == 'classifier':
            loss = 1 - best_score
        else:
            loss = best_score

        # 将结果写入文件
        # out_file = "./bayes_iteration_{}.csv".format(self.MAX_EVALS)


        if not os.path.exists(os.path.abspath(self.out_of_file)):
            of_connection = open(self.out_of_file, 'w')
            writer = csv.writer(of_connection)
            headers = ['loss', 'hyperparameters',
                       'iteration', 'runtime', 'score']
            writer.writerow(headers)
            writer.writerow(
                [loss, hyperparameters, self.ITERATION, runtime, best_score])
            of_connection.close()
        else:
            of_connection = open(self.out_of_file, 'a')
            writer = csv.writer(of_connection)
            writer.writerow(
                [loss, hyperparameters, self.ITERATION, runtime, best_score])
            of_connection.close()

        return loss

    @staticmethod
    def get_hyperparameter_search_space():
        """
        定义搜索的参数空间
        :param self:
        :return:
        """
        space = {
            'boosting_type': hp.choice('boosting_type',
                                       [{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)},
                                        {'boosting_type': 'dart', 'subsample': hp.uniform(
                                            'dart_subsample', 0.5, 1)},
                                        {'boosting_type': 'goss', 'subsample': 1.0}]),
            'max_bin':hp.choice('max_bin',[63,128,255]),
            # 'max_depth':hp.quniform('max_depth',3,10,1),
            'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.5)),
            # 'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),
            'min_child_samples': hp.quniform('min_child_samples', 20, 100, 2),
            # 'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),
            # 'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),
            # 'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0),
        }
        return space

    @staticmethod
    def get_optimization_Algorithm():
        tpe_algorithm = partial(tpe.suggest,n_startup_jobs=5)

        return tpe_algorithm

    @staticmethod
    def get_result_history():
        trials = Trials()

    def run_optimization(self):
        fn = self.objective
        space = self.get_hyperparameter_search_space()
        algo = self.get_optimization_Algorithm()
        trials = self.get_result_history()
        max_evals = self.MAX_EVALS
        best = fmin(fn=fn, space=space, algo=algo,
                    trials=trials, max_evals=max_evals)
        return best


def fimpCal(df, feature_name, imp_name):
    feature_col = df[feature_name]
    first_list = []
    second_list = []
    first_list_imp = []
    second_list_imp = []
    for col in feature_col:
        tmp = col.split('_')
        first_list.append(tmp[0])
        second_list.append("_".join(tmp[:2]))
    first_list = list(set(first_list))
    second_list = list(set(second_list))
    for a in first_list:
        first_list_imp.append(df.loc[df['feature'].isin(
            list(filter(lambda x: x.startswith(a), feature_col))), imp_name].sum())
    for b in second_list:
        second_list_imp.append(df.loc[df['feature'].isin(
            list(filter(lambda x: x.startswith(b), feature_col))), imp_name].sum())
    tmp = pd.DataFrame()
    first_list = list(map(lambda x: x + "_total", first_list))
    second_list = list(map(lambda x: x + "_total", second_list))
    tmp[feature_name] = first_list + second_list
    tmp[imp_name] = first_list_imp + second_list_imp
    df = pd.concat([df, tmp], axis=0)
    df.reset_index(drop=True, inplace=True)

    return df


if __name__ == "__main__":
	logs = './xyh/data/bayes_seach_logs.csv'
	#额外指定验证集
	X_train, X_test, y_train, y_test = train_test_split(df[predictors],df['target'], test_size=0.33, random_state=42)
	#指定验证集
	bayes_opt = AutoHyperparameter(X_train,y_train,max_iter=100,eval_set=(X_test,y_test),out_of_file=logs)
	#不指定验证集,使用cv方式
	bayes_opt = AutoHyperparameter(X_train,y_train,max_iter=100,out_of_file=logs)

	#开始优化
	bayes_opt.run_optimization()

	result = pd.read_csv(logs)

	opt_parameter = result.sort_values(by='best_score',ascending=False).loc[0,'hyperparameters']
	print("The best parameter is {}".format(opt_parameter))

```

### spark 初始化

```python

spark=SparkSession \
        .builder \
        .config("spark.eventLog.enabled", "True") \
        .config("spark.num.excutor","4")\
        .config("spark.executor.memory", "8g")\
        .config("spark.driver.memory", "4g")\
        .config("spark.cores.max", "3")\
        .config("spark.task.maxFailures", "1000")\
        .config("spark.default.parallelism", "500")\
        .config('spark.sql.execution.arrow.enabled', 'true')\
        .appName('spark_credit') \
        .master('yarn')\
        .enableHiveSupport()\
        .getOrCreate()
        
```

## spark 相关函数

```python
tmp = df_train.agg(*[(1-(F.count(c) /F.count('*'))).alias(c+'_missing') for c in df_train.columns]).toPandas()


def group(df_to_agg, prefix, aggregations, aggregate_by= 'sk_id_curr'):
    """
     对每个表按照主键groupby
    """
    agg_df = df_to_agg.groupby(aggregate_by).agg(*aggregations)
    df_col =agg_df.columns
    df_col.remove(aggregate_by)
    rename_col =[]
    for col in df_col:
        tmp_col = re.split("\(|\)",col)
        tmp_col.remove('')
        tmp_col = tmp_col[::-1]
        rename_col.append(prefix+'_'.join(tmp_col))
    #映射后的列名
    mapping = dict(zip(df_col,rename_col))
  
    #agg_df = agg_df.select([col(c).alias(mapping.get(c, c)) for c in df_col])
    for col in df_col:
        agg_df = agg_df.withColumnRenamed(col,mapping.get(col))
    
    return agg_df


def dict_to_list(x:dict)-> list:
    if  not isinstance(x,dict):
        raise ValueError("x need to be a dict !")
    tmp_list =[]
    for k,v in x.items():
        for func_name in v:
            tmp_list.append(eval(func_name)(k))
    return tmp_list
```



**计算auc值**

```Python
import  random
from sklearn.metrics import roc_auc_score

def gen_label_pred(n_sample):
    """
    随机生成n个样本的标签和预测值
    """
    labels = [random.randint(0, 1) for _ in range(n_sample)]
    preds = [random.random() for _ in range(n_sample)]
    return labels, preds


def naive_auc(labels, preds):
	"""
	最简单粗暴的方法
　　　先排序，然后统计有多少正负样本对满足：正样本预测值>负样本预测值, 再除以总的正负样本对个数
	 复杂度 O(NlogN), N为样本数
	"""
	n_pos = sum(labels)
	n_neg = len(labels) - n_pos
	total_pair = n_pos * n_neg

	labels_preds = zip(labels, preds)
	labels_preds = sorted(labels_preds, key=lambda x: x[1])
	accumulated_neg = 0
	satisfied_pair = 0
	for i in range(len(labels_preds)):
		if labels_preds[i][0] == 1:
			satisfied_pair += accumulated_neg
		else:
			accumulated_neg += 1

	return satisfied_pair / float(total_pair)


def cal_auc(labels,pred):
	pos_label = sum(labels)
	neg_label = len(labels)-pos_label
	total_num = pos_label*neg_label
	#总的正负样本对
	pos_list =[]
	neg_list = []
	for x,y in zip(labels,pred):
		if x==1:
			pos_list.append((x,y))
		else:
			neg_list.append((x,y))
	satis = 0
	for i in range(pos_label):
		for j in range(neg_label):
			if pos_list[i][1]>neg_list[j][1]:
				satis+=1
	return satis/total_num

```

**SMOTE采样函数**

```python
class imbalanceData():  
    
    """ 
      处理不均衡数据
        train训练集
        test测试集
        mmin低分段错分比例
        mmax高分段错分比例
        flag样本标签
        lis不参与建模变量列表
    """
    def __init__(self, train,test,mmin,mmax, flag,lis=[]):
        self.flag = flag
        self.train_x = train.drop([flag]+lis,axis=1)
        self.train_y = train[flag]
        self.test_x = test.drop([flag]+lis,axis=1)
        self.test_y = test[flag]
        self.columns = list(self.train_x.columns)
        self.keep = self.columns + [self.flag]
        self.mmin = 0.1
        self.mmax = 0.7
    
    '''
        设置不同比例，
        将头部和尾部预测不准的样本，
        将样本分成两类。
        0.1为噪声的权重，不参与过采样。
        1为正常样本权重，参与过采样。
    '''
    def weight(self,x,y):
        if x == 0 and y < self.mmin:
            return 0.1
        elif x == 1 and y > self.mmax:
            return 0.1
        else:
            return 1
    '''
        用一个LightGBM算法和weight函数进行样本选择
        只取预测准确的部分进行后续的smote过采样
    '''
    def data_cleaning(self):
        lgb_model,lgb_auc  = self.lgb_test()
        sample = self.train_x.copy()
        sample[self.flag] = self.train_y
        sample['pred'] = lgb_model.predict_proba(self.train_x)[:,1]
        sample = sample.sort_values(by=['pred'],ascending=False).reset_index()
        sample['rank'] = np.array(sample.index)/len(sample)
        sample['weight'] = sample.apply(lambda x: self.weight(x.flag,x['rank']),axis = 1)
        osvp_sample = sample[sample.weight == 1][self.keep]
        osnu_sample = sample[sample.weight < 1][self.keep]   
        train_x_osvp = osvp_sample[self.columns]
        train_y_osvp = osvp_sample[self.flag]
        return train_x_osvp,train_y_osvp,osnu_sample

    '''
        实施smote过采样
    '''
    def apply_smote(self):
        '''
            选择样本，只对部分样本做过采样
            train_x_osvp,train_y_osvp 为参与过采样的样本
            osnu_sample为不参加过采样的部分样本
        '''
        train_x_osvp,train_y_osvp,osnu_sample = self.data_cleaning()
        rex,rey = self.smote(train_x_osvp,train_y_osvp)
        print('badpctn:',rey.sum()/len(rey))
        df_rex = pd.DataFrame(rex)
        df_rex.columns =self.columns
        df_rex['weight'] = 1
        df_rex[self.flag] = rey
        df_aff_ovsp = df_rex.append(osnu_sample)
        return df_aff_ovsp

    '''
        定义LightGBM函数
    '''
    def lgb_test(self):
        import lightgbm as lgb
        clf =lgb.LGBMClassifier(boosting_type = 'gbdt',
                               objective = 'binary',
                               metric = 'auc',
                               learning_rate = 0.1,
                               n_estimators = 24,
                               max_depth = 4,
                               num_leaves = 25,
                               max_bin = 40,
                               min_data_in_leaf = 5,
                               bagging_fraction = 0.6,
                               bagging_freq = 0,
                               feature_fraction = 0.8,
                               )
        clf.fit(self.train_x,self.train_y,eval_set = [(self.train_x,self.train_y),(self.test_x,self.test_y)],eval_metric = 'auc')
        return clf,clf.best_score_['valid_1']['auc']

    '''
        调用imblearn中的smote函数
    '''
    def smote(self,train_x_osvp,train_y_osvp,m=4,K=15,random_state=0):
        from imblearn.over_sampling import SMOTE
        smote = SMOTE(k_neighbors=K, kind='borderline1', m_neighbors=m, n_jobs=1,
                      out_step='deprecated', random_state=random_state, ratio=None,
                      svm_estimator='deprecated')
        rex,rey = smote.fit_resample(train_x_osvp,train_y_osvp)
        return rex,rey
```

###  类别变量可视化

```python
def ploting_cat_fet(df, cols, vis_row=5, vis_col=2):
    
    grid = gridspec.GridSpec(vis_row,vis_col) # The grid of chart
    plt.figure(figsize=(17, 35)) # size of figure

    # loop to get column and the count of plots
    for n, col in enumerate(df_train[cols]): 
        tmp = pd.crosstab(df_train[col], df_train['target'], normalize='index') * 100
        tmp = tmp.reset_index()
        tmp.rename(columns={0:'No',1:'Yes'}, inplace=True)

        ax = plt.subplot(grid[n]) # feeding the figure of grid
        sns.countplot(x=col, data=df_train, order=list(tmp[col].values) , color='green') 
        ax.set_ylabel('Count', fontsize=15) # y axis label
        ax.set_title(f'{col} Distribution by Target', fontsize=18) # title label
        ax.set_xlabel(f'{col} values', fontsize=15) # x axis label

        # twinX - to build a second yaxis
        gt = ax.twinx()
        gt = sns.pointplot(x=col, y='Yes', data=tmp,
                           order=list(tmp[col].values),
                           color='black', legend=False)
        gt.set_ylim(tmp['Yes'].min()-5,tmp['Yes'].max()*1.1)
        gt.set_ylabel("Target %True(1)", fontsize=16)
        sizes=[] # Get highest values in y
        for p in ax.patches: # loop to all objects
            height = p.get_height()
            sizes.append(height)
            ax.text(p.get_x()+p.get_width()/2.,
                    height + 3,
                    '{:1.2f}%'.format(height/total*100),
                    ha="center", fontsize=14) 
        ax.set_ylim(0, max(sizes) * 1.15) # set y limit based on highest heights


    plt.subplots_adjust(hspace = 0.5, wspace=.3)
    plt.show()



def color_and_top(nb_mod, feature, typ, top_n=None):
    """
    nb_mod: 特征的取值状态数量
    feature: 特征名称
    top_n:取前topn个取值状态
    """
    
    if top_n is None:
        resu = ["g", nb_mod]
    elif nb_mod > 2*top_n:
        resu = ["r", top_n]
    elif nb_mod > top_n:
        resu =["orange", top_n]
    else: 
        resu = ["g", nb_mod]
    
    title = feature[:20]+" ("+typ[:3]+"-{})".format(nb_mod)
    resu.append(title)
    
    return resu


def plot_multiple_categorical(df, features, col_target=None, top_n=None
                              , nb_subplots_per_row = 4, hspace = 1.3, wspace = 0.5
                              , figheight=15, m_figwidth=4.2, landmark = .01):
    """
    nb_subplots_per_row:每行图的数量
    col_target:标签列
    """
#    sns.set_style('whitegrid')
    
    if not (col_target is None):
        ref = df[col_target].mean() # Reference
    
    plt.figure()
    if len(features) % nb_subplots_per_row >0:
        nb_rows = int(np.floor(len(features) / nb_subplots_per_row)+1)
    else:
        nb_rows = int(np.floor(len(features) / nb_subplots_per_row))
    fig, ax = plt.subplots(nb_rows, nb_subplots_per_row, figsize=(figheight, m_figwidth * nb_rows))
    plt.subplots_adjust(hspace = hspace, wspace = wspace)

    i = 0; n_row=0; n_col=0
    for feature in features:
        
        i += 1
        plt.subplot(nb_rows, nb_subplots_per_row, i)

        dff = df[[feature, col_target]].copy() # I don't want transform data, only study them
        
        # Missing values
        if dff[feature].dtype.name in ["float16", "float32", "float64"]:
            dff[feature].fillna(-997, inplace=True)
            
        if dff[feature].dtype.name in ["object"]:
            dff[feature].fillna("_NaN", inplace=True)
            
        if dff[feature].dtype.name == "category" and dff[feature].isnull().sum() > 0:
            dff[feature] = dff[feature].astype(str).replace('', '_NaN', regex=False).astype("category")
            
        # Colors, title
        bar_colr, top_nf, title = color_and_top(dff[feature].nunique(), feature, str(dff[feature].dtype), top_n)
        
        # stats
        tdf = dff.groupby([feature]).agg({col_target: ['count', 'mean']})
        tdf = tdf.sort_values((col_target, 'count'), ascending=False).head(top_nf).sort_index()
        
        tdf.index = tdf.index.map(str)
        tdf = tdf.rename(index={'-997.0':'NaN'}) # Missing values
        if not (top_n is None):
            tdf.index = tdf.index.map(lambda x: x[:top_n]) # tronque les libellés des modalités en abcisse
        
        tdf["ref"] = ref
        tdf["ref-"] = ref-landmark
        tdf["ref+"] = ref+landmark
        
        # First Y axis, on the left
        plt.bar(tdf.index, tdf[col_target]['count'].values, color=bar_colr) # Count of each category
        
        plt.title(title, fontsize=11)
        plt.xticks(rotation=90)
        
        # Second Y axis, on the right
        xx = plt.xlim()
        if nb_subplots_per_row == 1:
            ax2 = fig.add_subplot(nb_rows, nb_subplots_per_row, i, sharex = ax[n_row], frameon = False)
        else:
            ax2 = fig.add_subplot(nb_rows, nb_subplots_per_row, i, sharex = ax[n_row, n_col], frameon = False)
        if not (col_target is None):
            ax2.plot(tdf[col_target]['mean'].values, marker = 'x', color = 'b', linestyle = "solid") # Mean of each Category
            ax2.plot(tdf["ref"].values, marker = '_', color = 'black', linestyle = "solid", linewidth=4.0) # Reference
            ax2.plot(tdf["ref-"].values, marker = '_', color = 'black', linestyle = "solid", linewidth=1.0) # Reference
            ax2.plot(tdf["ref+"].values, marker = '_', color = 'black', linestyle = "solid", linewidth=1.0) # Reference
        ax2.yaxis.tick_right()
        ax2.axes.get_xaxis().set_visible(False)
        plt.xlim(xx)

        n_col += 1
        if n_col == nb_subplots_per_row:
            n_col = 0
            n_row += 1
            
    plt.show();
```

### 输出日志

```python
def logger_info(filename,mode):
    import  logging
    import logging.handlers
    logger = logging.getLogger("logger")

    handler1 = logging.StreamHandler()
    handler2 = logging.FileHandler(filename=filename, mode=mode,encoding='utf-8')
    logger.setLevel(logging.DEBUG)
    handler1.setLevel(logging.INFO)
    handler2.setLevel(logging.DEBUG)

    formatter = logging.Formatter("%(asctime)s %(name)s %(levelname)s %(message)s")
    handler1.setFormatter(formatter)
    handler2.setFormatter(formatter)
    logger.addHandler(handler1)
    logger.addHandler(handler2)
    return logger
logger = logger_info(filename='test.log',mode='w')
```

## summay

```python
def resumetable(df):
  	from scipy import stats
    print(f"Dataset Shape: {df.shape}")
    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])
    summary = summary.reset_index()
    summary['Name'] = summary['index']
    summary = summary[['Name','dtypes']]
    summary['Missing'] = df.isnull().sum().values    
    summary['Uniques'] = df.nunique().values
    summary['First Value'] = df.loc[0].values
    summary['Second Value'] = df.loc[1].values
    summary['Third Value'] = df.loc[2].values

    for name in summary['Name'].value_counts().index:
        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) 

    return summary


```

### target 分布

```python
def plot_target_distribution(df_train,target):
    total = len(df_train)
    plt.figure(figsize=(12,6))

    g = sns.countplot(x=target, data=df_train, color='green')
    g.set_title("TARGET DISTRIBUTION", fontsize = 20)
    g.set_xlabel("Target Vaues", fontsize = 15)
    g.set_ylabel("Count", fontsize = 15)
    sizes=[] # Get highest values in y
    for p in g.patches:
        height = p.get_height()
        sizes.append(height)
        g.text(p.get_x()+p.get_width()/2.,
                height + 3,
                '{:1.2f}%'.format(height/total*100),
                ha="center", fontsize=14) 
    g.set_ylim(0, max(sizes) * 1.15) # set y limit based on highest heights

    plt.show()
```



### 直方图

```python
import matplotlib.gridspec as gridspec # to do the grid of plots
grid = gridspec.GridSpec(3, 2) # The grid of chart
plt.figure(figsize=(16,20)) # size of figure

# loop to get column and the count of plots
for n, col in enumerate(df_train[bin_cols]): 
    ax = plt.subplot(grid[n]) # feeding the figure of grid
    sns.countplot(x=col, data=df_train, hue='target', palette='hls') 
    ax.set_ylabel('Count', fontsize=15) # y axis label
    ax.set_title(f'{col} Distribution by Target', fontsize=18) # title label
    ax.set_xlabel(f'{col} values', fontsize=15) # x axis label
    sizes=[] # Get highest values in y
    for p in ax.patches: # loop to all objects
        height = p.get_height()
        sizes.append(height)
        ax.text(p.get_x()+p.get_width()/2.,
                height + 3,
                '{:1.2f}%'.format(height/total*100),
                ha="center", fontsize=14) 
    ax.set_ylim(0, max(sizes) * 1.15) # set y limit based on highest heights
    
plt.show()
```

![](https://www.kaggleusercontent.com/kf/20081151/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..KoMbJY_iAURBwQ9B_Uji_g.8kiN4kDFdmGL3OA76myTCuBBFacqrobHwPbj0cLdo52KmLWunNrmkdSK8vRkqCy3oH14iRDh8ZtX5gTiH1wEWVwEjSzzv8eeXGuZtWUyoFxxWEYy-b0HsgwvpPOPQKBvF_L0e3ij1oV8m858-OY8tLJEObZtwtEghqv6fTgxOek.LNgS7LwNVZosslavPcWa7g/__results___files/__results___17_0.png)

### 多模型评估

```python
clfs = []
seed = 42

clfs.append(("LogReg", 
             Pipeline([("Scaler", StandardScaler()),
                       ("LogReg", LogisticRegression())])))

clfs.append(("XGBClassifier", XGBClassifier()))

# clfs.append(("KNN", 
#              Pipeline([("Scaler", StandardScaler()),
#                        ("KNN", KNeighborsClassifier(n_neighbors=5))])))

clfs.append(("DecisionTreeClassifier", DecisionTreeClassifier()))

clfs.append(("RandomForestClassifier", RandomForestClassifier(n_estimators=100)))

clfs.append(("GradientBoostingClassifier", GradientBoostingClassifier(n_estimators=100)))

clfs.append(("RidgeClassifier", 
             Pipeline([("Scaler", StandardScaler()),
                       ("RidgeClassifier", RidgeClassifier())])))

clfs.append(("BaggingClassifier",
             Pipeline([("Scaler", StandardScaler()),
                       ("BaggingClassifier", BaggingClassifier())])))

clfs.append(("ExtraTreesClassifier",ExtraTreesClassifier()))

#'neg_mean_absolute_error', 'neg_mean_squared_error','r2'
scoring = 'roc_auc'
n_folds = 7

results, names  = [], [] 

for name, model  in clfs:
    kfold = KFold(n_splits=n_folds, shuffle=False, random_state=seed)
    
    cv_results = cross_val_score(model, 
                                 X_train.values, y_train, 
                                 cv= kfold, scoring=scoring,
                                 n_jobs=-1)    
    names.append(name)
    results.append(cv_results)    
    msg = "%s: %f (+/- %f)" % (name, cv_results.mean(),  
                               cv_results.std())
    print(msg)
    
# boxplot algorithm comparison
fig = plt.figure(figsize=(15,6))
fig.suptitle('Classifier Algorithm Comparison', fontsize=22)
ax = fig.add_subplot(111)
sns.boxplot(x=names, y=results)
ax.set_xticklabels(names)
ax.set_xlabel("Algorithmn", fontsize=20) 
ax.set_ylabel("Accuracy of Models", fontsize=18)
ax.set_xticklabels(ax.get_xticklabels(),rotation=45)

plt.show()
```

### 使用lightgbm构建交叉特征

~~~python
class lgb_feature_cv:
    def __init__(self,params,transet,testset,label=None):
        self.params = params
        self.trainset = transet
        self.testset = testset
        self.label = label

        self.onehot = OneHotEncoder()
        if  not isinstance(str,label):
            raise  ValueError("label should be string")
        self.feature_list = [col for col in transet.columns if col !=self.label]

        self.leaf_feature_train = None
        self.leaf_feature_test = None
    def getFeature(self):
        lgb_trainSet = lgb.Dataset(self.trainset[self.feature_list],self.trainset[self.label])
        lgb_teseSet = lgb.Dataset(self.testset[self.feature_list],self.testset[self.label])
        model = lgb.train(self.params,lgb_trainSet,valid_sets=[lgb_trainSet,lgb_teseSet],
                          valid_names=['train','eval'],early_stopping_rounds=50)
        leaf = model.predict(lgb_trainSet,pred_leaf=True)
        leaf = self.onehot.fit(leaf)
        leaf = self.onehot.transform(leaf)
        self.leaf_feature_train = pd.DataFrame(leaf,columns=['ft'+str(i) for i in range(leaf).shape[1]])
        # merge_data_train = pd.concat([self.trainset[self.feature_list],leaf],axis=1)
        #对验证集做同样的操作
        leaf_test = model.predict(self.testset[self.feature_list],pred_leaf=True)
        leaf_test = self.onehot.transform(leaf_test)
        self.leaf_feature_test = pd.DataFrame(leaf_test,columns=['ft'+str(i) for i in range(leaf_test).shape[1]])
        # merge_data_test = pd.concat([self.testset[self.feature_list],leaf_test],axis=1)

    def merge_feature(self):
        merge_data_train = pd.concat([self.trainset[self.feature_list],self.leaf_feature_train],axis=1)
        merge_data_test = pd.concat([self.testset[self.feature_list],self.leaf_feature_test],axis=1)
        return merge_data_train,merge_data_test

    def make_psi_data(self,df: pd.DataFrame):
        dftot = pd.DataFrame()
        for col in df.columns:
            zero = sum(df[col] == 0)
            one = sum(df[col] == 1)
            ftdf = pd.DataFrame(np.array([zero, one]))
            ftdf.columns = [col]
            if len(dftot) == 0:
                dftot = ftdf.copy()
            else:
                dftot[col] = ftdf[col].copy()
        return dftot

    def var_psi(self,train_data, test_data):
        train_cnt, test_cnt = sum(train_data), sum(test_data)
        if train_cnt * test_data == 0:
            return 0
        PSI = 0
        for i in range(len(train_data)):
            train_ratio = train_data[i] / train_cnt
            test_ratio = test_cnt[i] / test_cnt + 1e-10
            psi = (train_ratio - test_ratio) * np.log(train_ratio / test_ratio)
            PSI += psi
        return PSI
~~~

### lightgbm训练函数

~~~python
NUM_THREADS = 4
DATA_DIRECTORY = "../input/"
SUBMISSION_SUFIX = "_model_3_29"

# 模型参数以及超参
GENERATE_SUBMISSION_FILES = True
STRATIFIED_KFOLD = True
RANDOM_SEED = 2020
NUM_FOLDS = 5
EARLY_STOPPING = 100

LIGHTGBM_PARAMS = {
    'boosting_type': 'gbdt',
    'n_estimators': 10000,
    'learning_rate': 0.05134,
    'num_leaves': 54,
    'max_depth': 10,
    'subsample_for_bin': 240000,
    'reg_alpha': 0.436193,
    'reg_lambda': 0.479169,
    'colsample_bytree': 0.508716,
    'min_split_gain': 0.024766,
    'subsample': 1,
    'is_unbalance': False,
    'silent':-1,
    'verbose':-1
}

def kfold_lightgbm_sklearn(data, categorical_feature = None,STRATIFIED_KFOLD=None):
    df = data[data['target'].notnull()]
    test = data[data['target'].isnull()]
    print("Train/valid shape: {}, test shape: {}".format(df.shape, test.shape))
    del_features = ['target']
    predictors = list(filter(lambda v: v not in del_features, df.columns))

    if not STRATIFIED_KFOLD:
        folds = KFold(n_splits= NUM_FOLDS, shuffle=True, random_state= RANDOM_SEED)
    else:
        folds = StratifiedKFold(n_splits= NUM_FOLDS, shuffle=True, random_state= RANDOM_SEED)

    oof_preds = np.zeros(df.shape[0])
    sub_preds = np.zeros(test.shape[0])
    importance_df = pd.DataFrame()
    eval_results = dict()

    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(df[predictors], df['target'])):
        train_x, train_y = df[predictors].iloc[train_idx], df['target'].iloc[train_idx]
        valid_x, valid_y = df[predictors].iloc[valid_idx], df['target'].iloc[valid_idx]

        params = {'random_state': RANDOM_SEED, 'nthread': NUM_THREADS}
        clf = LGBMClassifier(**{**params, **LIGHTGBM_PARAMS})
        if not categorical_feature:
            clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],
                    eval_metric='auc', verbose=400, early_stopping_rounds= EARLY_STOPPING)
        else:
            clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],
                    eval_metric='auc', verbose=400, early_stopping_rounds=EARLY_STOPPING,
                    feature_name= list(df[predictors].columns), categorical_feature= categorical_feature)

        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]
#         sub_preds += clf.predict_proba(test[predictors], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits

        fold_importance = pd.DataFrame()
        fold_importance["feature"] = predictors
        fold_importance["gain"] = clf.booster_.feature_importance(importance_type='gain')
        fold_importance["split"] = clf.booster_.feature_importance(importance_type='split')
        importance_df = pd.concat([importance_df, fold_importance], axis=0)
        eval_results['train_{}'.format(n_fold+1)]  = clf.evals_result_['training']['auc']
        eval_results['valid_{}'.format(n_fold+1)] = clf.evals_result_['valid_1']['auc']

        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))
        del clf, train_x, train_y, valid_x, valid_y
        gc.collect()

    print('Full AUC score %.6f' % roc_auc_score(df['target'], oof_preds))
#     test['target'] = sub_preds.copy()

    mean_importance = importance_df.groupby('feature').mean().reset_index()
    mean_importance.sort_values(by= 'gain', ascending=False, inplace=True)
#     if GENERATE_SUBMISSION_FILES:

#         oof = pd.DataFrame()
#         oof['sk_id_curr'] = df['sk_id_curr'].copy()
#         oof['predictions'] = oof_preds.copy()
#         oof['target'] = df['target'].copy()
#         oof.to_csv('oof{}.csv'.format(SUBMISSION_SUFIX), index=False)
#         test[['sk_id_curr', 'target']].to_csv('submission{}.csv'.format(SUBMISSION_SUFIX), index=False)
#         mean_importance.to_csv('feature_importance{}.csv'.format(SUBMISSION_SUFIX), index=False)
    return mean_importance

~~~

### 字段report 

~~~python
import pandas as pd

def getTopValues(series, top = 5, reverse = False):
    """Get top/bottom n values

    Args:
        series (Series): data series
        top (number): number of top/bottom n values
        reverse (bool): it will return bottom n values if True is given

    Returns:
        Series: Series of top/bottom n values and percentage. ['value:percent', None]
    """
    itype = 'top'
    counts = series.value_counts()
    counts = list(zip(counts.index, counts, counts.divide(series.size)))

    if reverse:
        counts.reverse()
        itype = 'bottom'

    template = "{0[0]}:{0[2]:.2%}"
    indexs = [itype + str(i + 1) for i in range(top)]
    values = [template.format(counts[i]) if i < len(counts) else None for i in range(top)]

    return pd.Series(values, index = indexs)


def getDescribe(series, percentiles = [.25, .5, .75]):
    """Get describe of series

    Args:
        series (Series): data series
        percentiles: the percentiles to include in the output

    Returns:
        Series: the describe of data include mean, std, min, max and percentiles
    """
    d = series.describe(percentiles)
    return d.drop('count')


def countBlank(series, blanks = [None]):
    """Count number and percentage of blank values in series

    Args:
        series (Series): data series
        blanks (list): list of blank values

    Returns:
        number: number of blanks
        str: the percentage of blank values
    """
    # n = 0
    # counts = series.value_counts()
    # for blank in blanks:
    #     if blank in counts.keys():
    #         n += counts[blank]

    n = series.isnull().sum()

    return (n, "{0:.2%}".format(n / series.size))


def isNumeric(series):
    """Check if the series's type is numeric

    Args:
        series (Series): data series

    Returns:
        bool
    """
    return series.dtype.kind in 'ifc'


def detect(dataframe):
    """ Detect data

    Args:
        dataframe (DataFrame): data that will be detected

    Returns:
        DataFrame: report of detecting
    """

    rows = []
    for name, series in dataframe.items():
        numeric_index = ['mean', 'std', 'min', '1%', '10%', '50%', '75%', '90%', '99%', 'max']
        discrete_index = ['top1', 'top2', 'top3', 'top4', 'top5', 'bottom5', 'bottom4', 'bottom3', 'bottom2', 'bottom1']

        details_index = [numeric_index[i] + '_or_' + discrete_index[i] for i in range(len(numeric_index))]
        details = []

        if isNumeric(series):
            desc = getDescribe(
                series,
                percentiles = [.01, .1, .5, .75, .9, .99]
            )
            details = desc.tolist()
        else:
            top5 = getTopValues(series)
            bottom5 = getTopValues(series, reverse = True)
            details = top5.tolist() + bottom5[::-1].tolist()

        # print(details_index)
        nblank, pblank = countBlank(series)

        row = pd.Series(
            index = ['type', 'size', 'missing', 'unique'] + details_index,
            data = [series.dtype, series.size, pblank, series.nunique()] + details
        )

        row.name = name
        rows.append(row)

    return pd.DataFrame(rows)
~~~

#### 分位数粗筛异常样本

~~~python

def detect_outliers(df, n, features):
    '''
    这种方法允许输入是nan值
    df: 表示dataFrame类型
    n: 表示被多少个特征判定为异常点才输出
    features: 表示检测异常点的特征
    '''
    # 用来保存异常点的索引
    outlier_indices = []
    for col in features:
        ## 第1个四分位点
        Q1 = np.percentile(df[col], 25)
        ## 第3个四分位点
        Q3 = np.percentile(df[col], 75)
        
        IQR = Q3 - Q1 
        
        lower_limit = Q1 - 1.5 * IQR  # 下界
        upper_limit = Q3 + 1.5 * IQR  # 上届
        
        outlier_list_col = df[(df[col]<lower_limit) | (df[col] > upper_limit)].index
        
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    # 如果被n个特征判定为异常点，则输出
    multiple_outliers = list(k for k, v in outlier_indices.items() if v > n)
    
    return multiple_outliers

# 决定如果处理异常点，可以丢弃
df = df.drop(outliers, axis=0).reset_index(drop=True)
~~~

#### learning curve

~~~python
def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,
                        train_sizes=np.linspace(.1, 1.0, 5)):
    """
    画出data在某模型上的learning curve.
    参数解释
    ----------
    estimator : 你用的分类器。
    title : 表格的标题。
    X : 输入的feature，numpy类型
    y : 输入的target vector
    ylim : 设定图像中纵坐标的最低点和最高点
    cv : 做cross-validation的时候，数据分成的份数，其中一份作为cv集，其余n-1份作为training(默认为3份)
    """

    plt.figure()
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=5, n_jobs=-1, train_sizes=train_sizes,scoring='roc_auc')
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)

    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std, alpha=0.1,
                     color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                     test_scores_mean + test_scores_std, alpha=0.1, color="g")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r",
             label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g",
             label="Cross-validation score")

    plt.xlabel("Training examples")
    plt.ylabel("Score")
    plt.legend(loc="best")
    plt.grid() 
    if ylim:
        plt.ylim(ylim)
    plt.title(title)
    plt.show()
    
~~~

#### kS 报告

~~~python
def plot_ks(preds, labels, n):
    from sklearn.metrics import roc_curve
    plt.style.use('seaborn')
    df = pd.DataFrame()   
    preds = np.array(preds)
    labels = np.array(labels)
    df['pred'] = np.array(preds)
    df['bad'] = np.array(labels)
    df['good'] = 1- df['bad']
    df.sort_values(by='pred', ascending=False,inplace=True)
    df.reset_index(drop=True,inplace=True)
    df['bins'] = pd.qcut(df.index, q=n, labels=range(1, n + 1))
    # TPR
    df['cumsum_bad_rate'] = df['bad'].cumsum() / df['bad'].sum()
    # FPR
    df['cumsum_good_rate'] = df['good'].cumsum() / df['good'].sum()
    df['ks'] = df['cumsum_bad_rate'] - df['cumsum_good_rate']
    #
    report = df.groupby(by='bins').agg({'bins':'count','bad': "sum", 'good': 'sum'})
    report.rename(columns={'bad': '负样本个数', 'good': '正样本个数','bins':'箱内样本数'}, inplace=True)
    report['累计正样本个数'] = report['正样本个数'].cumsum()
    report['累计负样本个数'] = report['负样本个数'].cumsum()
    report['好用户占比'] = report['正样本个数'] / report['正样本个数'].sum()
    report['坏用户占比'] = report['负样本个数'] / report['负样本个数'].sum()
    report.reset_index(inplace=True)
    report['due_rate_min'] = df.groupby('bins')['pred'].min().values
    report['due_rate_max'] = df.groupby('bins')['pred'].max().values
    report['due_rate_mean'] = df.groupby('bins')['pred'].mean().values
    report['cumsum_bad_rate'] = df.groupby('bins')['cumsum_bad_rate'].last().values
    report['cumsum_good_rate'] = df.groupby('bins')['cumsum_good_rate'].last().values
    

    report['ks'] = df.groupby(by='bins')['ks'].max().values

    fpr, tpr, _ = roc_curve(labels, preds)
    plt.plot(fpr, label='fpr',
             color='blue', linestyle='-', linewidth=2)
    plt.plot(tpr, label='trp',
             color='red', linestyle='-', linewidth=2)
    plt.plot(abs(fpr - tpr), label='ks',
             color='green', linestyle='-', linewidth=2)
    max_index = np.argmax(abs(tpr-fpr))
    max_ks = abs(tpr-fpr).max()
    bucket = report['ks'].argmax() +1
    plt.scatter([max_index,max_index],[tpr[max_index],fpr[max_index]],color='black')
    plt.axvline(x=max_index,color='gray',linestyle='--')
    
    plt.axhline(tpr[max_index], color='red', linestyle='--')
    plt.axhline(fpr[max_index],color='blue',linestyle='--')
    plt.axhline(max_ks,color='green',linestyle='--')
    plt.title('KS=%s ' % np.round(max_ks, 4) +
              'at threshold =%s ' % np.round(_[max_index],4)+'at bin =%d '%(bucket), fontsize=15)
#     plt.xlim(0,12)
#     plt.xticks(ticks=list(range(1,11)))
    plt.legend(loc='best')

    return report
~~~

##### stacking

```python
class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):
    def __init__(self, base_models, meta_model, n_folds=5):
        self.base_models = base_models
        self.meta_model = meta_model
        self.n_folds = n_folds
   
    # We again fit the data on clones of the original models
    def fit(self, X, y):
        self.base_models_ = [list() for x in self.base_models]
        self.meta_model_ = clone(self.meta_model)
        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)
        
        # Train cloned base models then create out-of-fold predictions
        # that are needed to train the cloned meta-model
        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))
        for i, model in enumerate(self.base_models):
            for train_index, holdout_index in kfold.split(X, y):
                instance = clone(model)
                self.base_models_[i].append(instance)
                instance.fit(X[train_index], y[train_index])
                y_pred = instance.predict(X[holdout_index])
                out_of_fold_predictions[holdout_index, i] = y_pred
                
        # Now train the cloned  meta-model using the out-of-fold predictions as new feature
        self.meta_model_.fit(out_of_fold_predictions, y)
        return self
   
    #Do the predictions of all base models on the test data and use the averaged predictions as 
    #meta-features for the final prediction which is done by the meta-model
    def predict(self, X):
        meta_features = np.column_stack([
            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)
            for base_models in self.base_models_ ])
        return self.meta_model_.predict(meta_features)
```

### PSI

~~~python
def cal_psi(actual, predict, bins=10):
    """
    功能: 计算PSI值，并输出实际和预期占比分布曲线
    :param actual: Array或series，代表真实数据，如训练集模型得分
    :param predict: Array或series，代表预期数据，如测试集模型得分
    :param bins: 分段数
    :return:
        psi: float，PSI值
        psi_df:DataFrame
    
    Examples
    -----------------------------------------------------------------
    >>> import random
    >>> act = np.array([random.random() for _ in range(5000000)])
    >>> pct = np.array([random.random() for _ in range(500000)])
    >>> psi, psi_df = cal_psi(act,pct)
    >>> psi
    1.65652278590053e-05
    >>> psi_df
       actual  predict  actual_rate  predict_rate           psi
    0  498285    49612     0.099657      0.099226  1.869778e-06
    1  500639    50213     0.100128      0.100428  8.975056e-07
    2  504335    50679     0.100867      0.101360  2.401777e-06
    3  493872    49376     0.098775      0.098754  4.296694e-09
    4  500719    49710     0.100144      0.099422  5.224199e-06
    5  504588    50691     0.100918      0.101384  2.148699e-06
    6  499988    50044     0.099998      0.100090  8.497110e-08
    7  496196    49548     0.099239      0.099098  2.016157e-07
    8  498963    50107     0.099793      0.100216  1.790906e-06
    9  502415    50020     0.100483      0.100042  1.941479e-06

    """
    actual_min = actual.min()  # 实际中的最小概率
    actual_max = actual.max()  # 实际中的最大概率
    binlen = (actual_max - actual_min) / bins
    cuts = [actual_min + i * binlen for i in range(1, bins)]#设定分组
    cuts.insert(0, -float("inf"))
    cuts.append(float("inf"))
    actual_cuts = np.histogram(actual, bins=cuts)#将actual等宽分箱
    predict_cuts = np.histogram(predict, bins=cuts)#将predict按actual的分组等宽分箱
    actual_df = pd.DataFrame(actual_cuts[0],columns=['actual'])
    predict_df = pd.DataFrame(predict_cuts[0], columns=['predict'])
    psi_df = pd.merge(actual_df,predict_df,right_index=True,left_index=True)
    psi_df['actual_rate'] = (psi_df['actual'] + 1) / psi_df['actual'].sum()#计算占比，分子加1，防止计算PSI时分子分母为0
    psi_df['predict_rate'] = (psi_df['predict'] + 1) / psi_df['predict'].sum()
    psi_df['psi'] = (psi_df['actual_rate'] - psi_df['predict_rate']) * np.log(
        psi_df['actual_rate'] / psi_df['predict_rate'])
    psi = psi_df['psi'].sum()
    return psi, psi_df
~~~

##### 检查缺失列是否是相同样本

~~~python
def check_nan_sample(train):
    nans_df = train.isna()
    nans_groups={}
    for col in train.columns:
        cur_group = nans_df[col].sum()
        try:
            nans_groups[cur_group].append(col)
        except:
            nans_groups[cur_group]=[col]
    del nans_df;gc.collect()

    for k,v in nans_groups.items():
        print('####### NAN count =',k)
        print(v)

def make_plots(Vs,train):
    col = 4
    row = len(Vs)//4+1
    plt.figure(figsize=(20,row*5))
    #非空的行index
    idx = train[~train[Vs[0]].isna()].index
    for i,v in enumerate(Vs):
        plt.subplot(row,col,i+1)
        n = train[v].nunique()
        x = np.sum(train.loc[idx,v]!=train.loc[idx,v].astype(int))
        y = np.round(100*np.sum(train[v].isna())/len(train),2)
        t = 'int'
        if x!=0: t = 'float'
        plt.title(v+' has '+str(n)+' '+t+' and '+str(y)+'% nan')
        plt.yticks([])
        #非nan的分布
        h = plt.hist(train.loc[idx,v],bins=100)
        if len(h[0])>1: plt.ylim((0,np.sort(h[0])[-2]))
    plt.show()

def check_nan(train,Vs,nan_number):
    from itertools import combinations, permutations
    cross_features = list(combinations(Vs, 2))
    for item in cross_features:
        tp=(train[item[0]]+train[item[1]]).isnull().sum()-nan_number
        print("check "+item[0]+' and '+item[1]+': '+str(tp))

~~~

