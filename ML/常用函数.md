### 构建[min,max,median,sum]等统计特征

```python
def agg_numeric(df, group_var, df_name):
    """Aggregates the numeric values in a dataframe. This can
 be used to create features for each instance of the grouping variable.

 Parameters
 --------
 df (dataframe): 
 the dataframe to calculate the statistics on
 group_var (string): 
 the variable by which to group df
 df_name (string): 
 the variable used to rename the columns

 Return
 --------
 agg (dataframe): 
 a dataframe with the statistics aggregated for 
 all numeric columns. Each instance of the grouping variable will have 
 the statistics (mean, min, max, sum; currently supported) calculated. 
 The columns are also renamed to keep track of features created.

 """
    # Remove id variables other than grouping variable
    for col in df:
        if col != group_var and 'SK_ID' in col:
            df = df.drop(columns = col)

    group_ids = df[group_var]
    numeric_df = df.select_dtypes('number')
    numeric_df[group_var] = group_ids

    # Group by the specified variable and calculate the statistics
    agg = numeric_df.groupby(group_var).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()

    # Need to create new column names
    columns = [group_var]

    # Iterate through the variables names
    for var in agg.columns.levels[0]:
        # Skip the grouping variable
        if var != group_var:
            # Iterate through the stat names
            for stat in agg.columns.levels[1][:-1]:
                # Make a new column name for the variable and stat
                columns.append('%s_%s_%s' % (df_name, var, stat))

    agg.columns = columns
    return agg
```

### 计算变量和目标变量之间的相关性
```python
def target_corrs(df):

    # List of correlations
    corrs = []

    # Iterate through the columns 
    for col in df.columns:
        print(col)
        # Skip the target column
        if col != 'TARGET':
            # Calculate correlation with the target
            corr = df['TARGET'].corr(df[col])

            # Append the list as a tuple
            corrs.append((col, corr))

    # Sort by absolute magnitude of correlations
    corrs = sorted(corrs, key = lambda x: abs(x[1]), reverse = True)

    return corrs
```

### 对类别型变量的一些衍生
```python
def count_categorical(df, group_var, df_name):
    """Computes counts and normalized counts for each observation
 of `group_var` of each unique category in every categorical variable

 Parameters
 --------
 df : dataframe 
 The dataframe to calculate the value counts for.

 group_var : string
 The variable by which to group the dataframe. For each unique
 value of this variable, the final dataframe will have one row

 df_name : string
 Variable added to the front of column names to keep track of columns

 Return
 --------
 categorical : dataframe
 A dataframe with counts and normalized counts of each unique category in every categorical variable
 with one row for every unique value of the `group_var`.

 """

    # Select the categorical columns
    categorical = pd.get_dummies(df.select_dtypes('object'))

    # Make sure to put the identifying id on the column
    categorical[group_var] = df[group_var]

    # Groupby the group var and calculate the sum and mean
    categorical = categorical.groupby(group_var).agg(['sum', 'mean'])

    column_names = []

    # Iterate through the columns in level 0
    for var in categorical.columns.levels[0]:
        # Iterate through the stats in level 1
        for stat in ['count', 'count_norm']:
            # Make a new column name
            column_names.append('%s_%s_%s' % (df_name, var, stat))

    categorical.columns = column_names

    return categorica
```
### 特征重要性
```python
def plot_feature_importances(df):
    """
 Plot importances returned by a model. This can work with any measure of
 feature importance provided that higher importance is better. 

 Args:
 df (dataframe): feature importances. Must have the features in a column
 called `features` and the importances in a column called `importance

 Returns:
 shows a plot of the 15 most importance features

 df (dataframe): feature importances sorted by importance (highest to lowest) 
 with a column for normalized importance
 """

    # Sort features according to importance
    df = df.sort_values('importance', ascending = False).reset_index()

    # Normalize the feature importances to add up to one
    df['importance_normalized'] = df['importance'] / df['importance'].sum()

    # Make a horizontal bar chart of feature importances
    plt.figure(figsize = (10, 6))
    ax = plt.subplot()

    # Need to reverse the index to plot most important on top
    ax.barh(list(reversed(list(df.index[:15]))), 
            df['importance_normalized'].head(15), 
            align = 'center', edgecolor = 'k')

    # Set the yticks and labels
    ax.set_yticks(list(reversed(list(df.index[:15]))))
    ax.set_yticklabels(df['feature'].head(15))

    # Plot labeling
    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')
    plt.show()

    return df
```
### 删除共性特征
```python
def identify_collinear(df,correlation_threshold):
    corr_matrix = df.corr()
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))
    to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]
    record_collinear = pd.DataFrame(columns = ['drop_feature', 'corr_feature', 'corr_value'])
    for column in to_drop:
     # Find the correlated features
        corr_features = list(upper.index[upper[column].abs() > correlation_threshold])

        # Find the correlated values
        corr_values = list(upper[column][upper[column].abs() > correlation_threshold])
        drop_features = [column for _ in range(len(corr_features))]    

        # Record the information (need a temp df for now)
        temp_df = pd.DataFrame.from_dict({'drop_feature': drop_features,
                                         'corr_feature': corr_features,
                                         'corr_value': corr_values})

        # Add to dataframe
        record_collinear = record_collinear.append(temp_df, ignore_index = True)
    print('%d features with a correlation magnitude greater than %0.2f.\n' % (len(to_drop),correlation_threshold)
    return record_collinear,to_drop

```

```python
import sys

"""
保存控制台信息到日志
"""
class Logger(object):
	def __init__(self, filename="Default.log"):
		self.terminal = sys.stdout
		self.log = open(filename, "a")

	def write(self, message):
		self.terminal.write(message)
		self.log.write(message)

	def flush(self):
		pass


sys.stdout = Logger("yourlogfilename.txt")
print("Hello world !")  # this is should be saved in yourlogfilename.txt
```

```python
#按照主键groupby
def group(df_to_agg, prefix, aggregations, aggregate_by= 'sk_id_curr'):
    """
     对每个表按照主键groupby
    """
    agg_df = df_to_agg.groupby(aggregate_by).agg(aggregations)
    agg_df.columns = pd.Index(['{}{}_{}'.format(prefix, e[0], e[1].lower())
                               for e in agg_df.columns.tolist()])
    return agg_df.reset_index()
#合并groupby之后的数据集
def group_and_merge(df_to_agg, df_to_merge, prefix, aggregations, aggregate_by= 'sk_id_curr'):
    """
    合并groupby后的数据集
    """
    agg_df = group(df_to_agg, prefix, aggregations, aggregate_by= aggregate_by)
    return df_to_merge.merge(agg_df, how='left', on= aggregate_by)


#减少内存的使用
def reduce_memory(df):
    """Reduce memory usage of a dataframe by setting data types. """
    start_mem = df.memory_usage().sum() / 1024 ** 2
    print('Initial df memory usage is {:.2f} MB for {} columns'
          .format(start_mem, len(df.columns)))

    for col in df.columns:
        col_type = df[col].dtypes
        if col_type != object:
            cmin = df[col].min()
            cmax = df[col].max()
            if str(col_type)[:3] == 'int':
                # Can use unsigned int here too
                if cmin > np.iinfo(np.int8).min and cmax < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif cmin > np.iinfo(np.int16).min and cmax < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif cmin > np.iinfo(np.int32).min and cmax < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif cmin > np.iinfo(np.int64).min and cmax < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)
            else:
                if cmin > np.finfo(np.float16).min and cmax < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif cmin > np.finfo(np.float32).min and cmax < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
    end_mem = df.memory_usage().sum() / 1024 ** 2
    memory_reduction = 100 * (start_mem - end_mem) / start_mem
    print('Final memory usage is: {:.2f} MB - decreased by {:.1f}%'.format(end_mem, memory_reduction))
    return df

```

**混淆矩阵**

```python
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt 
from sklearn.utils.multiclass import unique_labels

def plot_confusion_matrix(y_true, y_pred, classes=[0,1],
                          normalize=False,
                          title=None,
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    # Only use the labels that appear in the data
#     classes = classes[unique_labels(y_true, y_pred)]
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    fig, ax = plt.subplots()
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    # We want to show all ticks...
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           # ... and label them with the respective list entries
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')

    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")

    # Loop over data dimensions and create text annotations.
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()
    return ax


```
## 贝叶斯优化

```python

#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Date    : 2019-08-04 15:11:05
# @Author  : xuyinghao (xyh650209@163.com)
# @Link    : https://github.com/seyoulala/Basic-Algorithm
# @Version : $Id$

import numpy as np
import pandas as pd
import warnings
import hyperopt
from hyperopt import hp
from hyperopt.pyll.stochastic import sample
from hyperopt import tpe
from hyperopt import Trials
from hyperopt import fmin
from hyperopt import STATUS_OK
from hyperopt import partial
import gc
import csv
import os
import sys
import lightgbm as lgb
from lightgbm import LGBMClassifier, LGBMRegressor
from timeit import default_timer as timer
from sklearn.metrics import roc_auc_score, mean_squared_error



warnings.filterwarnings('ignore')



class AutoHyperparameter():

    def __init__(self, data, labels, max_iter=100, task='classifier', eval_set=None, out_of_file=None):
    	"""
		max_iter:搜索次数
		task:学习任务
		eval_set:额外指定的验证集
		out_of_file:日志文件

    	"""
        self.data = data
        self.labels = labels
        if self.labels is None:
            raise ValueError("labels is None please input lables")

        self.eval_set = eval_set
        if self.eval_set:
            if not isinstance(eval_set, tuple):
                raise ValueError("eval_set should be a tuple")
        # 交叉验证次数
        self.N_fold = 5
        self.ITERATION = 0
        self.eval_set = eval_set
        self.out_of_file = out_of_file
        self.task = task
        self.MAX_EVALS = max_iter

    def objective(self, hyperparameters):
        """
        :param hyperparameters:
        :return:

        建立需要优化的目标函数
        """
        self.ITERATION += 1

        if 'n_estimators' in hyperparameters:
            del hyperparameters['n_estimators']

        subsample = hyperparameters['boosting_type'].get('subsample', 1.0)
        hyperparameters['boosting_type'] = hyperparameters['boosting_type']['boosting_type']
        hyperparameters['subsample'] = subsample

        for parameter_name in ['num_leaves', 'min_child_samples']:
            hyperparameters[parameter_name] = int(
                hyperparameters[parameter_name])

        n_estimators = None
        start = timer()
        if not self.eval_set:
            train_set = lgb.Dataset(self.data, self.labels)
            if self.task == 'classifier':
                cv_result = lgb.cv(hyperparameters, train_set=train_set, num_boost_round=10000, stratified=True, nfold=self.N_fold,
                                   shuffle=True, early_stopping_rounds=50, verbose_eval=50,metrics='auc', seed=42)
                best_score = np.max(cv_result['auc-mean'])
                n_estimators = int(np.argmax(cv_result['auc-mean']) + 1)
                hyperparameters['n_estimators'] = n_estimators

            else:
                cv_result = lgb.cv(hyperparameters, train_set=train_set, num_boost_round=10000, stratified=False, nfold=self.N_fold,
                                   shuffle=True, early_stopping_rounds=50, verbose_eval=50,metrics='rmse', seed=42)
                best_score = np.min(cv_result['rmse-mean'])
                n_estimators = int(np.argmin(cv_result['rmse-mean']) + 1)
                hyperparameters['n_estimators'] = n_estimators
        else:
            if self.task == 'classifier':
                hyperparameters['n_estimators'] = 10000
                clf = LGBMClassifier(**hyperparameters)
                clf.fit(self.data, self.labels, eval_set=[(self.data,self.labels),self.eval_set],verbose=50,early_stopping_rounds=50)
                valid_pro = clf.predict_proba(
                    self.eval_set[0], num_iteration=clf.best_iteration_)[:, 1]
                best_score = roc_auc_score(self.eval_set[1], valid_pro)
                hyperparameters['n_estimators'] = clf.best_iteration_
                print(hyperparameters)
            else:
                hyperparameters['n_estimators'] = 10000
                reg = LGBMRegressor(**hyperparameters)
                reg.fit(self.data, self.labels, eval_set=[(self.data,self.labels),self.eval_set],verbose=50,
                        early_stopping_rounds=50, eval_metric='rmse')
                vaild_ = reg.predict(
                    self.eval_set[0], num_iteration=reg.best_iteration_)
                best_score = np.sqrt(mean_squared_error(eval_set[1], vaild_))
                hyperparameters['n_estimators'] = reg.best_iteration_

        runtime = timer() - start

        # 需要最小化的东西
        if self.task == 'classifier':
            loss = 1 - best_score
        else:
            loss = best_score

        # 将结果写入文件
        # out_file = "./bayes_iteration_{}.csv".format(self.MAX_EVALS)


        if not os.path.exists(os.path.abspath(self.out_of_file)):
            of_connection = open(self.out_of_file, 'w')
            writer = csv.writer(of_connection)
            headers = ['loss', 'hyperparameters',
                       'iteration', 'runtime', 'score']
            writer.writerow(headers)
            writer.writerow(
                [loss, hyperparameters, self.ITERATION, runtime, best_score])
            of_connection.close()
        else:
            of_connection = open(self.out_of_file, 'a')
            writer = csv.writer(of_connection)
            writer.writerow(
                [loss, hyperparameters, self.ITERATION, runtime, best_score])
            of_connection.close()

        return loss

    @staticmethod
    def get_hyperparameter_search_space():
        """
        定义搜索的参数空间
        :param self:
        :return:
        """
        space = {
            'boosting_type': hp.choice('boosting_type',
                                       [{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)},
                                        {'boosting_type': 'dart', 'subsample': hp.uniform(
                                            'dart_subsample', 0.5, 1)},
                                        {'boosting_type': 'goss', 'subsample': 1.0}]),
            'max_bin':hp.choice('max_bin',[63,128,255]),
            # 'max_depth':hp.quniform('max_depth',3,10,1),
            'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.5)),
            # 'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),
            'min_child_samples': hp.quniform('min_child_samples', 20, 100, 2),
            # 'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),
            # 'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),
            # 'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0),
        }
        return space

    @staticmethod
    def get_optimization_Algorithm():
        tpe_algorithm = partial(tpe.suggest,n_startup_jobs=5)

        return tpe_algorithm

    @staticmethod
    def get_result_history():
        trials = Trials()

    def run_optimization(self):
        fn = self.objective
        space = self.get_hyperparameter_search_space()
        algo = self.get_optimization_Algorithm()
        trials = self.get_result_history()
        max_evals = self.MAX_EVALS
        best = fmin(fn=fn, space=space, algo=algo,
                    trials=trials, max_evals=max_evals)
        return best


def fimpCal(df, feature_name, imp_name):
    feature_col = df[feature_name]
    first_list = []
    second_list = []
    first_list_imp = []
    second_list_imp = []
    for col in feature_col:
        tmp = col.split('_')
        first_list.append(tmp[0])
        second_list.append("_".join(tmp[:2]))
    first_list = list(set(first_list))
    second_list = list(set(second_list))
    for a in first_list:
        first_list_imp.append(df.loc[df['feature'].isin(
            list(filter(lambda x: x.startswith(a), feature_col))), imp_name].sum())
    for b in second_list:
        second_list_imp.append(df.loc[df['feature'].isin(
            list(filter(lambda x: x.startswith(b), feature_col))), imp_name].sum())
    tmp = pd.DataFrame()
    first_list = list(map(lambda x: x + "_total", first_list))
    second_list = list(map(lambda x: x + "_total", second_list))
    tmp[feature_name] = first_list + second_list
    tmp[imp_name] = first_list_imp + second_list_imp
    df = pd.concat([df, tmp], axis=0)
    df.reset_index(drop=True, inplace=True)

    return df


if __name__ == "__main__":
	logs = './xyh/data/bayes_seach_logs.csv'
	#额外指定验证集
	X_train, X_test, y_train, y_test = train_test_split(df[predictors],df['target'], test_size=0.33, random_state=42)
	#指定验证集
	bayes_opt = AutoHyperparameter(X_train,y_train,max_iter=100,eval_set=(X_test,y_test),out_of_file=logs)
	#不指定验证集,使用cv方式
	bayes_opt = AutoHyperparameter(X_train,y_train,max_iter=100,out_of_file=logs)

	#开始优化
	bayes_opt.run_optimization()

	result = pd.read_csv(logs)

	opt_parameter = result.sort_values(by='best_score',ascending=False).loc[0,'hyperparameters']
	print("The best parameter is {}".format(opt_parameter))

```



