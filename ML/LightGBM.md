# lightGBM

## Abstract 

传统的GBDT在面对大规模数据以及高维特征的时候效率和可扩展性就显得有些不足了,主要原因是因为在树的学习过程中,在分裂节点时候需要遍历所有数据所有的特征来计算增益已找到最优的分割点,这是非常耗时间的.为了解决这个问题,我们提出了两种新颖的技术:*Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB).* 在Goss中,我们排除了一些具有小梯度的样本实例(梯度越小,误差越小样本就被学的越好),只使用剩下的梯度较大的实例来计算增益.我们证明，这些梯度大的实例在计算信息增益中扮演重要角色，GOSS可以用更小的数据量对信息增益进行相当准确的估计（疑问：怎么证明，数学 or 实验？）。对于EFB，我们捆绑互斥的特征（什么是互斥特征：例如特征间很少同时非零。），来降低特征的个数。我们证明完美地捆绑互斥特征是NP难的，但贪心算法能够实现相当好的逼近率，因此我们能够在不损害分割点准确率许多，有效减少特征的数量。（牺牲一点分割准确率降低特征数量），这一算法命名为LightGBM。在多个公共数据集实验证明LightGBM加速了传统GBDT训练过程20倍以上，同时达到了几乎相同的精度。

**通过GOSS来减少样本数量,通过EFB来减少特征数量**


## 2 Preliminaries
### 2.1 GBDT and Its Complexity Analysis

GBDT是一基于树的集成模型,boosting的训练方式是串行的.在每一次iteration中,GBDT通过拟合loss function 对当前模型的负梯度来学习一个树.
GBDT训练时间主要是花在学习树上面,而学习一个树所花的时间主要花在寻找最优分割点上面.一种很流行的算法是预排序算法(pre-sorted),通过枚举出所有可能的分割点找到最佳的分割点.缺失就是大数据量情况下非常耗时间,而且也可能出现数据无法全部加载到内存中.另一种流行的方式是直方图算法.通过将特征连续值离散化后将连续特征映射到bin中.在训练中使用这些bin来构建feature的直方图.


基于直方图的寻找特征分割点算法如下图所示

<img src='/home/xyh/Pictures/gbm1.png'>

 算法cost$O(#data × #f eature)$ for histogram building and $O(#bin × #f eature)$ for
split point finding. bin的数量通常都是远小于data的数量的.通过这种方式可以加速GBDT的训练.

- 通常减少data数量的方法是通过下采样如果样本点有权重的话,但是在GBDT中,样本点是没有权重的.所以无法直接通过这种方式来减少数据量.

- 通常减少特征数量的很自然的想法就是过滤掉若弱特征.可以通过pca或者投影法来做到.但是这些方式都是假设特征之间存在冗余.通常数据集中特征都会尤独特的贡献,随便移除的话很可能会在某种程度上影响accuracy.

实际中大规模的数据集通常都是非常稀疏的，使用预排序算法的GBDT能够通过无视为0的特征来降低训练时间消耗。然后直方图算法没有优化稀疏的方案。因为直方图算法无论特征值是否为0，都需要为每个数据检索特征区间值。如果基于直方图的GBDT能够有效利用稀疏特征将是最优。

为了解决这些问题,ligtm通过goss和EBF来解决.


## 3 Gradient-based One-Side Sampling

goss是一种在减少数据量情况下并且能确保已学习的树有较好的accuracy的采样学习算法

<img src='/home/xyh/Pictures/gbm2.png' >

### 3.1 Algorithm Description

AdaBoost中，样本权重是数据实例重要性的指标。然而在GBDT中没有原始样本权重，不能应用权重采样。幸运的事，我们观察到GBDT中每个数据都有不同的梯度值，对采样十分有用，即实例的梯度小，实例训练误差也就较小，已经被学习得很好了，直接想法就是丢掉这部分梯度小的数据。然而这样做会改变数据的分布，将会影响训练的模型的精确度，为了避免此问题，我们提出了GOSS。

GOSS保留所有的梯度较大的实例，在梯度小的实例上使用随机采样。为了抵消对数据分布的影响，计算信息增益的时候，GOSS对小梯度的数据引入常量乘数。GOSS首先根据数据的梯度绝对值排序，选取top a个实例。然后在剩余的数据中随机采样b个实例。接着计算信息增益时为采样出的小梯度数据乘以(1-a)/b，这样算法就会更关注训练不足的实例，而不会过多改变原数据集的分布。

### 3.2 Theoretical Analysis

GBDT使用决策树来学习一个将输入空间映射到梯度空间的function.假设训练集有n个实例$x_1,x_2,....x_n$,特征维度为S,每次迭代是,当前损失函数对当前模型的负梯度表示为$g_1,g_2,g_3,...g_n$,对于GBDT,信息增益的衡量通常是使用节点分裂之后的方差.定义的方差函数如下所示:

<img src='/home/xyh/Pictures/gbm3.png'>

遍历每个特征,找到使方差函数最大的特征j,然后将数据根据特征j的分裂点d将数据分到左右节点中.

GOSS处理过程

- 首先,根据样本梯度的绝对值降序排序
- 然后,保留topa个大梯度的样本,作为数据集A
- 然后对于剩下的梯度较小的样本,随机采样出b个样本作为数据集B
- 最后在数据集A,B上使用自定义的方差函数来将样本分到各个节点.

因此在GOSS算中,我们使用采样后的样本点通过定义的方差函数来找到最佳分割点.这样就大大减少了样本的数量,加快了GBDT的训练速度



