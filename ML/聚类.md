# 无监督学习(聚类)
**聚类试图将数据集中的样本划分为k个互不相交的子集，每个子称为一个簇**

## 性能度量
聚类性能度量也称为有效性指标。通过指标来了解聚类的好坏，明确了指标就可将其作为优化目标。

性能度量大致有两类。
- 与某个参考模型对比，称为外部指标
- 直接考察聚类结果而不利用任何参考模型，称为内部指标。


![聚类 2](%E8%81%9A%E7%B1%BB%202.png)

基于式（9.1）~(9.4)可导出以下常用的聚类性能度量的外部指标

- Jaccard系数
$$
JC=\frac{a}{a+b+c}
$$

- FM指数
$$
FMI = \sqrt{\frac{a}{a+b}\cdot {\frac{a}{a+c}}}
$$

![聚类2](%E8%81%9A%E7%B1%BB3.png)

基于式（9.8）~(9.11)可导出以下常用的聚类性能度量的内部指标

$$
DBI =\frac{1}{k}\sum_{i=1}^k\max_{i \neq j}\left ( \frac{avg(C_i)+avg(c_j)}{d_{cen}{(u_i,u_j)}} \right)
$$

DB计算的任意两类别的类内平均距离之和除以两聚类中心聚类的最大值的平均。
**DB越小意味着类内距离越小，同时类间距离越大**

## 距离计算

给定样本$x_i=(x_{i1};x_{i2}....;x_{im})$ 与$x_j=(x_{j1};x_{j2}....;x_{jm})$最常用的闵可夫斯基距离。
$$
dist_{mk}(x_i,x_j) = \left( \sum_{u=1}^n|x_{iu}-x_{ju}|^p\right)^{\frac{1}{p}}
$$
对于有序属性可使用闵可夫斯基距离计算距离。但是对于离散属性中的如{飞机，火车，轮船}之类的属性则要使用VDM(Value Difference Metric)
$$
VDM_p(a,b) = \sum_{i=1}^k \left|\frac{m_{u,a,i}}{m_{u,a}} -\frac{m_{u,b,i}}{m_{u,b}} \right| ^p
$$
上式为在属性u上取值为a,b两个离散值之间的VDM距离.

### 常用算法介绍
#### kmeans

- 从数据集中随机选择K个样本作为初始的聚类中心$C={c_1,c_2,c_3...c_k}$
- 计算数据集中每个样本点xi到聚类中心的距离，并将其分到距离最小的聚类中心的簇中。
- 更新每个簇的中心.计算均值向量
- 重复上面两步，直到聚类中心位置不再变化

聚类中心的初始化对聚类效果有很大影响。因此提出了kmeans++。
#### kmeans ++
k个初始聚类中心相互之间应该分得越开越好。
- 从数据集中随机取一个样本点作为初始的聚类中心。
- 计算每个样本与当前已有聚类中心之间的最短距离（即与最近的一个聚类中心的距离），用$D(X)$表示，接着计算每个样本被选为下一个聚类中心的概率$\frac{D(x)^2}{\sum{_{x \in X}{D(x)^2}}}$,最后按照轮盘法选出下一个聚类中心
- 重复上一步 ，直到选出K个聚类中心
- 然后按照常规的Kmeans算法

下面结合一个简单的例子说明k-means++是如何选取聚类中心的。

![K-means++示例](https://images2015.cnblogs.com/blog/1024143/201701/1024143-20170111025944728-1116870094.png)

假设6号点被选为第一个初始聚类中心，那在进行步骤二时每个样本的$D(x)$和被选择成为第二个聚类中心的概率如下表所示:

![](https://images2015.cnblogs.com/blog/1024143/201701/1024143-20170111025946338-787569010.png)


其中的$P(x)$就是每个样本被选为下一个聚类中心的概率。最后一行的$sum$ 累加用于轮盘法选择出第二个聚类中心。方法是随机产生出一个0~1之间的随机数，判断它属于哪个区间，那么该区间对应的序号就是被选择出来的第二个聚类中心了。例如1号点的区间为[0,0.2)，2号点的区间为[0.2, 0.525)。

 从上表可以直观的看到第二个初始聚类中心是1号，2号，3号，4号中的一个的概率为0.9。而这4个点正好是离第一个初始聚类中心6号点较远的四个点。这也验证了K-means的改进思想：即离当前已有聚类中心较远的点有更大的概率被选为下一个聚类中心。可以看到，该例的K值取2是比较合适的。当K值大于2时，每个样本会有多个距离，需要取最小的那个距离作为$D(x)$
 
**总结**

1.优点
- 收敛速度快
- 实现简单，聚类效果也不错
- 容易解释

2.缺点
- 需要提前确定K值
- 对异常值敏感(更新簇心的时候是要做平均运算的) 
- 簇心的选取对聚类效果有很大影响容易收敛到局部最优(进行多次Kmeans取最优的来解决)