## PyTorch 基础 

##  损失函数(Loss Function)
损失函数（loss function）是用来估量模型的预测值(我们例子中的output)与真实值（例子中的y_train）的不一致程度，它是一个非负实值函数,损失函数越小，模型的鲁棒性就越好。
我们训练模型的过程，就是通过不断的迭代计算，使用梯度下降的优化算法，使得损失函数越来越小。损失函数越小就表示算法达到意义上的最优。

这里有一个重点：因为PyTorch是使用mini-batch来进行计算的，所以损失函数的计算出来的结果已经对mini-batch取了平均

常见（PyTorch内置）的损失函数有以下几个：
### nn.L1Loss:
输入x和目标y之间差的绝对值，要求 x 和 y 的维度要一样（可以是向量或者矩阵），得到的 loss 维度也是对应一样的
$$
loss(x,y)=1/n\sum|x_i-y_i|
$$


### nn.NLLLoss:
用于多分类的负对数似然损失函数
$$
loss(x, class) = -x[class]
$$
NLLLoss中如果传递了weights参数，会对损失进行加权，公式就变成了
$$
loss(x, class) = -weights[class] * x[class]
$$


### nn.MSELoss:
均方损失函数 ，输入x和目标y之间均方差
$$
loss(x,y)=1/n\sum(x_i-y_i)^2
$$

### nn.CrossEntropyLoss:
多分类用的交叉熵损失函数，LogSoftMax和NLLLoss集成到一个类中，会调用nn.NLLLoss函数,我们可以理解为CrossEntropyLoss()=log_softmax() + NLLLoss()
$$
\begin{aligned} loss(x, class) &= -\text{log}\frac{exp(x[class])}{\sum_j exp(x[j]))}\ &= -x[class] + log(\sum_j exp(x[j])) \end{aligned}
$$
 因为使用了NLLLoss，所以也可以传入weight参数，这时loss的计算公式变为：


$$
loss(x, class) = weights[class] * (-x[class] + log(\sum_j exp(x[j])))
$$
 所以一般多分类的情况会使用这个损失函数



### nn.BCELoss:
计算 x 与 y 之间的二进制交叉熵。

$$
loss(o,t)=-\frac{1}{n}\sum_i(t[i]* log(o[i])+(1-t[i])* log(1-o[i]))
$$
与NLLLoss类似，也可以添加权重参数： 

$$
loss(o,t)=-\frac{1}{n}\sum_iweights[i]* (t[i]* log(o[i])+(1-t[i])* log(1-o[i])) 
$$
用的时候需要在该层前面加上 Sigmoid 函数。





### pytorch函数的作用

------

pytorch中在rnn的batch中处理可变长序列的函数

`torch.nn.utils.rnn.pack_padded_sequence()`

`torch.nn.utils.rnn.pad_packed_sequence()`

[pytorch中处理变长序列](<https://zhuanlan.zhihu.com/p/34418001>)

结合上面两个函数来避免padding对句子表示的影响

`torch.squeeze()`

这个函数的作用是对数据的维度进行压缩。

`b = torch.squeeze(b)`意思就是将b中维度为1的维度删除

```pyton
t = torch.randn(10,1,4)
t.size()
torch.Size([10, 1, 4])
t.squeeze_()
tensor([[-1.2979,  1.0459,  0.8371, -1.1103],
        [-0.8757,  0.0768,  0.7131, -0.0699],
        [ 2.1082,  0.9104,  0.4271,  0.0503],
        [ 0.2596, -0.2568,  0.2504,  0.6831],
        [ 0.4804, -0.0536,  0.3152, -0.6436],
        [ 1.0560,  0.5040, -0.4237,  0.5763],
        [ 0.3579,  1.3508,  0.1080, -0.5490],
        [ 0.7102,  0.1416,  0.7483, -0.8637],
        [ 2.4638, -1.3936,  0.4531,  0.2021],
        [-0.7095,  1.4866,  0.5356,  0.0461]])
t.size()
torch.Size([10, 4])
```



`torch.unsqueeze()`

这个函数的作用是扩充维度

```python
t = torch.randn(10,4)
t.size()
torch.Size([10, 4])
t = t.unsqueeze(0)
t.size()
torch.Size([1, 10, 4])
```



### 拼接向量的一些方法

`torch.cat()`

对tensor的列表在指定的dim上进行拼接

`torch.Tensor.expand(*sizes)`

返回张量的一个新视图，可以讲张量的单个维度扩到更大的尺寸

在某个维度上传－１意味维度扩大不涉及这个维度

```
t = torch.randn(1,1,1)
t.size()
torch.Size([1, 1, 1])
t = t.expand(3,4,-1)
t.size()
torch.Size([3, 4, 1])
```



`torch.topk()`

返回一个tensor中topk个元素以及对应的索引

```
>>> x = torch.arange(1., 6.)
>>> x
tensor([ 1.,  2.,  3.,  4.,  5.])
>>> torch.topk(x, 3)
torch.return_types.topk(values=tensor([5., 4., 3.]), indices=tensor([4, 3, 2]))
```



`torch.mul(input, value, out=**None**)`

用标量值`value`乘以输入`input`的每个元素，并返回一个新的结果张量。 *o**u**t*=*t**e**n**s**o**r*∗*v**a**l**u*

```
>>> a = torch.randn(4,4)
>>> a

-0.7280  0.0598 -1.4327 -0.5825
-0.1427 -0.0690  0.0821 -0.3270
-0.9241  0.5110  0.4070 -1.1188
-0.8308  0.7426 -0.6240 -1.1582
[torch.FloatTensor of size 4x4]

>>> b = torch.randn(2, 8)
>>> b

 0.0430 -1.0775  0.6015  1.1647 -0.6549  0.0308 -0.1670  1.0742
-1.2593  0.0292 -0.0849  0.4530  1.2404 -0.4659 -0.1840  0.5974
[torch.FloatTensor of size 2x8]

>>> torch.mul(a, b)

-0.0313 -0.0645 -0.8618 -0.6784
 0.0934 -0.0021 -0.0137 -0.3513
 1.1638  0.0149 -0.0346 -0.5068
-1.0304 -0.3460  0.1148 -0.6919
[torch.FloatTensor of size 4x4]
```








