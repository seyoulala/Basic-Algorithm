###  Xgboost 近似算法

xgboost 的近似算法也是依据将连续特征离散化后构建直方图.xgboost这里构建直方图采用的策略是通过特征分布的百分位数找到分割点然后构建直方图,通过自定义的ranks函数(带权重的直方图)找到分割点的候选集合,将连续值映射到由分割点构建的bucket中,统计每个bucket中的梯度信息(一阶导数,二阶导数)来找到最佳的分割点.

#### rank function
$$
D_k = \lbrace (x_{1,k},h_1),(x_{2,k},h_2),(x_{3,k},h_3),...(x_{n,k},h_n) \rbrace
$$
$x_{n,k}$代表了第n个样本第K个特征值,$h_n$代表第n个样本的二阶导,定义以下rank function $r_k$: $R \to [0,+∞]$

$$
r_k(z) = \frac{1}{\sum_{(x,h)\in D_k}h}\sum_{(x,h)\in D_k,x \lt z}h
$$

表示了第k个特征值小于z的样本比例.

满足要求的特征百分位分割点需要满足以下要求

$$
 \left |r_k(s_{k,j})-r_k(s_{k,j+1}) \right| \lt \epsilon ,s_{k1} = \min_i{x_{ik}},s_{kl}=\max_i{x_{ik}}
 $$

也就是相邻的特征分割点的rank 函数值小于eps, eps是一个近似因子,直观上看,每个特征大概有1/eps 个候选分割点,同时每一个样本都是带权重的. 为什么二阶导能代表权重呢? 通过对目标函数重新配方,得到如下公式.

$$

\sum_{i=1}^n{\frac{1}{2}h_i(f_{t}(x_i)-\frac{g_i}{h_i})^2}+ \Omega(f_t) +constant
$$

现在还没有能处理样本带有权重的近视算法,所以xgboost作者提出了一种新的带权重的直方图算法来解决这个问题.通常思路是通过一种支持合并和修改操作的数据结构,同时确保每次操作accuracy都能保持一定的准确度.


#### Sparsity-aware Split Finding

xgboost的稀疏感知算法能处理大规模稀疏数据,相比较与没有考虑数据稀疏性的算法,其运行速度要快50倍.

<img src='https://github.com/seyoulala/Basic-Algorithm/blob/master/picture/xgboost.png'>

算法首先统计当前节点中所有样本的梯度信息,**然后遍历每个特征没有缺失的样本**,分别将在特征上有缺失的样本分到左右两边.通过评分函数选出划分后取得gain最大的方向作为默认的方向,之后在改特征上遇到缺失,就会默认的将改特征划分到之前学出来的方向.

#### Column Block for Parallel Learning

树学习算法大多时间都花在树生成上面,其中每次选择划分点又要对特征值排序花费的时间占了很大比例,xgboost通过将特征按列预排序保存成一种叫block的数据结构,贪心算法是将数据集保存在一个block中,近似算法是将数据集保存在多个block结构中.确定某一列的分割点可以并行运行.

保存成block结构的时间复杂度分析.

```
d表示树的最大深度,k表示树的颗数,对于贪心算法在原始稀疏空间中的时间复杂度为$O(kd||x||_{o}log_n)$,||x||表示在traindata中没有缺失值的样本.使用block结构后时间复杂度变成了$kd||x||_{o} + ||x||_{o}log_n$,对于近似算法时间复杂度为$O(Kd||x||_{0}log_q).$,q表示的候选分割点数量,通常在32-100之间
```
