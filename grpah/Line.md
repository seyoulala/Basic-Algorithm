### Line : Large-scale information network embedding

#### 想要解决什么问题?

deepwakl使用类似于dfs的随机游走策略采样得到顶点序列，然后使用skipgram的方式来训练节点的embedding。作者认为这种方式无法很好捕捉网络的结构特征。因此在Line中作者通过将顶点和顶点之间直接连接的边权定义为一阶相似度，边权越强节点之间的相似度越大比如图中的节点{6,7}。同时作者认为在现实的网络中，能够被直接观察到的有直接链接的节点属于少数。如果两个节点的邻居节点的重叠度很高的话如节点{5,6}，那么也能说明两个节点的相似度很高，因此作者将两个节点邻居节点的重叠度定义为二阶相似度。然后通过设计的损失函数使得算法能够捕捉到网络的局部结构和全局结构，使得最终学习到的embedding具有更强的表达能力。

同时作者希望一个好的embedding算法能做到以下三点:

1. 能够捕捉到节点之间的一阶相似度和二阶相似度。
2. 算法具备可扩展性，能够用于百万节点，10亿级别边的网络中。
3. 算法可处理任意类型的网络。如directed, undirected and/or weighted.

Line同时满足以上三个要求。

<img src="/Users/eason/Library/Application Support/typora-user-images/image-20201130155008578.png" alt="image-20201130155008578" style="zoom:50%;" />

#### 如何学习节点的embedding？





#### 和之前的方法有什么不同？

1.deepwalk没有定义一个很清晰的目标函数来保留网络的特性。直觉上deepwalk使用类dfs的搜索策略进行随机游走，希望捕捉到节点之间的二阶相似度。Line通过定义清晰的目标函数，使得算法能够同时优化顶点之间的一阶相似度和二阶相似度,从而保留节点之间的一阶相似度和二阶相似度。

2.deepwalk用于无向无权图，Line可处理任意类型的网络。

3.Line通过边采样的方式来构造训练样本。deepwalk是通过随机游走的方式来进行采样顶点序列

#### 模型细节

##### 一阶相似度

一阶相似度指在网络中顶点对之间的局部相似度。对于一条有向边$edge(i,j)$,我们定义它的联合概率分布为$p_{1}(v_i,v_j)=\frac{1}{1+exp(-u_i^T*u_j)}$

其中$u_i$表示的的顶点v_i的低维向量。同时还有一个经验分布，通过节点对之间的边权进行定义。$\hat{p_1}(v_i,v_j)=\frac{w_{ij}}{W}$,其中W是graph中所有边权的累加和，主要用作归一化。然后使用KL散度来计算两个分布的差异。通过最小化KL散度并去掉常数项，我们得到以下待优化的目标函数.如公式(2)，但是公式(2)中没有负样本，直接优化的话会造成某些向量的值异常的大，论文中有提到。为了避免这个问题，论文借鉴了word2vec中的负采样，从而得到公式(3)。一阶相似度只能用于无向图中。


$$
O_1=d(\hat{p_i}(.,.),p_i(.,.))
$$

$$
O_1=-\sum{w_{ij}}logp_1(v_i,v_j)
$$

$$
O_1=-\sum{w_{ij}}\cdot(\log\sigma(\vec{u}_j^T \cdot\vec{u}_i)+\sum_i^K{E_{v_n \backsim P_n(v)}[log\sigma(-\vec{u}_n^T \cdot\vec{u}_i)]})
$$

$$

$$



##### 二阶相似度

二阶相似度主要是为了建模图中没有直接相连的边，但是邻居节点的重合度很好的节点对。在这种情况下，一个顶点用两种embedding来进行表达，一种是顶点v_i作为中心节点时候的表达u_i，一种是顶点v_i作为其它顶点的邻居节点时候的表达$u_i^{,}$,针对每一个从节点i到j的有向边edge (i,j)，定义一个条件概率，如公式(5)所示，其中|V|是Graph中所有的节点数量，这其实是一个softmax函数。同样，还有一个经验概率，如公式(6)所示，N(i)是从节点i出发指向的邻居， ![[公式]](https://www.zhihu.com/equation?tex=d_i) 是权之和。同样需要计算条件概率和经验概率之间的距离，如公式(7)所示， ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda_i) 表示不同的节点有不同的重要程度。假设度比较高的节点权重较高，令 ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda_i%3Dd_i) ，采用KL散度来计算距离，略去常数项后，得到公式(8)。直接去优化公式(8)计算复杂度很高，每次迭代需要对所有的节点向量做优化，再次使用word2vec中的负采样方法，得到二阶近邻的优化目标，如公式(9)所示。从计算的过程可以看到，二阶近邻可以描述有向图。

![[公式]](https://www.zhihu.com/equation?tex=p_2%28v_j%7Cv_i%29%3D%5Cfrac+%7Bexp%28u_i+%5Ccdot+u_j%27%29%7D+%7B%5Csum_%7Bk%3D1%7D%5E%7B%7CV%7C%7D+exp%28u_i+%5Ccdot+u_k%27%29%7D+++++++%5Ctag%7B5%7D)

![[公式]](https://www.zhihu.com/equation?tex=%5Chat+p_2%28v_j%7Cv_i%29%3D%5Cfrac+%7Bw_%7Bij%7D%7D+%7Bd_i%7D%2C++++d_i%3D%5Csum_%7Bk%5Cin+N%28i%29%7Dw_%7Bik%7D++++++%5Ctag%7B6%7D)

![[公式]](https://www.zhihu.com/equation?tex=O_2%3D%5Csum%5Clambda_i+d%28%5Chat+p_2%28%5Ccdot+%7Cv_i%29%2C+p_2%28%5Ccdot%7Cv_i%29%29++++++++++%5Ctag%7B7%7D)

![[公式]](https://www.zhihu.com/equation?tex=O_2%3D-%5Csum+w_%7Bij%7Dlogp_2%28v_j%7Cv_i%29++++++++++++++++++++%5Ctag%7B8%7D)

![[公式]](https://www.zhihu.com/equation?tex=O_2%3D-%5Csum+w_%7Bij%7D+%5Ccdot+%28log%5Csigma%28u_i%2C+u_j%27%29+%2B+%5Csum+E_%7Bv_n%5Csim+P_n%28v%29%7D%5Blog+%5Csigma%28u_i%2C+-u_n%27%29%5D%29+++++%5Ctag%7B9%7D)

对比一阶近邻和二阶近邻的优化目标，差别就在于，二阶近邻对每个节点多引入了一个向量表示。实际使用的时候，对一阶近邻和二阶近邻分别训练，然后将两个向量拼接起来作为节点的向量表示。

##### Edge Sample

通过待优化的目标函数(4)还有(9)，在进行反向传播更新梯度的时候，边权会直接乘在梯度上面。这样使得学习率很难进行选择，如果选择较小的学习率，那么边权的样本学习的会很慢。如果选择较大的学习率，那么边权大的样本会使得梯度的更新过程中出现梯度爆炸的可能。因此论文中提出了Edge Sample。在采样的时候边的采样概率正比于边的权重。同时相同的边可以被重复采样到。采样的方法使用alias采样方法。

#####  Low degree vertices and New vertices.

在实际问题中对于低度节点，很难获取其准确的embedding表示，尤其是二阶相似度这种需要高度依赖于节点的领居节点。一种直观的想法就是给这些低度节点增加更高阶的邻居节点。比如添加邻居的邻居节点。在Line中，作者只考虑给低度节点添加二阶的邻居节点。

还有一个问题就是如何表示一个新的节点？第一种情况是如果这个节点和图中其它节点有能被观察到直接连接的边，那么通过优化一阶相似度或者二阶相似度的目标函数从而来更新该新来节点的embedding。在更新新节点的embedding的时候需要保持其它节点的embedding保持不变。第二种情况就是新来节点和图中节点没有直接连接的边的话，只能通过其它的信息来。作者将这个问题作为后续的工作。

#### 总结

1.Line优化了顶点之间的一阶相似度和二阶相似度，同时通过设计的损失函数，使得模型能够建模出顶点之间的这两种关系

2.提出edge sample的采样方式，主要是为了处理带权图中因为权重的大小差异造成的学习率选择难的问题以及解决在反向传播过程中因为梯度过大造成的学习不稳定的问题

