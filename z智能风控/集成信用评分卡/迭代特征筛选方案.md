1.solveKS函数

solveKS函数用来计算当前模型在某数据集上的KS，KS值对模型的评价不受样本不均衡问题的干扰，但是仅限于模型的评价。

>
>
>ks 计算方法
>
>- 计算每个评分区间的好坏账户数（计算的是特征的KS的话，是每个特征对应的好坏账户数）。
>- 计算每个评分区间的累计好账户数占总好账户数比率(good%)和累计坏账户数占总坏账户数比率(bad%)。
>- 计算每个评分区间累计坏账户占比与累计好账户占比差的绝对值（累计good%-累计bad%），然后对这些绝对值取最大值即得此评分卡的KS值。



~~~python

def solveKS(model,x,y,weight):
    """
    model : 当前使用的模型
    x： 训练数据
    y：训练标签
    weight ： 样本权重
    
    """
    import  numpy as np
    y_predict_pro = model.predict_proba(x)
    nrows = x.shape[0]
    #还原样本权重
    lis = [(y_predict_pro[i],y[i],weight[i]) for i in range(nrows)]
    #按照预测概率升序排序
    ks_lis = sorted(lis,key=lambda x:x[0],reverse=True)
    KS = []
    #计算所有坏样本数
    bad = sum([w for (p,y,w) in ks_lis if y>0.5])
    good = sum([w for (p,y,w) in ks_lis if y<=0.5])
    bad_cnt,good_cnt=0,0
    for (p,y,w) in ks_lis:
        if y>0.5:
            bad_cnt+=w
        else:
            good_cnt+=w
        ks = np.abs((bad_cnt/bad) - (good/good_cnt))
        KS.append(ks)
    return max(KS)            
~~~

### 计算训练集和验证集或者时间外样本集特征之间的psi

~~~python
def slovePSI(dev_col,val_col):
    """
    dev_col:参考分布
    val_col:实际分布
    """
    import numpy as np
    nrows = len(dev_col)
    dev_col = sorted(dev_col)
    ##等频分成10个箱子
    step =nrows//10
    #确定分割点
    cutpoint = [dev_col[i] for i in range(step-1,nrows,step)]
    #每一箱计算psi
    PSI=0
    for i in range(len(cutpoint)-1):
        start_point,end_point = cutpoint[i],cutpoint[i+1]
        dev_cnt = [value for value in dev_col if start_point<=value<end_point]
        val_cnt = [value for value in val_col if start_point<=value<end_point]
        dev_ratio = len(dev_cnt)/nrows +1e-10
        val_ratio = len(val_cnt)/nrows + 1e-10
        psi = (dev_ratio-val_ratio)*(np.log(dev_ratio/val_ratio))
        PSI+=psi
    return PSI
## 计算只有两箱的psi，箱中的值为0或者1
def make_psi_data(df:pd.DataFrame):
    dftot = pd.DataFrame()
    for col in df.columns:
        zero = sum(df[col]==0)
        one = sum(df[col]==1)
        ftdf = pd.DataFrame(np.array([zero,one]))
        ftdf.columns = [col]
        if len(dftot)==0:
            dftot = ftdf.copy()
        else:
            dftot[col] = ftdf[col].copy()
    return dftot

def var_psi(train_data,test_data):
    train_cnt,test_cnt = sum(train_data),sum(test_data)
    if train_cnt * test_data ==0:
        return 0
    PSI=0
    for i in range(len(train_data)):
        train_ratio = train_data[i] / train_cnt+1e-10
        test_ratio = test_cnt[i] / test_cnt+1e-10
        psi = (train_ratio - test_ratio)*np.log(train_ratio/test_ratio)
        PSI+=psi
    return PSI
  
def cal_psi(actual, predict, bins=10):
    """
    功能: 计算PSI值，并输出实际和预期占比分布曲线
    :param actual: Array或series，代表真实数据，如训练集模型得分
    :param predict: Array或series，代表预期数据，如测试集模型得分
    :param bins: 分段数
    :return:
        psi: float，PSI值
        psi_df:DataFrame
    
    Examples
    -----------------------------------------------------------------
    >>> import random
    >>> act = np.array([random.random() for _ in range(5000000)])
    >>> pct = np.array([random.random() for _ in range(500000)])
    >>> psi, psi_df = cal_psi(act,pct)
    >>> psi
    1.65652278590053e-05
    >>> psi_df
       actual  predict  actual_rate  predict_rate           psi
    0  498285    49612     0.099657      0.099226  1.869778e-06
    1  500639    50213     0.100128      0.100428  8.975056e-07
    2  504335    50679     0.100867      0.101360  2.401777e-06
    3  493872    49376     0.098775      0.098754  4.296694e-09
    4  500719    49710     0.100144      0.099422  5.224199e-06
    5  504588    50691     0.100918      0.101384  2.148699e-06
    6  499988    50044     0.099998      0.100090  8.497110e-08
    7  496196    49548     0.099239      0.099098  2.016157e-07
    8  498963    50107     0.099793      0.100216  1.790906e-06
    9  502415    50020     0.100483      0.100042  1.941479e-06

    """
    actual_min = actual.min()  # 实际中的最小概率
    actual_max = actual.max()  # 实际中的最大概率
    binlen = (actual_max - actual_min) / bins
    cuts = [actual_min + i * binlen for i in range(1, bins)]#设定分组
    cuts.insert(0, -float("inf"))
    cuts.append(float("inf"))
    actual_cuts = np.histogram(actual, bins=cuts)#将actual等宽分箱
    predict_cuts = np.histogram(predict, bins=cuts)#将predict按actual的分组等宽分箱
    actual_df = pd.DataFrame(actual_cuts[0],columns=['actual'])
    predict_df = pd.DataFrame(predict_cuts[0], columns=['predict'])
    psi_df = pd.merge(actual_df,predict_df,right_index=True,left_index=True)
    psi_df['actual_rate'] = (psi_df['actual'] + 1) / psi_df['actual'].sum()#计算占比，分子加1，防止计算PSI时分子分母为0
    psi_df['predict_rate'] = (psi_df['predict'] + 1) / psi_df['predict'].sum()
    psi_df['psi'] = (psi_df['actual_rate'] - psi_df['predict_rate']) * np.log(
        psi_df['actual_rate'] / psi_df['predict_rate'])
    psi = psi_df['psi'].sum()
    return psi, psi_df
~~~

### 删除对提升ks无效的特征

~~~python
def auto_delete_vars(dev_data,off_data,params,target):
    model = xgb.XGBClassifier(**params)
    col_names = [col for col in dev_data.columns if col !=target]
    model.fit(dev_data[col_names],dev_data[target])
    offks = solveKS(model,off_data[col_names],off_data[target])
    train_number =0
    print("train_number: %s, offks: %s"%(train_number,offks))
    del_list =[]
    oldks = offks
    while True:
        flag = True
        #遍历每个特征，将当前特征从特征列表中拿掉去掉一些无用的特征
        for col in col_names:
            model = xgb.XGBClassifier(**params)
            current_names =[var for var in col_names if var !=col]
            model.fit(dev_data[current_names],dev_data[target])
            train_number+=1
            offks = solveKS(model,off_data[current_names],off_data[target])
            #比较ks是否有变化
            if offks>=oldks:
                oldks=offks
                flag=False
                del_list.append(col)
                col_names = current_names
            else:
                continue
        if flag:
            break    
    return  col_names
~~~

## 概率转评分

~~~python
def score(pred):
    import  numpy as np
    score = 600+50*np.log2((1-pred)/pred)
    return score
~~~

### 根据业务来改写xgboost的评价函数

> 假设现在有一个提额模型，用处是给与分数最高的20%客户更高的额度，也就是期望分数最高的20%的客群正样本捕获率最大，同时保证模型对整体的正负样本有一定的区分能力。因此可以改写一个正样本捕获率的评价函数

~~~python
#自定义损失函数，需要提供函数函数的一阶导和二阶导
import numpy as np
import pandas as pd 
def loglikelood(preds,dtrain):
    labels = dtrain.get_label()
    preds = 1.0 /(1.0+np.exp(-preds))
    grad = preds -labels
    hess = preds *(1.0-preds)
    return  grad,hess


#字定义前20%正样本占比最大化的评价函数
def binary_error(preds,train_data):
    labels = train_data.get_label()
    dct = pd.DataFrame({'pred':preds,'percent':preds,'labels':labels})
    #取20%分位点上的值
    key = dct['percent'].quantile(q=0.2)
    dct['percent'] = dct['percent'].map(lambda x: 1 if x<=key else 0)
    #计算前20%样本中的损失
    result = np.mean(dct[dct['percent']==1]['labels']==1)*0.5 + \
        np.mean((dct['label']-preds)**2)*0.5
    return 'error',result
  
  

  	
~~~

