什么是冷启动？

​	冷启动是指在没有或只有很少量数据的情况下，从0到1建立业务模型的过程。

为什么要进行迁移学习？

​	在某一领域面对具体问题时，如果没有办法获取到大量的数据，同时很多算法在学习的时候需要大量的数据，因此这个时候利用一些迁移学习，可以让不同领域的知识互相借鉴。

3. 风控领域中会用到迁移学习的场景
   1. 新开了某个消费分期的场景，由于只有少量样本，需要用其他场景的数据来建模。此时其它场景知识为源域，新消费分期场景为目标域。
   2. 业务被迫停止3个月后项目重启，大部分训练样本比较老旧，新的训练样本又不够。此时大部分旧的样本为源域，新的少量样本为目标域
   3. 在某个国家开展了类似国内的业务，因为国情不同，部分特征分布也是不同的。此时有数据积累的国内数据为源域，新国家场景为目标域。

**总结一下：源域和目标域样本分布有区别，目标域样本量又不够，可以借鉴迁移学习的知识**



### 三类常见算法

1. 第一类算法：赋予源域中的样本某种权重，使其分布靠近目标域。如TrAdaBoost(Transfer AdaBoost)
2. 第二类算法：寻找一个低维子空间，使得源域和目标域的数据样本在映射到该子空间后服从相同或相近的分布。如联合分布适配算法(JDA)
3. 第三类算法：利用低秩矩阵重构数据点，实现域之间的鲁棒自适应。如迁移极限学习机(Domain Transfer Extreme Learning Machine)的域自适应算法。

第一类算法因为不在特征空间上做任何的扭曲变换，可以很好地保留模型的可解释性。第二三类算法虽然在解释性上有一定不足，但不需要在目标场景中有真实的样本标签，对于初期的业务支持度更好





### TraAdaboost

TraAdaboost算法用来解决**训练集和测试集分布不同的问题**。

在一个包含源域训练数据和目标域样本的训练集中，TraAdaBoost算法会对训练样本进行权重调整。对于目标域样本，如果被错误分类，根据目标域样本的分类错误率进行调整，增加其权重，使得下次训练的时候更关注这些被错分的目标域样本。对于源域样本，当其被误分类后，TraAdaboost算法则会认为它们是与目标数据不同分布，因此会降低其权重。

~~~python
def Tr_clf_boost(clf,trans_A,trans_T,label_A,label_T,test,label_test,N=500,early_stopping_rounds=100):
    """

    :param trans_A: 源域样本
    :param trans_T: 目标域样本
    :param label_A: 源域标签
    :param label_T: 目标域标签
    :param test: 测试样本
    :param label_test:
    :param N: 迭代次数
    :param early_stopping_rounds: 提前停止轮次
    
    在实际使用中需要权衡迭代次数，因为容易过拟合
    :return:
    """
    from  sklearn.metrics import  roc_curve

    #权重计算
    def calcuate_P(weights):
        total = np.sum(weights)
        return np.asarray(weights/total,order='C')

    #用一种基学习器，输出分类概率
    def train_classify(clf,train_data,train_label,test_data,p):
        model = clf
        model.fit(train_data,train_label,sample_weight=p[:,0])
        return model.predict_proba(test_data)[:,1],model

    def calcuateErrorRate(label_r,label_h,weight):
        total = np.sum(weight)
        return np.sum(weight[:,0]/total *np.abs(label_r-label_h))

    #根据分类器输出的proba得到标签
    def scoreToLabel(score_h,thread):
        new_label_h = []
        for i in score_h:
            if i<=thread:
                new_label_h.append(0)
            else:
                new_label_h.append(1)
        return new_label_h
    
    #拼接数据集
    train_data = pd.concat([trans_A,trans_T],axis=0)
    train_label = pd.concat([label_A,label_T],axis=0)
    
    row_A = trans_A.shape[0]
    row_T = trans_T.shape[0]
    row_test = test.shape[0]
    
    #将三个数据集合并
    test_data = pd.concat([train_data,test],axis=0)
    
    #初始化权重
    weights_A = np.ones([row_A,1])/row_A
    weights_T = np.ones([row_T,1])/row_T*2
    weights = np.concatenate((weights_A,weights_T),axis=0)
    
    #初始化beta
    beta = 1 / (1+np.sqrt(2*np.log(row_A/N)))
    
    #存储每一部迭代过程中的beta
    beta_N =np.ones([1,N])
    #存储每次迭代过程中的标签
    result_label = np.ones([row_A+row_T+row_test,N])
    train_data = np.asarray(train_data)
    train_label = np.asarray(train_label)
    test_data = np.asarray(test_data)

    best_ks =-1
    best_round=-1
    best_model = -1

    #开始迭代
    for i in range(N):
        p = calcuate_P(weights)
        result_label[:,i],model = train_classify(clf,train_data,train_label,test_data,p)
        #目标域样本得分
        score_h = result_label[row_A:row_A+row_T,i]
        #总体样本中正样本的占比
        pctg = np.sum(train_data['label'])/train_data.shape[0]
        thred = pd.DataFrame(score_h).quantile(1-pctg)[0]
        label_h = scoreToLabel(score_h,thred)

        #计算在目标域上的错误率
        error_rate = calcuateErrorRate(label_T,label_h,weights[row_A:row_A+row_T,:])

        if error_rate >0.5:
            error_rate = 0.5
        if error_rate==0:
            N = i
            break
        #记录bate值
        beta_N[:,i] = error_rate / (1-error_rate)

        #调整目标域样本权重
        for j in range(row_T):
            weights[row_A+j] = weights[row_A+j] * np.power(beta_N[:,i],(-np.abs(score_h[j] - label_T[j])))

        #调整源域样本权重
        for j in range(row_A):
            weights[j] = weights[j] * np.power(beta,np.abs(result_label[j,i] - label_A[j]))

        #测试集预测结果
        y_pred = result_label[(row_A+row_T):,i]
        fpr,tpr = roc_curve(label_test,y_pred)
        ks = abs(fpr-tpr).max()
        print("test ks: ",ks,"当前第 ",i+1,"轮")
        
        if ks > best_ks:
            best_ks = ks
            best_round = i
            best_model = model
        
        #如果ks在超过early_stopping_rounds还没有提升，那么就停止训练了
        if i - early_stopping_rounds > best_round:
            break
    return best_ks,best_round,best_model,early_stopping_rounds

~~~

