{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在运行dw相关表之前，首先运行下面部分以创建中间表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/thd/人行征信\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark import SparkContext,SQLContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "import os\n",
    "import sklearn\n",
    "print(os.path.abspath(os.curdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.eventLog.enabled\", \"false\") \\\n",
    "        .config(\"spark.executor.memory\", \"16g\")\\\n",
    "        .config(\"spark.driver.memory\", \"16g\")\\\n",
    "        .config(\"spark.cores.max\", \"10\")\\\n",
    "        .config(\"spark.task.maxFailures\", \"1000\")\\\n",
    "        .config(\"spark.default.parallelism\", \"500\")\\\n",
    "        .config(\"spark.sql.shuffle.partitions\",100)\\\n",
    "        .appName('renhang_etl') \\\n",
    "        .master('yarn')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建DW.dw_baseinfo_detail\n",
    "table1 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRMESSAGEHEADER\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table1.createOrReplaceTempView(\"ICRMESSAGEHEADER\")\n",
    "\n",
    "table2 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRCREDITCUE\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table2.createOrReplaceTempView(\"ICRCREDITCUE\")\n",
    "\n",
    "table3 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRLOANCARDINFO\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table3.createOrReplaceTempView(\"ICRLOANCARDINFO\")\n",
    "\n",
    "table4 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRCREDITCUE\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table4.createOrReplaceTempView(\"ICRCREDITCUE\")\n",
    "\n",
    "table5 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRLATEST2YEAROVERDUECARD\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table5.createOrReplaceTempView(\"ICRLATEST2YEAROVERDUECARD\")\n",
    "\n",
    "table6 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRLATEST5YEAROVERDUEDETAIL\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table6.createOrReplaceTempView(\"ICRLATEST5YEAROVERDUEDETAIL\")\n",
    "\n",
    "table7 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRLATEST2YEAROVERDUE\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table7.createOrReplaceTempView(\"ICRLATEST2YEAROVERDUE\")\n",
    "\n",
    "table8 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRLOANINFO\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table8.createOrReplaceTempView(\"ICRLOANINFO\")\n",
    "\n",
    "table9 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRUNDESTORYLOANCARD\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table9.createOrReplaceTempView(\"ICRUNDESTORYLOANCARD\")\n",
    "\n",
    "table10 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRUNDESTORYSTANDARDLOANCARD\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table10.createOrReplaceTempView(\"ICRUNDESTORYSTANDARDLOANCARD\")\n",
    "\n",
    "table11 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRUNPAIDLOAN\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table11.createOrReplaceTempView(\"ICRUNPAIDLOAN\")\n",
    "\n",
    "\n",
    "table12 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICROVERDUESUMMARY\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table12.createOrReplaceTempView(\"ICROVERDUESUMMARY\")\n",
    "\n",
    "\n",
    "table13 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRFELLBACKSUMMARY\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table13.createOrReplaceTempView(\"ICRFELLBACKSUMMARY\")\n",
    "\n",
    "\n",
    "table14 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRGUARANTEE\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table14.createOrReplaceTempView(\"ICRGUARANTEE\")\n",
    "\n",
    "\n",
    "table15 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRGUARANTEESUMMARY\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table15.createOrReplaceTempView(\"ICRGUARANTEESUMMARY\")\n",
    "\n",
    "\n",
    "#######\n",
    "\n",
    "table16 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRCIVILJUDGEMENT\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table16.createOrReplaceTempView(\"ICRCIVILJUDGEMENT\")\n",
    "\n",
    "\n",
    "\n",
    "table17 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRFORCEEXECUTION\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table17.createOrReplaceTempView(\"ICRFORCEEXECUTION\")\n",
    "\n",
    "\n",
    "table18 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRADMINPUNISHMENT\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table18.createOrReplaceTempView(\"ICRADMINPUNISHMENT\")\n",
    "\n",
    "\n",
    "table19 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRTAXARREAR\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table19.createOrReplaceTempView(\"ICRTAXARREAR\")\n",
    "\n",
    "\n",
    "table20 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRACCFUND\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table20.createOrReplaceTempView(\"ICRACCFUND\")\n",
    "\n",
    "\n",
    "\n",
    "table21 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRENDOWMENTINSURANCEDEPOSIT\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table21.createOrReplaceTempView(\"ICRENDOWMENTINSURANCEDEPOSIT\")\n",
    "\n",
    "\n",
    "table22 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRRECORDDETAIL\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table22.createOrReplaceTempView(\"ICRRECORDDETAIL\")\n",
    "\n",
    "\n",
    "\n",
    "table23 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRRECORDDETAIL\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table23.createOrReplaceTempView(\"ICRRECORDDETAIL\")\n",
    "\n",
    "\n",
    "table24 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.QUERY_HISTORY\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table24.createOrReplaceTempView(\"QUERY_HISTORY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select \n",
    "\t\tordue.reportno,\n",
    "\t\tordue.ACCOUNT,\n",
    "\t\thead.QUERYTIME,\n",
    "\t\tordue.MONTH,\n",
    "\t\tmonth(date_format(head.QUERYTIME,'%Y-%m-%d')) - month(date_format(concat(ordue.month,'.01'),'%Y-%m-%d')) as gap_months,\n",
    "\t\tcase \n",
    "\t\t\twhen ordue.LASTMONTHS in ('G','Z','D') then 8 \n",
    "\t\t\telse ordue.LASTMONTHS \n",
    "\t\tend as LASTMONTHS ,\n",
    "\t\tloanfo.GUARANTEETYPE,STATE,FINANCEORG\n",
    "from ICRLATEST2YEAROVERDUECARD ordue\n",
    "-- 只取贷记卡的信息\n",
    "inner join ICRLOANCARDINFO loanfo on ordue.reportno=loanfo.reportno and ordue.ACCOUNT=loanfo.ACCOUNT \n",
    "left join ICRMESSAGEHEADER head on ordue.reportno=head.reportno\n",
    "where BIZTYPE='准贷记卡' and ordue.LASTMONTHS!='C'\n",
    "\"\"\").createOrReplaceTempView(\"temp_standcard_2yearoverdue\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select \n",
    "\t\tordue.reportno,\n",
    "\t\tordue.ACCOUNT,\n",
    "\t\thead.QUERYTIME,\n",
    "\t\tordue.MONTH,\n",
    "\t\tmonth(date_format(head.QUERYTIME,'%Y-%m-%d')) - month(date_format(concat(ordue.month,'.01'),'%Y-%m-%d')) as gap_months,\n",
    "\t\tcase \n",
    "\t\t\twhen ordue.LASTMONTHS in ('G','Z','D') then 8 \n",
    "\t\t\telse ordue.LASTMONTHS \n",
    "\t\tend as LASTMONTHS ,\n",
    "\t\tloanfo.GUARANTEETYPE,STATE,FINANCEORG\n",
    "from ICRLATEST2YEAROVERDUECARD ordue\n",
    "-- 只取贷记卡的信息\n",
    "inner join ICRLOANCARDINFO loanfo on ordue.reportno=loanfo.reportno and ordue.ACCOUNT=loanfo.ACCOUNT \n",
    "left join ICRMESSAGEHEADER head on ordue.reportno=head.reportno\n",
    "where BIZTYPE='贷记卡' and ordue.LASTMONTHS!='C'\n",
    "\"\"\").createOrReplaceTempView(\"temp_creditcard_2yearoverdue\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select \n",
    "\t\tordue.reportno,\n",
    "\t\tordue.ACCOUNT,\n",
    "\t\thead.QUERYTIME,\n",
    "\t\tordue.MONTH,\n",
    "\t\tmonth(date_format(head.QUERYTIME,'%Y-%m-%d')) - month(date_format(concat(ordue.month,'.01'),'%Y-%m-%d')) as gap_months,\n",
    "\t\tordue.LASTMONTHS,ordue.AMOUNT,loanfo.GUARANTEETYPE,STATE,FINANCEORG\n",
    "from ICRLATEST5YEAROVERDUEDETAIL ordue\n",
    "-- 只取贷记卡的信息\n",
    "inner join ICRLOANCARDINFO loanfo on ordue.reportno=loanfo.reportno and ordue.ACCOUNT=loanfo.ACCOUNT \n",
    "left join ICRMESSAGEHEADER head on ordue.reportno=head.reportno\n",
    "where month!='--'  and BIZTYPE='贷记卡'\n",
    "\"\"\").createOrReplaceTempView(\"temp_creditcard_overdue\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select \n",
    "\t\treportno,\n",
    "\t\tACCOUNT,\n",
    "\t\tQUERYTIME,\n",
    "\t\tMONTH,\n",
    "\t\tgap_months,\n",
    "\t\tLASTMONTHS,\n",
    "\t\tGUARANTEETYPE,\n",
    "\t\tSTATE,\n",
    "\t\tFINANCEORG \n",
    "from temp_creditcard_2yearoverdue\n",
    "union \n",
    "select \n",
    "\t\treportno,\n",
    "\t\tACCOUNT,\n",
    "\t\tQUERYTIME,\n",
    "\t\tdate_format(concat(month,'.01'),'%Y-%m'),\n",
    "        gap_months,\n",
    "\t\tLASTMONTHS,\n",
    "\t\tGUARANTEETYPE,\n",
    "\t\tSTATE,\n",
    "\t\tFINANCEORG \n",
    "from temp_creditcard_overdue\n",
    "\"\"\").createOrReplaceTempView(\"temp_creditcard_5yearoverdue\")\n",
    "\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select \n",
    "\t\tordue.reportno,\n",
    "\t\tordue.ACCOUNT,\n",
    "\t\thead.QUERYTIME,\n",
    "\t\tordue.MONTH,\n",
    "\t\tmonth(date_format(head.QUERYTIME,'%Y-%m-%d')) - month(date_format(concat(ordue.month,'.01'),'%Y-%m-%d')) as gap_months,\n",
    "\t\tcase \n",
    "\t\t\twhen ordue.LASTMONTHS in ('G','Z','D') then 8 \n",
    "\t\t\telse ordue.LASTMONTHS \n",
    "\t\tend as LASTMONTHS ,\n",
    "\t\tloanfo.GUARANTEETYPE,\n",
    "\t\tSTATE,\n",
    "\t\tFINANCEORG\n",
    "from ICRLATEST2YEAROVERDUE ordue\n",
    "-- 只取贷款的信息\n",
    "inner join ICRLOANINFO loanfo on ordue.reportno=loanfo.reportno and ordue.ACCOUNT=loanfo.ACCOUNT \n",
    "left join ICRMESSAGEHEADER head on ordue.reportno=head.reportno\n",
    "where ordue.LASTMONTHS!='C'\n",
    "\"\"\").createOrReplaceTempView(\"temp_loan_2yearoverdue\")\n",
    "\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select \n",
    "\t\tordue.reportno,\n",
    "\t\tordue.ACCOUNT,\n",
    "\t\thead.QUERYTIME,\n",
    "\t\tordue.MONTH,\n",
    "\t\tmonth(date_format(head.QUERYTIME,'%Y-%m-%d')) - month(date_format(concat(ordue.month,'.01'),'%Y-%m-%d')) as gap_months,\n",
    "\t\tordue.LASTMONTHS,\n",
    "\t\tordue.AMOUNT,\n",
    "\t\tloanfo.GUARANTEETYPE,\n",
    "\t\tSTATE,FINANCEORG\n",
    "from ICRLATEST5YEAROVERDUEDETAIL ordue\n",
    "-- 只取贷款的信息\n",
    "inner join ICRLOANINFO loanfo on ordue.reportno=loanfo.reportno and ordue.ACCOUNT=loanfo.ACCOUNT \n",
    "left join ICRMESSAGEHEADER head on ordue.reportno=head.reportno\n",
    "where month!='--'\n",
    "\"\"\").createOrReplaceTempView(\"temp_loan_overdue\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select \n",
    "\t\treportno,\n",
    "\t\tACCOUNT,\n",
    "\t\tQUERYTIME,\n",
    "        MONTH,\n",
    "\t\tgap_months,\n",
    "\t\tLASTMONTHS,\n",
    "\t\tGUARANTEETYPE,\n",
    "\t\tSTATE,\n",
    "\t\tFINANCEORG \n",
    "from temp_loan_2yearoverdue\n",
    "union \n",
    "select \n",
    "\t\treportno,\n",
    "\t\tACCOUNT,\n",
    "\t\tQUERYTIME,\n",
    "\t\tdate_format(concat(month,'.01'),'%Y-%m'),gap_months,LASTMONTHS,\n",
    "\t\tGUARANTEETYPE,\n",
    "\t\tSTATE,\n",
    "\t\tFINANCEORG \n",
    "from temp_loan_overdue\n",
    "\"\"\").createOrReplaceTempView(\"temp_loan_5yearoverdue\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中间表已运行完成，spark sql 创建所需要的表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "select reportno from ICRMESSAGEHEADER\n",
    "where reportno not in(select distinct reportno from temp_creditcard_5yearoverdue)\n",
    "and reportno not in(select distinct reportno from temp_loan_5yearoverdue)\n",
    "and reportno not in(select distinct reportno from temp_standcard_2yearoverdue)\n",
    "\n",
    "\"\"\").createOrReplaceTempView(\"temp_never_overdue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "select reportno\n",
    "from ICRMESSAGEHEADER\n",
    "where reportno not in (select reportno from ICRLOANINFO)\n",
    "and reportno not in (select reportno from ICRLOANCARDINFO)\n",
    "\n",
    "\"\"\").createOrReplaceTempView(\"temp_without_account\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "\n",
    "select \n",
    "\t\thead.reportno,\n",
    "\t\tnvl(mth.max_gap_months,-9999999) as max_gap_months,\n",
    "\t\tnvl(mth.min_gap_months,-9999999) as min_gap_months,\n",
    "\t\tcase \n",
    "\t\t\twhen wthacc.reportno is not null then -9999999\n",
    "\t\t\twhen wthacc.reportno is null and nrod.reportno is not null then -9999998\n",
    "\t\t\telse all_mth1.mth1_gap_months \n",
    "\t\tend as mth1_gap_months,\n",
    "\t\tcase \n",
    "\t\t\twhen wthacc.reportno is not null then -9999999\n",
    "\t\t\twhen wthacc.reportno is null and nrod.reportno is not null then -9999998\n",
    "\t\t\telse all_mth2.mth2_gap_months \n",
    "\t\tend as mth2_gap_months,\t\n",
    "\t\tcase \n",
    "\t\t\twhen wthacc.reportno is not null then -9999999\n",
    "\t\t\twhen wthacc.reportno is null and nrod.reportno is not null then -9999998\n",
    "\t\t\telse all_mth3.mth3_gap_months \n",
    "\t\tend as mth3_gap_months,\t\t\n",
    "\t\tcase \n",
    "\t\t\twhen wthacc.reportno is not null then -9999999\n",
    "\t\t\twhen wthacc.reportno is null and nrod.reportno is not null then -9999998\n",
    "\t\t\telse all_mth4.mth4_gap_months \n",
    "\t\tend as mth4_gap_months,\t\n",
    "\t\tcase \n",
    "\t\t\twhen wthacc.reportno is not null then -9999999\n",
    "\t\t\twhen wthacc.reportno is null and nrod.reportno is not null then -9999998\n",
    "\t\t\telse all_mth5.mth5_gap_months \n",
    "\t\tend as mth5_gap_months,\t\n",
    "\t\t\n",
    "\t\tmonth(head.QUERYTIME)-month(max_qtime) as max_query_mth,\t\n",
    "\t\tday(head.QUERYTIME) - day(max_qtime) as max_query_day,\n",
    "\t\tnvl(loan.CREDITLIMITAMOUNT,-9999999) as max_loan,\n",
    "\t\tnvl(cloan.CREDITLIMITAMOUNT,-9999999) as max_cloan,\n",
    "\t\tnvl(sloan.CREDITLIMITAMOUNT,-9999999) as max_sloan,\n",
    "\t\tcase \n",
    "\t\t\twhen loan.CREDITLIMITAMOUNT is null then -9999999 \n",
    "\t\t\telse nvl(outl.out_num,0) \n",
    "\t\tend as out_loan_num,\n",
    "\tnow() as create_time,now() as update_time,now() as etl_date\n",
    "\t\n",
    "from ICRMESSAGEHEADER head\n",
    "\n",
    "left join(\n",
    "\n",
    "\t\tselect \n",
    "\t\t\treportno,\n",
    "\t\t\tmax(gap_months) as max_gap_months,\n",
    "\t\t\tmin(gap_months) as min_gap_months\n",
    "\t\tfrom \n",
    "\t\t(\n",
    "\t\t\tselect \n",
    "\t\t\t\tinfo.reportno,\n",
    "\t\t\t\tcase \n",
    "\t\t\t\t\twhen info.ENDDATE='-' then substr(PAYMENTCYC,1,length(PAYMENTCYC)-3)+0\n",
    "\t\t\t\t\twhen info.ENDDATE!='-' and day(date_format(info.ENDDATE,'%Y-%m-%d'))-day(date_format(head.QUERYTIME,'%Y-%m-%d'))<0 then month(info.ENDDATE )-month(info.OPENDATE)\n",
    "\t\t\t\t\telse month(head.QUERYTIME) - month(info.OPENDATE)\n",
    "\t\t\t\tend as gap_months\n",
    "\t\t\tfrom ICRLOANINFO info\n",
    "\n",
    "\t\t\tleft join ICRMESSAGEHEADER head on info.reportno=head.reportno  where PAYMENTCYC!='-' or state!='结清' or ENDDATE!='-' --去掉了and\n",
    "\t\t\n",
    "\t\t)t group by reportno\n",
    "\n",
    "\t)mth on head.reportno=mth.reportno\n",
    "\n",
    "left join temp_never_overdue nrod on nrod.reportno=head.reportno\n",
    "\n",
    "left join temp_without_account wthacc on wthacc.reportno=head.reportno\n",
    "\n",
    "left join \n",
    "(\n",
    "\t\tselect \n",
    "\t\t\tt1.reportno,\n",
    "\t\t\tmonth(date_format(head.QUERYTIME,'%Y-%m-%d'))-month(date_format(concat(t1.month,'.01'),'%Y-%m-%d')) as mth1_gap_months\n",
    "\t\tfrom (\n",
    "\t\t\tselect \n",
    "\t\t\t\treportno,\n",
    "\t\t\t\tmax(month) as month\n",
    "\t\t\tfrom \n",
    "\t\t\t(\n",
    "\t\t\t\tselect reportno,month,LASTMONTHS from temp_standcard_2yearoverdue\n",
    "\t\t\t\tunion all\n",
    "\t\t\t\tselect reportno,month,LASTMONTHS from temp_creditcard_5yearoverdue\n",
    "\t\t\t\tunion all\n",
    "\t\t\t\tselect reportno,month,LASTMONTHS from temp_loan_5yearoverdue\n",
    "\t\t\t\n",
    "\t\t\t)t   where LASTMONTHS>0 group by reportno\n",
    "\t\t)t1 \n",
    "\n",
    "\t\tleft join ICRMESSAGEHEADER head on t1.reportno=head.reportno\n",
    "\n",
    ") all_mth1 on all_mth1.reportno=head.reportno\n",
    "\n",
    "\n",
    "left join \n",
    "(\n",
    "select \n",
    "\t\tt1.reportno,\n",
    "\t\tmonth(date_format(head.QUERYTIME,'%Y-%m-%d')) - month(date_format(concat(t1.month,'.01'),'%Y-%m-%d')) as mth2_gap_months\n",
    "from (\n",
    "\tselect \n",
    "\t\treportno,\n",
    "\t\tmax(month) as month\n",
    "\tfrom \n",
    "\t\t(\n",
    "\t\t\tselect reportno,month,LASTMONTHS from temp_standcard_2yearoverdue\n",
    "\t\t\tunion all\n",
    "\t\t\tselect reportno,month,LASTMONTHS from temp_creditcard_5yearoverdue\n",
    "\t\t\tunion all\n",
    "\t\t\tselect reportno,month,LASTMONTHS from temp_loan_5yearoverdue\n",
    "\n",
    "\t\t)t where LASTMONTHS>1 group by reportno\n",
    "\t)t1 \n",
    "\n",
    "\tleft join ICRMESSAGEHEADER head on t1.reportno=head.reportno\n",
    "\n",
    ") all_mth2 on all_mth2.reportno=head.reportno\n",
    "\n",
    "left join \n",
    "(\n",
    "\tselect \n",
    "\t\tt1.reportno,\n",
    "\t\tmonth(date_format(head.QUERYTIME,'%Y-%m-%d'))- month(date_format(concat(t1.month,'.01'),'%Y-%m-%d'))  as mth3_gap_months\n",
    "\tfrom (\n",
    "\t\tselect \n",
    "\t\t\treportno,\n",
    "\t\t\tmax(month) as month\n",
    "\t\tfrom \n",
    "\t\t\t(\n",
    "\t\t\t\tselect reportno,month,LASTMONTHS from temp_standcard_2yearoverdue\n",
    "\t\t\t\tunion all\n",
    "\t\t\t\tselect reportno,month,LASTMONTHS from temp_creditcard_5yearoverdue\n",
    "\t\t\t\tunion all\n",
    "\t\t\t\tselect reportno,month,LASTMONTHS from temp_loan_5yearoverdue\n",
    "\t\t\n",
    "\t\t\t)t where LASTMONTHS>2 group by reportno\n",
    "\t\t)t1 \n",
    "\t\tleft join ICRMESSAGEHEADER head on t1.reportno=head.reportno\n",
    ") all_mth3 on all_mth3.reportno=head.reportno\n",
    "\n",
    "left join \n",
    "(\n",
    "\tselect \n",
    "\t\t\tt1.reportno,\n",
    "\t\t\tmonth(date_format(head.QUERYTIME,'%Y-%m-%d'))-month(date_format(concat(t1.month,'.01'),'%Y-%m-%d')) as mth4_gap_months\n",
    "\tfrom (\n",
    "\t\t\tselect \n",
    "\t\t\t\treportno,\n",
    "\t\t\t\tmax(month) as month\n",
    "\t\t\tfrom \n",
    "\t\t\t(\n",
    "\t\t\t\tselect reportno,month,LASTMONTHS from temp_standcard_2yearoverdue\n",
    "\t\t\t\tunion all\n",
    "\t\t\t\tselect reportno,month,LASTMONTHS from temp_creditcard_5yearoverdue\n",
    "\t\t\t\tunion all\n",
    "\t\t\t\tselect reportno,month,LASTMONTHS from temp_loan_5yearoverdue\n",
    "\n",
    "\t\t\t)t where LASTMONTHS>3 group by reportno\n",
    "\t\t)t1 \n",
    "\t\tleft join ICRMESSAGEHEADER head on t1.reportno=head.reportno\n",
    ") all_mth4 on all_mth4.reportno=head.reportno\n",
    "\n",
    "left join \n",
    "(\n",
    "\tselect \n",
    "\t\tt1.reportno,\n",
    "\t\tmonth(date_format(head.QUERYTIME,'%Y-%m-%d'))-month(date_format(concat(t1.month,'.01'),'%Y-%m-%d')) as mth5_gap_months\n",
    "\tfrom (\n",
    "\t\tselect \n",
    "\t\t\treportno,max(month) as month\n",
    "\t\t\tfrom \n",
    "\t\t\t(\n",
    "\t\t\t\tselect reportno,month,LASTMONTHS from temp_standcard_2yearoverdue\n",
    "\t\t\t\tunion all\n",
    "\t\t\t\tselect reportno,month,LASTMONTHS from temp_creditcard_5yearoverdue\n",
    "\t\t\t\tunion all\n",
    "\t\t\t\tselect reportno,month,LASTMONTHS from temp_loan_5yearoverdue\n",
    "\n",
    "\t\t\t)t where LASTMONTHS>4 group by reportno\n",
    "\t\t)t1 \n",
    "\t\tleft join ICRMESSAGEHEADER head on t1.reportno=head.reportno\n",
    ") all_mth5 on all_mth5.reportno=head.reportno\n",
    "\n",
    "left join \n",
    "\n",
    "(\n",
    "\tselect \n",
    "\t\t\treportno,\n",
    "\t\t\tmax(querytime) as max_qtime\n",
    "\tfrom QUERY_HISTORY\n",
    " \twhere reportno is not null \n",
    "\tgroup by reportno\n",
    ") max_q on max_q.reportno=head.reportno\n",
    "\n",
    "left join \n",
    "\n",
    "(\tselect \n",
    "\t\treportno,\n",
    "\t\tmax(CREDITLIMITAMOUNT) as CREDITLIMITAMOUNT \n",
    "\tfrom ICRLOANINFO\n",
    "\tgroup by reportno\n",
    "\n",
    ") loan on loan.reportno=head.reportno\n",
    "\n",
    "left join \n",
    "\n",
    "(\tselect \n",
    "\t\treportno,\n",
    "\t\tmax(CREDITLIMITAMOUNT) as CREDITLIMITAMOUNT \n",
    "\tfrom ICRLOANINFO\n",
    "\twhere GUARANTEETYPE='信用/免担保'\n",
    "\tgroup by reportno\n",
    ")cloan on cloan.reportno=head.reportno\n",
    "\n",
    "left join \n",
    "\n",
    "(\tselect \n",
    "\t\treportno,\n",
    "\t\tmax(CREDITLIMITAMOUNT) as CREDITLIMITAMOUNT \n",
    "\tfrom ICRLOANINFO\n",
    "\twhere GUARANTEETYPE='信用/免担保' and state!='结清'\n",
    "\tgroup by reportno\n",
    ") sloan on sloan.reportno=head.reportno\n",
    "\n",
    "left join\n",
    "\n",
    "(\tselect \n",
    "\t\treportno,\n",
    "\t\tcount(*)  as out_num \n",
    "\tfrom ICRLOANINFO\n",
    "\twhere CLASS5STATE='转出'\n",
    "\tgroup by reportno\n",
    ")outl on outl.reportno=head.reportno\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\").createOrReplaceTempView(\"dw_other_info2\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o78.sql.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:211)\n\tat org.apache.spark.sql.hive.execution.InsertIntoHiveTable.run(InsertIntoHiveTable.scala:337)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:66)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:61)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:84)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.hive.execution.CreateHiveTableAsSelectCommand.run(CreateHiveTableAsSelectCommand.scala:81)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:64)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:61)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:77)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:183)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:183)\n\tat org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:2841)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2840)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:183)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:68)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:632)\n\tat sun.reflect.GeneratedMethodAccessor41.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(reportno#3729, 100)\n+- *Project [REPORTNO#3729]\n   +- BroadcastNestedLoopJoin BuildLeft, LeftAnti, ((REPORTNO#3729 = reportno#3109) || isnull((REPORTNO#3729 = reportno#3109)))\n      :- BroadcastExchange IdentityBroadcastMode\n      :  +- BroadcastNestedLoopJoin BuildLeft, LeftAnti, ((REPORTNO#3729 = reportno#3133) || isnull((REPORTNO#3729 = reportno#3133)))\n      :     :- BroadcastExchange IdentityBroadcastMode\n      :     :  +- BroadcastNestedLoopJoin BuildLeft, LeftAnti, ((REPORTNO#3729 = reportno#3109) || isnull((REPORTNO#3729 = reportno#3109)))\n      :     :     :- BroadcastExchange IdentityBroadcastMode\n      :     :     :  +- *Scan JDBCRelation(ICR_PROD.ICRMESSAGEHEADER) [numPartitions=1] [REPORTNO#3729,QUERYTIME#3730,REPORTCREATETIME#3731] ReadSchema: struct<REPORTNO:string,QUERYTIME:string,REPORTCREATETIME:string>\n      :     :     +- *HashAggregate(keys=[reportno#3109], functions=[], output=[reportno#3109])\n      :     :        +- Exchange hashpartitioning(reportno#3109, 100)\n      :     :           +- *HashAggregate(keys=[reportno#3109], functions=[], output=[reportno#3109])\n      :     :              +- *HashAggregate(keys=[reportno#3109, ACCOUNT#3110, QUERYTIME#2986, MONTH#3111, gap_months#3594, LASTMONTHS#3595, GUARANTEETYPE#3040, STATE#3041, FINANCEORG#3036], functions=[], output=[reportno#3109])\n      :     :                 +- Exchange hashpartitioning(reportno#3109, ACCOUNT#3110, QUERYTIME#2986, MONTH#3111, gap_months#3594, LASTMONTHS#3595, GUARANTEETYPE#3040, STATE#3041, FINANCEORG#3036, 100)\n      :     :                    +- *HashAggregate(keys=[reportno#3109, ACCOUNT#3110, QUERYTIME#2986, MONTH#3111, gap_months#3594, LASTMONTHS#3595, GUARANTEETYPE#3040, STATE#3041, FINANCEORG#3036], functions=[], output=[reportno#3109, ACCOUNT#3110, QUERYTIME#2986, MONTH#3111, gap_months#3594, LASTMONTHS#3595, GUARANTEETYPE#3040, STATE#3041, FINANCEORG#3036])\n      :     :                       +- Union\n      :     :                          :- *Project [reportno#3109, ACCOUNT#3110, QUERYTIME#2986, MONTH#3111, (month(cast(date_format(cast(QUERYTIME#2986 as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date)) - month(cast(date_format(cast(concat(month#3111, .01) as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date))) AS gap_months#3594, CASE WHEN LASTMONTHS#3112 IN (G,Z,D) THEN 8 ELSE LASTMONTHS#3112 END AS LASTMONTHS#3595, GUARANTEETYPE#3040, STATE#3041, FINANCEORG#3036]\n      :     :                          :  +- SortMergeJoin [reportno#3109], [reportno#2985], LeftOuter\n      :     :                          :     :- *Sort [reportno#3109 ASC NULLS FIRST], false, 0\n      :     :                          :     :  +- Exchange hashpartitioning(reportno#3109, 100)\n      :     :                          :     :     +- *Project [reportno#3109, account#3110, month#3111, lastmonths#3112, FINANCEORG#3036, GUARANTEETYPE#3040, STATE#3041]\n      :     :                          :     :        +- *SortMergeJoin [reportno#3109, ACCOUNT#3110], [reportno#3031, ACCOUNT#3034], Inner\n      :     :                          :     :           :- *Sort [reportno#3109 ASC NULLS FIRST, ACCOUNT#3110 ASC NULLS FIRST], false, 0\n      :     :                          :     :           :  +- Exchange hashpartitioning(reportno#3109, ACCOUNT#3110, 100)\n      :     :                          :     :           :     +- *Scan JDBCRelation(ICR_PROD.ICRLATEST2YEAROVERDUECARD) [numPartitions=1] [reportno#3109,account#3110,month#3111,lastmonths#3112] PushedFilters: [*IsNotNull(lastmonths), *Not(EqualTo(lastmonths,C)), *IsNotNull(account)], ReadSchema: struct<reportno:string,account:string,month:string,lastmonths:string>\n      :     :                          :     :           +- *Sort [reportno#3031 ASC NULLS FIRST, ACCOUNT#3034 ASC NULLS FIRST], false, 0\n      :     :                          :     :              +- Exchange hashpartitioning(reportno#3031, ACCOUNT#3034, 100)\n      :     :                          :     :                 +- *Project [REPORTNO#3031, ACCOUNT#3034, FINANCEORG#3036, GUARANTEETYPE#3040, STATE#3041]\n      :     :                          :     :                    +- *Scan JDBCRelation(ICR_PROD.ICRLOANCARDINFO) [numPartitions=1] [ACCOUNT#3034,GUARANTEETYPE#3040,FINANCEORG#3036,REPORTNO#3031,STATE#3041] PushedFilters: [*IsNotNull(BIZTYPE), *EqualTo(BIZTYPE,贷记卡)], ReadSchema: struct<REPORTNO:string,ACCOUNT:string,FINANCEORG:string,GUARANTEETYPE:string,STATE:string>\n      :     :                          :     +- *Sort [reportno#2985 ASC NULLS FIRST], false, 0\n      :     :                          :        +- ReusedExchange [REPORTNO#2985, QUERYTIME#2986], Exchange hashpartitioning(reportno#2985, 100)\n      :     :                          +- *Project [reportno#3119, ACCOUNT#3120, QUERYTIME#2986, date_format(cast(concat(month#3122, .01) as timestamp), %Y-%m, Some(Asia/Shanghai)) AS date_format(CAST(concat(month, .01) AS TIMESTAMP), %Y-%m)#3620, (month(cast(date_format(cast(QUERYTIME#2986 as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date)) - month(cast(date_format(cast(concat(month#3122, .01) as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date))) AS gap_months#3607, cast(LASTMONTHS#3123 as string) AS LASTMONTHS#3621, GUARANTEETYPE#3040, STATE#3041, FINANCEORG#3036]\n      :     :                             +- SortMergeJoin [reportno#3119], [reportno#2985], LeftOuter\n      :     :                                :- *Sort [reportno#3119 ASC NULLS FIRST], false, 0\n      :     :                                :  +- Exchange hashpartitioning(reportno#3119, 100)\n      :     :                                :     +- *Project [REPORTNO#3119, ACCOUNT#3120, MONTH#3122, LASTMONTHS#3123, FINANCEORG#3036, GUARANTEETYPE#3040, STATE#3041]\n      :     :                                :        +- *SortMergeJoin [reportno#3119, ACCOUNT#3120], [reportno#3031, ACCOUNT#3034], Inner\n      :     :                                :           :- *Sort [reportno#3119 ASC NULLS FIRST, ACCOUNT#3120 ASC NULLS FIRST], false, 0\n      :     :                                :           :  +- Exchange hashpartitioning(reportno#3119, ACCOUNT#3120, 100)\n      :     :                                :           :     +- *Scan JDBCRelation(ICR_PROD.ICRLATEST5YEAROVERDUEDETAIL) [numPartitions=1] [REPORTNO#3119,ACCOUNT#3120,MONTH#3122,LASTMONTHS#3123] PushedFilters: [*IsNotNull(MONTH), *Not(EqualTo(MONTH,--)), *IsNotNull(ACCOUNT), *IsNotNull(REPORTNO)], ReadSchema: struct<REPORTNO:string,ACCOUNT:string,MONTH:string,LASTMONTHS:decimal(20,0)>\n      :     :                                :           +- *Sort [reportno#3031 ASC NULLS FIRST, ACCOUNT#3034 ASC NULLS FIRST], false, 0\n      :     :                                :              +- ReusedExchange [REPORTNO#3031, ACCOUNT#3034, FINANCEORG#3036, GUARANTEETYPE#3040, STATE#3041], Exchange hashpartitioning(reportno#3031, ACCOUNT#3034, 100)\n      :     :                                +- *Sort [reportno#2985 ASC NULLS FIRST], false, 0\n      :     :                                   +- ReusedExchange [REPORTNO#2985, QUERYTIME#2986], Exchange hashpartitioning(reportno#2985, 100)\n      :     +- *HashAggregate(keys=[reportno#3133], functions=[], output=[reportno#3133])\n      :        +- Exchange hashpartitioning(reportno#3133, 100)\n      :           +- *HashAggregate(keys=[reportno#3133], functions=[], output=[reportno#3133])\n      :              +- *HashAggregate(keys=[reportno#3133, ACCOUNT#3134, QUERYTIME#2986, MONTH#3135, gap_months#3633, LASTMONTHS#3634, GUARANTEETYPE#3166, STATE#3169, FINANCEORG#3160], functions=[], output=[reportno#3133])\n      :                 +- Exchange hashpartitioning(reportno#3133, ACCOUNT#3134, QUERYTIME#2986, MONTH#3135, gap_months#3633, LASTMONTHS#3634, GUARANTEETYPE#3166, STATE#3169, FINANCEORG#3160, 100)\n      :                    +- *HashAggregate(keys=[reportno#3133, ACCOUNT#3134, QUERYTIME#2986, MONTH#3135, gap_months#3633, LASTMONTHS#3634, GUARANTEETYPE#3166, STATE#3169, FINANCEORG#3160], functions=[], output=[reportno#3133, ACCOUNT#3134, QUERYTIME#2986, MONTH#3135, gap_months#3633, LASTMONTHS#3634, GUARANTEETYPE#3166, STATE#3169, FINANCEORG#3160])\n      :                       +- Union\n      :                          :- *Project [reportno#3133, ACCOUNT#3134, QUERYTIME#2986, MONTH#3135, (month(cast(date_format(cast(QUERYTIME#2986 as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date)) - month(cast(date_format(cast(concat(month#3135, .01) as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date))) AS gap_months#3633, CASE WHEN LASTMONTHS#3136 IN (G,Z,D) THEN 8 ELSE LASTMONTHS#3136 END AS LASTMONTHS#3634, GUARANTEETYPE#3166, STATE#3169, FINANCEORG#3160]\n      :                          :  +- SortMergeJoin [reportno#3133], [reportno#2985], LeftOuter\n      :                          :     :- *Sort [reportno#3133 ASC NULLS FIRST], false, 0\n      :                          :     :  +- Exchange hashpartitioning(reportno#3133, 100)\n      :                          :     :     +- *Project [reportno#3133, account#3134, month#3135, lastmonths#3136, FINANCEORG#3160, GUARANTEETYPE#3166, STATE#3169]\n      :                          :     :        +- *SortMergeJoin [reportno#3133, ACCOUNT#3134], [reportno#3156, ACCOUNT#3161], Inner\n      :                          :     :           :- *Sort [reportno#3133 ASC NULLS FIRST, ACCOUNT#3134 ASC NULLS FIRST], false, 0\n      :                          :     :           :  +- Exchange hashpartitioning(reportno#3133, ACCOUNT#3134, 100)\n      :                          :     :           :     +- *Scan JDBCRelation(ICR_PROD.ICRLATEST2YEAROVERDUE) [numPartitions=1] [reportno#3133,account#3134,month#3135,lastmonths#3136] PushedFilters: [*IsNotNull(lastmonths), *Not(EqualTo(lastmonths,C)), *IsNotNull(account)], ReadSchema: struct<reportno:string,account:string,month:string,lastmonths:string>\n      :                          :     :           +- *Sort [reportno#3156 ASC NULLS FIRST, ACCOUNT#3161 ASC NULLS FIRST], false, 0\n      :                          :     :              +- Exchange hashpartitioning(reportno#3156, ACCOUNT#3161, 100)\n      :                          :     :                 +- *Scan JDBCRelation(ICR_PROD.ICRLOANINFO) [numPartitions=1] [REPORTNO#3156,FINANCEORG#3160,ACCOUNT#3161,GUARANTEETYPE#3166,STATE#3169] ReadSchema: struct<REPORTNO:string,FINANCEORG:string,ACCOUNT:string,GUARANTEETYPE:string,STATE:string>\n      :                          :     +- *Sort [reportno#2985 ASC NULLS FIRST], false, 0\n      :                          :        +- ReusedExchange [REPORTNO#2985, QUERYTIME#2986], Exchange hashpartitioning(reportno#2985, 100)\n      :                          +- *Project [reportno#3119, ACCOUNT#3120, QUERYTIME#2986, date_format(cast(concat(month#3122, .01) as timestamp), %Y-%m, Some(Asia/Shanghai)) AS date_format(CAST(concat(month, .01) AS TIMESTAMP), %Y-%m)#3659, (month(cast(date_format(cast(QUERYTIME#2986 as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date)) - month(cast(date_format(cast(concat(month#3122, .01) as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date))) AS gap_months#3646, cast(LASTMONTHS#3123 as string) AS LASTMONTHS#3660, GUARANTEETYPE#3166, STATE#3169, FINANCEORG#3160]\n      :                             +- SortMergeJoin [reportno#3119], [reportno#2985], LeftOuter\n      :                                :- *Sort [reportno#3119 ASC NULLS FIRST], false, 0\n      :                                :  +- Exchange hashpartitioning(reportno#3119, 100)\n      :                                :     +- *Project [REPORTNO#3119, ACCOUNT#3120, MONTH#3122, LASTMONTHS#3123, FINANCEORG#3160, GUARANTEETYPE#3166, STATE#3169]\n      :                                :        +- *SortMergeJoin [reportno#3119, ACCOUNT#3120], [reportno#3156, ACCOUNT#3161], Inner\n      :                                :           :- *Sort [reportno#3119 ASC NULLS FIRST, ACCOUNT#3120 ASC NULLS FIRST], false, 0\n      :                                :           :  +- ReusedExchange [REPORTNO#3119, ACCOUNT#3120, MONTH#3122, LASTMONTHS#3123], Exchange hashpartitioning(reportno#3119, ACCOUNT#3120, 100)\n      :                                :           +- *Sort [reportno#3156 ASC NULLS FIRST, ACCOUNT#3161 ASC NULLS FIRST], false, 0\n      :                                :              +- ReusedExchange [REPORTNO#3156, FINANCEORG#3160, ACCOUNT#3161, GUARANTEETYPE#3166, STATE#3169], Exchange hashpartitioning(reportno#3156, ACCOUNT#3161, 100)\n      :                                +- *Sort [reportno#2985 ASC NULLS FIRST], false, 0\n      :                                   +- ReusedExchange [REPORTNO#2985, QUERYTIME#2986], Exchange hashpartitioning(reportno#2985, 100)\n      +- *HashAggregate(keys=[reportno#3109], functions=[], output=[reportno#3109])\n         +- *HashAggregate(keys=[reportno#3109], functions=[], output=[reportno#3109])\n            +- *Project [reportno#3109]\n               +- SortMergeJoin [reportno#3109], [reportno#2985], LeftOuter\n                  :- *Sort [reportno#3109 ASC NULLS FIRST], false, 0\n                  :  +- Exchange hashpartitioning(reportno#3109, 100)\n                  :     +- *Project [reportno#3109]\n                  :        +- *SortMergeJoin [reportno#3109, ACCOUNT#3110], [reportno#3031, ACCOUNT#3034], Inner\n                  :           :- *Sort [reportno#3109 ASC NULLS FIRST, ACCOUNT#3110 ASC NULLS FIRST], false, 0\n                  :           :  +- Exchange hashpartitioning(reportno#3109, ACCOUNT#3110, 100)\n                  :           :     +- *Project [reportno#3109, account#3110]\n                  :           :        +- *Scan JDBCRelation(ICR_PROD.ICRLATEST2YEAROVERDUECARD) [numPartitions=1] [reportno#3109,account#3110] PushedFilters: [*IsNotNull(lastmonths), *Not(EqualTo(lastmonths,C)), *IsNotNull(account)], ReadSchema: struct<reportno:string,account:string>\n                  :           +- *Sort [reportno#3031 ASC NULLS FIRST, ACCOUNT#3034 ASC NULLS FIRST], false, 0\n                  :              +- Exchange hashpartitioning(reportno#3031, ACCOUNT#3034, 100)\n                  :                 +- *Project [REPORTNO#3031, ACCOUNT#3034]\n                  :                    +- *Scan JDBCRelation(ICR_PROD.ICRLOANCARDINFO) [numPartitions=1] [REPORTNO#3031,ACCOUNT#3034] PushedFilters: [*IsNotNull(BIZTYPE), *EqualTo(BIZTYPE,准贷记卡)], ReadSchema: struct<REPORTNO:string,ACCOUNT:string>\n                  +- *Sort [reportno#2985 ASC NULLS FIRST], false, 0\n                     +- Exchange hashpartitioning(reportno#2985, 100)\n                        +- *Scan JDBCRelation(ICR_PROD.ICRMESSAGEHEADER) [numPartitions=1] [REPORTNO#2985] ReadSchema: struct<REPORTNO:string>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:173)\n\t... 34 more\nCaused by: java.util.concurrent.TimeoutException: Futures timed out after [300 seconds]\n\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:123)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:126)\n\tat org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec.doExecute(BroadcastNestedLoopJoinExec.scala:343)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:228)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:76)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:73)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:97)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-35e0f2bc0fc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drop table if EXISTS renhang_user_profile.%s \"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"create table renhang_user_profile.%s as select * from %s\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.0.1-1.cdh6.0.1.p0.590678/lib/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \"\"\"\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.0.1-1.cdh6.0.1.p0.590678/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.0.1-1.cdh6.0.1.p0.590678/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.0.1-1.cdh6.0.1.p0.590678/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o78.sql.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:211)\n\tat org.apache.spark.sql.hive.execution.InsertIntoHiveTable.run(InsertIntoHiveTable.scala:337)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:66)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:61)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:84)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.hive.execution.CreateHiveTableAsSelectCommand.run(CreateHiveTableAsSelectCommand.scala:81)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:64)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:61)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:77)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:183)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:183)\n\tat org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:2841)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2840)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:183)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:68)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:632)\n\tat sun.reflect.GeneratedMethodAccessor41.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(reportno#3729, 100)\n+- *Project [REPORTNO#3729]\n   +- BroadcastNestedLoopJoin BuildLeft, LeftAnti, ((REPORTNO#3729 = reportno#3109) || isnull((REPORTNO#3729 = reportno#3109)))\n      :- BroadcastExchange IdentityBroadcastMode\n      :  +- BroadcastNestedLoopJoin BuildLeft, LeftAnti, ((REPORTNO#3729 = reportno#3133) || isnull((REPORTNO#3729 = reportno#3133)))\n      :     :- BroadcastExchange IdentityBroadcastMode\n      :     :  +- BroadcastNestedLoopJoin BuildLeft, LeftAnti, ((REPORTNO#3729 = reportno#3109) || isnull((REPORTNO#3729 = reportno#3109)))\n      :     :     :- BroadcastExchange IdentityBroadcastMode\n      :     :     :  +- *Scan JDBCRelation(ICR_PROD.ICRMESSAGEHEADER) [numPartitions=1] [REPORTNO#3729,QUERYTIME#3730,REPORTCREATETIME#3731] ReadSchema: struct<REPORTNO:string,QUERYTIME:string,REPORTCREATETIME:string>\n      :     :     +- *HashAggregate(keys=[reportno#3109], functions=[], output=[reportno#3109])\n      :     :        +- Exchange hashpartitioning(reportno#3109, 100)\n      :     :           +- *HashAggregate(keys=[reportno#3109], functions=[], output=[reportno#3109])\n      :     :              +- *HashAggregate(keys=[reportno#3109, ACCOUNT#3110, QUERYTIME#2986, MONTH#3111, gap_months#3594, LASTMONTHS#3595, GUARANTEETYPE#3040, STATE#3041, FINANCEORG#3036], functions=[], output=[reportno#3109])\n      :     :                 +- Exchange hashpartitioning(reportno#3109, ACCOUNT#3110, QUERYTIME#2986, MONTH#3111, gap_months#3594, LASTMONTHS#3595, GUARANTEETYPE#3040, STATE#3041, FINANCEORG#3036, 100)\n      :     :                    +- *HashAggregate(keys=[reportno#3109, ACCOUNT#3110, QUERYTIME#2986, MONTH#3111, gap_months#3594, LASTMONTHS#3595, GUARANTEETYPE#3040, STATE#3041, FINANCEORG#3036], functions=[], output=[reportno#3109, ACCOUNT#3110, QUERYTIME#2986, MONTH#3111, gap_months#3594, LASTMONTHS#3595, GUARANTEETYPE#3040, STATE#3041, FINANCEORG#3036])\n      :     :                       +- Union\n      :     :                          :- *Project [reportno#3109, ACCOUNT#3110, QUERYTIME#2986, MONTH#3111, (month(cast(date_format(cast(QUERYTIME#2986 as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date)) - month(cast(date_format(cast(concat(month#3111, .01) as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date))) AS gap_months#3594, CASE WHEN LASTMONTHS#3112 IN (G,Z,D) THEN 8 ELSE LASTMONTHS#3112 END AS LASTMONTHS#3595, GUARANTEETYPE#3040, STATE#3041, FINANCEORG#3036]\n      :     :                          :  +- SortMergeJoin [reportno#3109], [reportno#2985], LeftOuter\n      :     :                          :     :- *Sort [reportno#3109 ASC NULLS FIRST], false, 0\n      :     :                          :     :  +- Exchange hashpartitioning(reportno#3109, 100)\n      :     :                          :     :     +- *Project [reportno#3109, account#3110, month#3111, lastmonths#3112, FINANCEORG#3036, GUARANTEETYPE#3040, STATE#3041]\n      :     :                          :     :        +- *SortMergeJoin [reportno#3109, ACCOUNT#3110], [reportno#3031, ACCOUNT#3034], Inner\n      :     :                          :     :           :- *Sort [reportno#3109 ASC NULLS FIRST, ACCOUNT#3110 ASC NULLS FIRST], false, 0\n      :     :                          :     :           :  +- Exchange hashpartitioning(reportno#3109, ACCOUNT#3110, 100)\n      :     :                          :     :           :     +- *Scan JDBCRelation(ICR_PROD.ICRLATEST2YEAROVERDUECARD) [numPartitions=1] [reportno#3109,account#3110,month#3111,lastmonths#3112] PushedFilters: [*IsNotNull(lastmonths), *Not(EqualTo(lastmonths,C)), *IsNotNull(account)], ReadSchema: struct<reportno:string,account:string,month:string,lastmonths:string>\n      :     :                          :     :           +- *Sort [reportno#3031 ASC NULLS FIRST, ACCOUNT#3034 ASC NULLS FIRST], false, 0\n      :     :                          :     :              +- Exchange hashpartitioning(reportno#3031, ACCOUNT#3034, 100)\n      :     :                          :     :                 +- *Project [REPORTNO#3031, ACCOUNT#3034, FINANCEORG#3036, GUARANTEETYPE#3040, STATE#3041]\n      :     :                          :     :                    +- *Scan JDBCRelation(ICR_PROD.ICRLOANCARDINFO) [numPartitions=1] [ACCOUNT#3034,GUARANTEETYPE#3040,FINANCEORG#3036,REPORTNO#3031,STATE#3041] PushedFilters: [*IsNotNull(BIZTYPE), *EqualTo(BIZTYPE,贷记卡)], ReadSchema: struct<REPORTNO:string,ACCOUNT:string,FINANCEORG:string,GUARANTEETYPE:string,STATE:string>\n      :     :                          :     +- *Sort [reportno#2985 ASC NULLS FIRST], false, 0\n      :     :                          :        +- ReusedExchange [REPORTNO#2985, QUERYTIME#2986], Exchange hashpartitioning(reportno#2985, 100)\n      :     :                          +- *Project [reportno#3119, ACCOUNT#3120, QUERYTIME#2986, date_format(cast(concat(month#3122, .01) as timestamp), %Y-%m, Some(Asia/Shanghai)) AS date_format(CAST(concat(month, .01) AS TIMESTAMP), %Y-%m)#3620, (month(cast(date_format(cast(QUERYTIME#2986 as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date)) - month(cast(date_format(cast(concat(month#3122, .01) as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date))) AS gap_months#3607, cast(LASTMONTHS#3123 as string) AS LASTMONTHS#3621, GUARANTEETYPE#3040, STATE#3041, FINANCEORG#3036]\n      :     :                             +- SortMergeJoin [reportno#3119], [reportno#2985], LeftOuter\n      :     :                                :- *Sort [reportno#3119 ASC NULLS FIRST], false, 0\n      :     :                                :  +- Exchange hashpartitioning(reportno#3119, 100)\n      :     :                                :     +- *Project [REPORTNO#3119, ACCOUNT#3120, MONTH#3122, LASTMONTHS#3123, FINANCEORG#3036, GUARANTEETYPE#3040, STATE#3041]\n      :     :                                :        +- *SortMergeJoin [reportno#3119, ACCOUNT#3120], [reportno#3031, ACCOUNT#3034], Inner\n      :     :                                :           :- *Sort [reportno#3119 ASC NULLS FIRST, ACCOUNT#3120 ASC NULLS FIRST], false, 0\n      :     :                                :           :  +- Exchange hashpartitioning(reportno#3119, ACCOUNT#3120, 100)\n      :     :                                :           :     +- *Scan JDBCRelation(ICR_PROD.ICRLATEST5YEAROVERDUEDETAIL) [numPartitions=1] [REPORTNO#3119,ACCOUNT#3120,MONTH#3122,LASTMONTHS#3123] PushedFilters: [*IsNotNull(MONTH), *Not(EqualTo(MONTH,--)), *IsNotNull(ACCOUNT), *IsNotNull(REPORTNO)], ReadSchema: struct<REPORTNO:string,ACCOUNT:string,MONTH:string,LASTMONTHS:decimal(20,0)>\n      :     :                                :           +- *Sort [reportno#3031 ASC NULLS FIRST, ACCOUNT#3034 ASC NULLS FIRST], false, 0\n      :     :                                :              +- ReusedExchange [REPORTNO#3031, ACCOUNT#3034, FINANCEORG#3036, GUARANTEETYPE#3040, STATE#3041], Exchange hashpartitioning(reportno#3031, ACCOUNT#3034, 100)\n      :     :                                +- *Sort [reportno#2985 ASC NULLS FIRST], false, 0\n      :     :                                   +- ReusedExchange [REPORTNO#2985, QUERYTIME#2986], Exchange hashpartitioning(reportno#2985, 100)\n      :     +- *HashAggregate(keys=[reportno#3133], functions=[], output=[reportno#3133])\n      :        +- Exchange hashpartitioning(reportno#3133, 100)\n      :           +- *HashAggregate(keys=[reportno#3133], functions=[], output=[reportno#3133])\n      :              +- *HashAggregate(keys=[reportno#3133, ACCOUNT#3134, QUERYTIME#2986, MONTH#3135, gap_months#3633, LASTMONTHS#3634, GUARANTEETYPE#3166, STATE#3169, FINANCEORG#3160], functions=[], output=[reportno#3133])\n      :                 +- Exchange hashpartitioning(reportno#3133, ACCOUNT#3134, QUERYTIME#2986, MONTH#3135, gap_months#3633, LASTMONTHS#3634, GUARANTEETYPE#3166, STATE#3169, FINANCEORG#3160, 100)\n      :                    +- *HashAggregate(keys=[reportno#3133, ACCOUNT#3134, QUERYTIME#2986, MONTH#3135, gap_months#3633, LASTMONTHS#3634, GUARANTEETYPE#3166, STATE#3169, FINANCEORG#3160], functions=[], output=[reportno#3133, ACCOUNT#3134, QUERYTIME#2986, MONTH#3135, gap_months#3633, LASTMONTHS#3634, GUARANTEETYPE#3166, STATE#3169, FINANCEORG#3160])\n      :                       +- Union\n      :                          :- *Project [reportno#3133, ACCOUNT#3134, QUERYTIME#2986, MONTH#3135, (month(cast(date_format(cast(QUERYTIME#2986 as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date)) - month(cast(date_format(cast(concat(month#3135, .01) as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date))) AS gap_months#3633, CASE WHEN LASTMONTHS#3136 IN (G,Z,D) THEN 8 ELSE LASTMONTHS#3136 END AS LASTMONTHS#3634, GUARANTEETYPE#3166, STATE#3169, FINANCEORG#3160]\n      :                          :  +- SortMergeJoin [reportno#3133], [reportno#2985], LeftOuter\n      :                          :     :- *Sort [reportno#3133 ASC NULLS FIRST], false, 0\n      :                          :     :  +- Exchange hashpartitioning(reportno#3133, 100)\n      :                          :     :     +- *Project [reportno#3133, account#3134, month#3135, lastmonths#3136, FINANCEORG#3160, GUARANTEETYPE#3166, STATE#3169]\n      :                          :     :        +- *SortMergeJoin [reportno#3133, ACCOUNT#3134], [reportno#3156, ACCOUNT#3161], Inner\n      :                          :     :           :- *Sort [reportno#3133 ASC NULLS FIRST, ACCOUNT#3134 ASC NULLS FIRST], false, 0\n      :                          :     :           :  +- Exchange hashpartitioning(reportno#3133, ACCOUNT#3134, 100)\n      :                          :     :           :     +- *Scan JDBCRelation(ICR_PROD.ICRLATEST2YEAROVERDUE) [numPartitions=1] [reportno#3133,account#3134,month#3135,lastmonths#3136] PushedFilters: [*IsNotNull(lastmonths), *Not(EqualTo(lastmonths,C)), *IsNotNull(account)], ReadSchema: struct<reportno:string,account:string,month:string,lastmonths:string>\n      :                          :     :           +- *Sort [reportno#3156 ASC NULLS FIRST, ACCOUNT#3161 ASC NULLS FIRST], false, 0\n      :                          :     :              +- Exchange hashpartitioning(reportno#3156, ACCOUNT#3161, 100)\n      :                          :     :                 +- *Scan JDBCRelation(ICR_PROD.ICRLOANINFO) [numPartitions=1] [REPORTNO#3156,FINANCEORG#3160,ACCOUNT#3161,GUARANTEETYPE#3166,STATE#3169] ReadSchema: struct<REPORTNO:string,FINANCEORG:string,ACCOUNT:string,GUARANTEETYPE:string,STATE:string>\n      :                          :     +- *Sort [reportno#2985 ASC NULLS FIRST], false, 0\n      :                          :        +- ReusedExchange [REPORTNO#2985, QUERYTIME#2986], Exchange hashpartitioning(reportno#2985, 100)\n      :                          +- *Project [reportno#3119, ACCOUNT#3120, QUERYTIME#2986, date_format(cast(concat(month#3122, .01) as timestamp), %Y-%m, Some(Asia/Shanghai)) AS date_format(CAST(concat(month, .01) AS TIMESTAMP), %Y-%m)#3659, (month(cast(date_format(cast(QUERYTIME#2986 as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date)) - month(cast(date_format(cast(concat(month#3122, .01) as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date))) AS gap_months#3646, cast(LASTMONTHS#3123 as string) AS LASTMONTHS#3660, GUARANTEETYPE#3166, STATE#3169, FINANCEORG#3160]\n      :                             +- SortMergeJoin [reportno#3119], [reportno#2985], LeftOuter\n      :                                :- *Sort [reportno#3119 ASC NULLS FIRST], false, 0\n      :                                :  +- Exchange hashpartitioning(reportno#3119, 100)\n      :                                :     +- *Project [REPORTNO#3119, ACCOUNT#3120, MONTH#3122, LASTMONTHS#3123, FINANCEORG#3160, GUARANTEETYPE#3166, STATE#3169]\n      :                                :        +- *SortMergeJoin [reportno#3119, ACCOUNT#3120], [reportno#3156, ACCOUNT#3161], Inner\n      :                                :           :- *Sort [reportno#3119 ASC NULLS FIRST, ACCOUNT#3120 ASC NULLS FIRST], false, 0\n      :                                :           :  +- ReusedExchange [REPORTNO#3119, ACCOUNT#3120, MONTH#3122, LASTMONTHS#3123], Exchange hashpartitioning(reportno#3119, ACCOUNT#3120, 100)\n      :                                :           +- *Sort [reportno#3156 ASC NULLS FIRST, ACCOUNT#3161 ASC NULLS FIRST], false, 0\n      :                                :              +- ReusedExchange [REPORTNO#3156, FINANCEORG#3160, ACCOUNT#3161, GUARANTEETYPE#3166, STATE#3169], Exchange hashpartitioning(reportno#3156, ACCOUNT#3161, 100)\n      :                                +- *Sort [reportno#2985 ASC NULLS FIRST], false, 0\n      :                                   +- ReusedExchange [REPORTNO#2985, QUERYTIME#2986], Exchange hashpartitioning(reportno#2985, 100)\n      +- *HashAggregate(keys=[reportno#3109], functions=[], output=[reportno#3109])\n         +- *HashAggregate(keys=[reportno#3109], functions=[], output=[reportno#3109])\n            +- *Project [reportno#3109]\n               +- SortMergeJoin [reportno#3109], [reportno#2985], LeftOuter\n                  :- *Sort [reportno#3109 ASC NULLS FIRST], false, 0\n                  :  +- Exchange hashpartitioning(reportno#3109, 100)\n                  :     +- *Project [reportno#3109]\n                  :        +- *SortMergeJoin [reportno#3109, ACCOUNT#3110], [reportno#3031, ACCOUNT#3034], Inner\n                  :           :- *Sort [reportno#3109 ASC NULLS FIRST, ACCOUNT#3110 ASC NULLS FIRST], false, 0\n                  :           :  +- Exchange hashpartitioning(reportno#3109, ACCOUNT#3110, 100)\n                  :           :     +- *Project [reportno#3109, account#3110]\n                  :           :        +- *Scan JDBCRelation(ICR_PROD.ICRLATEST2YEAROVERDUECARD) [numPartitions=1] [reportno#3109,account#3110] PushedFilters: [*IsNotNull(lastmonths), *Not(EqualTo(lastmonths,C)), *IsNotNull(account)], ReadSchema: struct<reportno:string,account:string>\n                  :           +- *Sort [reportno#3031 ASC NULLS FIRST, ACCOUNT#3034 ASC NULLS FIRST], false, 0\n                  :              +- Exchange hashpartitioning(reportno#3031, ACCOUNT#3034, 100)\n                  :                 +- *Project [REPORTNO#3031, ACCOUNT#3034]\n                  :                    +- *Scan JDBCRelation(ICR_PROD.ICRLOANCARDINFO) [numPartitions=1] [REPORTNO#3031,ACCOUNT#3034] PushedFilters: [*IsNotNull(BIZTYPE), *EqualTo(BIZTYPE,准贷记卡)], ReadSchema: struct<REPORTNO:string,ACCOUNT:string>\n                  +- *Sort [reportno#2985 ASC NULLS FIRST], false, 0\n                     +- Exchange hashpartitioning(reportno#2985, 100)\n                        +- *Scan JDBCRelation(ICR_PROD.ICRMESSAGEHEADER) [numPartitions=1] [REPORTNO#2985] ReadSchema: struct<REPORTNO:string>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:173)\n\t... 34 more\nCaused by: java.util.concurrent.TimeoutException: Futures timed out after [300 seconds]\n\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:123)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:126)\n\tat org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec.doExecute(BroadcastNestedLoopJoinExec.scala:343)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:228)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:76)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:73)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:97)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "table_name = \"dw_other_info2\"\n",
    "\n",
    "spark.sql(\"drop table if EXISTS renhang_user_profile.%s \"%table_name)\n",
    "spark.sql(\"create table renhang_user_profile.%s as select * from %s\"%(table_name,table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
