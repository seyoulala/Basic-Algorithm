{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在运行dw相关表之前，首先运行下面部分以创建中间表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/thd/人行征信\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark import SparkContext,SQLContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "import os\n",
    "import sklearn\n",
    "print(os.path.abspath(os.curdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.eventLog.enabled\", \"false\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\")\\\n",
    "        .config(\"spark.driver.memory\", \"8g\")\\\n",
    "        .config(\"spark.cores.max\", \"10\")\\\n",
    "        .config(\"spark.task.maxFailures\", \"1000\")\\\n",
    "        .config(\"spark.default.parallelism\", \"500\")\\\n",
    "        .config(\"spark.sql.shuffle.partitions\",100)\\\n",
    "        .appName('renhang_etl') \\\n",
    "        .master('yarn')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建DW.dw_baseinfo_detail\n",
    "table1 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRMESSAGEHEADER\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table1.createOrReplaceTempView(\"ICRMESSAGEHEADER\")\n",
    "\n",
    "table2 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRCREDITCUE\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table2.createOrReplaceTempView(\"ICRCREDITCUE\")\n",
    "\n",
    "table3 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRLOANCARDINFO\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table3.createOrReplaceTempView(\"ICRLOANCARDINFO\")\n",
    "\n",
    "table4 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRCREDITCUE\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table4.createOrReplaceTempView(\"ICRCREDITCUE\")\n",
    "\n",
    "table5 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRLATEST2YEAROVERDUECARD\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table5.createOrReplaceTempView(\"ICRLATEST2YEAROVERDUECARD\")\n",
    "\n",
    "table6 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRLATEST5YEAROVERDUEDETAIL\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table6.createOrReplaceTempView(\"ICRLATEST5YEAROVERDUEDETAIL\")\n",
    "\n",
    "table7 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRLATEST2YEAROVERDUE\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table7.createOrReplaceTempView(\"ICRLATEST2YEAROVERDUE\")\n",
    "\n",
    "table8 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRLOANINFO\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table8.createOrReplaceTempView(\"ICRLOANINFO\")\n",
    "\n",
    "table9 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRUNDESTORYLOANCARD\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table9.createOrReplaceTempView(\"ICRUNDESTORYLOANCARD\")\n",
    "\n",
    "table10 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRUNDESTORYSTANDARDLOANCARD\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table10.createOrReplaceTempView(\"ICRUNDESTORYSTANDARDLOANCARD\")\n",
    "\n",
    "table11 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRUNPAIDLOAN\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table11.createOrReplaceTempView(\"ICRUNPAIDLOAN\")\n",
    "\n",
    "\n",
    "table12 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICROVERDUESUMMARY\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table12.createOrReplaceTempView(\"ICROVERDUESUMMARY\")\n",
    "\n",
    "\n",
    "table13 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRFELLBACKSUMMARY\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table13.createOrReplaceTempView(\"ICRFELLBACKSUMMARY\")\n",
    "\n",
    "\n",
    "table14 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRGUARANTEE\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table14.createOrReplaceTempView(\"ICRGUARANTEE\")\n",
    "\n",
    "\n",
    "table15 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRGUARANTEESUMMARY\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table15.createOrReplaceTempView(\"ICRGUARANTEESUMMARY\")\n",
    "\n",
    "\n",
    "#######\n",
    "\n",
    "table16 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRCIVILJUDGEMENT\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table16.createOrReplaceTempView(\"ICRCIVILJUDGEMENT\")\n",
    "\n",
    "\n",
    "\n",
    "table17 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRFORCEEXECUTION\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table17.createOrReplaceTempView(\"ICRFORCEEXECUTION\")\n",
    "\n",
    "\n",
    "table18 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRADMINPUNISHMENT\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table18.createOrReplaceTempView(\"ICRADMINPUNISHMENT\")\n",
    "\n",
    "\n",
    "table19 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRTAXARREAR\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table19.createOrReplaceTempView(\"ICRTAXARREAR\")\n",
    "\n",
    "\n",
    "table20 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRACCFUND\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table20.createOrReplaceTempView(\"ICRACCFUND\")\n",
    "\n",
    "\n",
    "\n",
    "table21 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRENDOWMENTINSURANCEDEPOSIT\") \\\n",
    "    .option(\"user\", \"etl\") \\\n",
    "    .option(\"password\", \"%TGRDJh3Hg2e4f\") \\\n",
    "    .load()\n",
    "table21.createOrReplaceTempView(\"ICRENDOWMENTINSURANCEDEPOSIT\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select \n",
    "\t\tordue.reportno,\n",
    "\t\tordue.ACCOUNT,\n",
    "\t\thead.QUERYTIME,\n",
    "\t\tordue.MONTH,\n",
    "\t\tmonth(date_format(head.QUERYTIME,'%Y-%m-%d')) - month(date_format(concat(ordue.month,'.01'),'%Y-%m-%d')) as gap_months,\n",
    "\t\tcase \n",
    "\t\t\twhen ordue.LASTMONTHS in ('G','Z','D') then 8 \n",
    "\t\t\telse ordue.LASTMONTHS \n",
    "\t\tend as LASTMONTHS ,\n",
    "\t\tloanfo.GUARANTEETYPE,STATE,FINANCEORG\n",
    "from ICRLATEST2YEAROVERDUECARD ordue\n",
    "-- 只取贷记卡的信息\n",
    "inner join ICRLOANCARDINFO loanfo on ordue.reportno=loanfo.reportno and ordue.ACCOUNT=loanfo.ACCOUNT \n",
    "left join ICRMESSAGEHEADER head on ordue.reportno=head.reportno\n",
    "where BIZTYPE='准贷记卡' and ordue.LASTMONTHS!='C'\n",
    "\"\"\").createOrReplaceTempView(\"temp_standcard_2yearoverdue\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select \n",
    "\t\tordue.reportno,\n",
    "\t\tordue.ACCOUNT,\n",
    "\t\thead.QUERYTIME,\n",
    "\t\tordue.MONTH,\n",
    "\t\tmonth(date_format(head.QUERYTIME,'%Y-%m-%d')) - month(date_format(concat(ordue.month,'.01'),'%Y-%m-%d')) as gap_months,\n",
    "\t\tcase \n",
    "\t\t\twhen ordue.LASTMONTHS in ('G','Z','D') then 8 \n",
    "\t\t\telse ordue.LASTMONTHS \n",
    "\t\tend as LASTMONTHS ,\n",
    "\t\tloanfo.GUARANTEETYPE,STATE,FINANCEORG\n",
    "from ICRLATEST2YEAROVERDUECARD ordue\n",
    "-- 只取贷记卡的信息\n",
    "inner join ICRLOANCARDINFO loanfo on ordue.reportno=loanfo.reportno and ordue.ACCOUNT=loanfo.ACCOUNT \n",
    "left join ICRMESSAGEHEADER head on ordue.reportno=head.reportno\n",
    "where BIZTYPE='贷记卡' and ordue.LASTMONTHS!='C'\n",
    "\"\"\").createOrReplaceTempView(\"temp_creditcard_2yearoverdue\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select \n",
    "\t\tordue.reportno,\n",
    "\t\tordue.ACCOUNT,\n",
    "\t\thead.QUERYTIME,\n",
    "\t\tordue.MONTH,\n",
    "\t\tmonth(date_format(head.QUERYTIME,'%Y-%m-%d')) - month(date_format(concat(ordue.month,'.01'),'%Y-%m-%d')) as gap_months,\n",
    "\t\tordue.LASTMONTHS,ordue.AMOUNT,loanfo.GUARANTEETYPE,STATE,FINANCEORG\n",
    "from ICRLATEST5YEAROVERDUEDETAIL ordue\n",
    "-- 只取贷记卡的信息\n",
    "inner join ICRLOANCARDINFO loanfo on ordue.reportno=loanfo.reportno and ordue.ACCOUNT=loanfo.ACCOUNT \n",
    "left join ICRMESSAGEHEADER head on ordue.reportno=head.reportno\n",
    "where month!='--'  and BIZTYPE='贷记卡'\n",
    "\"\"\").createOrReplaceTempView(\"temp_creditcard_overdue\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select \n",
    "\t\treportno,\n",
    "\t\tACCOUNT,\n",
    "\t\tQUERYTIME,\n",
    "\t\tMONTH,\n",
    "\t\tgap_months,\n",
    "\t\tLASTMONTHS,\n",
    "\t\tGUARANTEETYPE,\n",
    "\t\tSTATE,\n",
    "\t\tFINANCEORG \n",
    "from temp_creditcard_2yearoverdue\n",
    "union \n",
    "select \n",
    "\t\treportno,\n",
    "\t\tACCOUNT,\n",
    "\t\tQUERYTIME,\n",
    "\t\tdate_format(concat(month,'.01'),'%Y-%m'),\n",
    "        gap_months,\n",
    "\t\tLASTMONTHS,\n",
    "\t\tGUARANTEETYPE,\n",
    "\t\tSTATE,\n",
    "\t\tFINANCEORG \n",
    "from temp_creditcard_overdue\n",
    "\"\"\").createOrReplaceTempView(\"temp_creditcard_5yearoverdue\")\n",
    "\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select \n",
    "\t\tordue.reportno,\n",
    "\t\tordue.ACCOUNT,\n",
    "\t\thead.QUERYTIME,\n",
    "\t\tordue.MONTH,\n",
    "\t\tmonth(date_format(head.QUERYTIME,'%Y-%m-%d')) - month(date_format(concat(ordue.month,'.01'),'%Y-%m-%d')) as gap_months,\n",
    "\t\tcase \n",
    "\t\t\twhen ordue.LASTMONTHS in ('G','Z','D') then 8 \n",
    "\t\t\telse ordue.LASTMONTHS \n",
    "\t\tend as LASTMONTHS ,\n",
    "\t\tloanfo.GUARANTEETYPE,\n",
    "\t\tSTATE,\n",
    "\t\tFINANCEORG\n",
    "from ICRLATEST2YEAROVERDUE ordue\n",
    "-- 只取贷款的信息\n",
    "inner join ICRLOANINFO loanfo on ordue.reportno=loanfo.reportno and ordue.ACCOUNT=loanfo.ACCOUNT \n",
    "left join ICRMESSAGEHEADER head on ordue.reportno=head.reportno\n",
    "where ordue.LASTMONTHS!='C'\n",
    "\"\"\").createOrReplaceTempView(\"temp_loan_2yearoverdue\")\n",
    "\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select \n",
    "\t\tordue.reportno,\n",
    "\t\tordue.ACCOUNT,\n",
    "\t\thead.QUERYTIME,\n",
    "\t\tordue.MONTH,\n",
    "\t\tmonth(date_format(head.QUERYTIME,'%Y-%m-%d')) - month(date_format(concat(ordue.month,'.01'),'%Y-%m-%d')) as gap_months,\n",
    "\t\tordue.LASTMONTHS,\n",
    "\t\tordue.AMOUNT,\n",
    "\t\tloanfo.GUARANTEETYPE,\n",
    "\t\tSTATE,FINANCEORG\n",
    "from ICRLATEST5YEAROVERDUEDETAIL ordue\n",
    "-- 只取贷款的信息\n",
    "inner join ICRLOANINFO loanfo on ordue.reportno=loanfo.reportno and ordue.ACCOUNT=loanfo.ACCOUNT \n",
    "left join ICRMESSAGEHEADER head on ordue.reportno=head.reportno\n",
    "where month!='--'\n",
    "\"\"\").createOrReplaceTempView(\"temp_loan_overdue\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select \n",
    "\t\treportno,\n",
    "\t\tACCOUNT,\n",
    "\t\tQUERYTIME,\n",
    "        MONTH,\n",
    "\t\tgap_months,\n",
    "\t\tLASTMONTHS,\n",
    "\t\tGUARANTEETYPE,\n",
    "\t\tSTATE,\n",
    "\t\tFINANCEORG \n",
    "from temp_loan_2yearoverdue\n",
    "union \n",
    "select \n",
    "\t\treportno,\n",
    "\t\tACCOUNT,\n",
    "\t\tQUERYTIME,\n",
    "\t\tdate_format(concat(month,'.01'),'%Y-%m'),gap_months,LASTMONTHS,\n",
    "\t\tGUARANTEETYPE,\n",
    "\t\tSTATE,\n",
    "\t\tFINANCEORG \n",
    "from temp_loan_overdue\n",
    "\"\"\").createOrReplaceTempView(\"temp_loan_5yearoverdue\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中间表已运行完成，spark sql 创建所需要的表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "select \n",
    "\tcard.reportno,\n",
    "\tbiztype,\n",
    "\tcard.account,\n",
    "\tmonth(head.QUERYTIME)-month(card.opendate) as months,\n",
    "\tcard.state,\n",
    "\tcase when ovd.reportno is null then 0 else 1 end as is_ovd\n",
    "from ICRLOANCARDINFO card\n",
    "left join ICRMESSAGEHEADER head on card.reportno=head.reportno\n",
    "left join \n",
    "(\tselect \n",
    "\t\t\tdistinct reportno,\n",
    "\t\t\taccount \n",
    "\tfrom temp_standcard_2yearoverdue\n",
    "\tunion \n",
    "\tselect \n",
    "\t\t\tdistinct reportno,\n",
    "\t\t\taccount \n",
    "\tfrom temp_creditcard_5yearoverdue\n",
    ")ovd on  card.reportno=ovd.reportno and card.account=ovd.account\n",
    "\n",
    "union all\n",
    "\n",
    "select \n",
    "\t\tloan.reportno,'贷款'  as biztype,\n",
    "\t\tloan.account,\n",
    "\t\tcase \n",
    "\t\t\twhen state='结清' and loan.ENDDATE='-' then substr(PAYMENTCYC,1,length(PAYMENTCYC)-3)+0\n",
    "\t\t\twhen state='结清' and loan.ENDDATE!='-' then month(loan.ENDDATE )-month(loan.OPENDATE)\n",
    "\t\t\twhen state!='结清' and day(date_format(loan.ENDDATE,'%Y-%m-%d'))-day(date_format(head.QUERYTIME,'%Y-%m-%d'))<0 then month(loan.ENDDATE)-month(loan.OPENDATE)\n",
    "\t\telse month(head.QUERYTIME)-month(loan.OPENDATE) end as months,\n",
    "\t-- TIMESTAMPDIFF(month,loan.opendate,head.QUERYTIME) as months,\n",
    "\t\tloan.state,\n",
    "\t\tcase when ovd.reportno is null then 0 else 1 end as is_ovd\n",
    "from ICRLOANINFO loan\n",
    "left join ICRMESSAGEHEADER head on loan.reportno=head.reportno\n",
    "left join \n",
    "(\t\n",
    "\tselect \n",
    "\t\tdistinct reportno,\n",
    "\t\taccount \n",
    "\tfrom temp_loan_5yearoverdue\n",
    ")ovd on  loan.reportno=ovd.reportno and loan.account=ovd.account\n",
    "where PAYMENTCYC!='-' or state!='结清' or ENDDATE!='-'\n",
    "\n",
    "\"\"\").createOrReplaceTempView(\"temp_notoverdue_account\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "select reportno,\n",
    "\tcount(distinct mth3_card) as mth3_card,\tcount(distinct mth6_card) as mth6_card,\n",
    "\tcount(distinct mth9_card) as mth9_card,\tcount(distinct mth12_card) as mth12_card,\n",
    "\tcount(distinct mth18_card) as mth18_card, count(distinct mth24_card) as mth24_card,\t\n",
    "\tcount(distinct mth3_scard) as mth3_scard,count(distinct mth6_scard) as mth6_scard,\n",
    "\tcount(distinct mth9_scard) as mth9_scard,\tcount(distinct mth12_scard) as mth12_scard,\n",
    "\tcount(distinct mth18_scard) as mth18_scard, count(distinct mth24_scard) as mth24_scard,\n",
    "\tcount(distinct mth3_loan) as mth3_loan,\tcount(distinct mth6_loan) as mth6_loan,\n",
    "\tcount(distinct mth9_loan) as mth9_loan,\tcount(distinct mth12_loan) as mth12_loan,\n",
    "\tcount(distinct mth18_loan) as mth18_loan, count(distinct mth24_loan) as mth24_loan\n",
    "from (\t\n",
    "\tselect reportno,\n",
    "\tcase when biztype='贷记卡' and months>=3 then account else null end as mth3_card,\n",
    "\tcase when biztype='贷记卡' and months>=6 then account else null end as mth6_card,\n",
    "\tcase when biztype='贷记卡' and months>=9 then account else null end as mth9_card,\n",
    "\tcase when biztype='贷记卡' and months>=12 then account else null end as mth12_card,\n",
    "\tcase when biztype='贷记卡' and months>=18 then account else null end as mth18_card,\n",
    "\tcase when biztype='贷记卡' and months>=24 then account else null end as mth24_card,\n",
    "\t\n",
    "\tcase when biztype='准贷记卡' and months>=3 then account else null end as mth3_scard,\n",
    "\tcase when biztype='准贷记卡' and months>=6 then account else null end as mth6_scard,\n",
    "\tcase when biztype='准贷记卡' and months>=9 then account else null end as mth9_scard,\n",
    "\tcase when biztype='准贷记卡' and months>=12 then account else null end as mth12_scard,\n",
    "\tcase when biztype='准贷记卡' and months>=18 then account else null end as mth18_scard,\n",
    "\tcase when biztype='准贷记卡' and months>=24 then account else null end as mth24_scard,\n",
    "\t\n",
    "\tcase when biztype='贷款' and months>=3 then account else null end as mth3_loan,\n",
    "\tcase when biztype='贷款' and months>=6 then account else null end as mth6_loan,\n",
    "\tcase when biztype='贷款' and months>=9 then account else null end as mth9_loan,\n",
    "\tcase when biztype='贷款' and months>=12 then account else null end as mth12_loan,\n",
    "\tcase when biztype='贷款' and months>=18 then account else null end as mth18_loan,\n",
    "\tcase when biztype='贷款' and months>=24 then account else null end as mth24_loan\n",
    "\t\n",
    "\tfrom temp_notoverdue_account\n",
    "\t)t\n",
    "group by reportno\n",
    "\n",
    "\"\"\").createOrReplaceTempView(\"temp_month_all\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "select reportno,\n",
    "\tsum(mth3_scard_notoverdue_count) as mth3_scard_notoverdue_count,\n",
    "\tsum(mth6_scard_notoverdue_count) as mth6_scard_notoverdue_count,\n",
    "\tsum(mth9_scard_notoverdue_count) as mth9_scard_notoverdue_count,\n",
    "\tsum(mth12_scard_notoverdue_count) as mth12_scard_notoverdue_count,\n",
    "\tsum(mth18_scard_notoverdue_count) as mth18_scard_notoverdue_count,\n",
    "\tsum(mth24_scard_notoverdue_count) as mth24_scard_notoverdue_count,\n",
    "\t\n",
    "\tsum(mth3_card_notoverdue_count) as mth3_card_notoverdue_count,\n",
    "\tsum(mth6_card_notoverdue_count) as mth6_card_notoverdue_count,\n",
    "\tsum(mth9_card_notoverdue_count) as mth9_card_notoverdue_count,\n",
    "\tsum(mth12_card_notoverdue_count) as mth12_card_notoverdue_count,\n",
    "\tsum(mth18_card_notoverdue_count) as mth18_card_notoverdue_count,\n",
    "\tsum(mth24_card_notoverdue_count) as mth24_card_notoverdue_count,\n",
    "\t\n",
    "\tsum(mth3_loan_notoverdue_count) as mth3_loan_notoverdue_count,\n",
    "\tsum(mth6_loan_notoverdue_count) as mth6_loan_notoverdue_count,\n",
    "\tsum(mth9_loan_notoverdue_count) as mth9_loan_notoverdue_count,\n",
    "\tsum(mth12_loan_notoverdue_count) as mth12_loan_notoverdue_count,\n",
    "\tsum(mth18_loan_notoverdue_count) as mth18_loan_notoverdue_count,\n",
    "\tsum(mth24_loan_notoverdue_count) as mth24_loan_notoverdue_count\n",
    "from \n",
    "\t(select reportno,\n",
    "\t\tcase when months>=3 and biztype='准贷记卡' then 1 else 0 end as mth3_scard_notoverdue_count,\n",
    "\t\tcase when months>=6 and biztype='准贷记卡' then 1 else 0 end as mth6_scard_notoverdue_count,\n",
    "\t\tcase when months>=9 and biztype='准贷记卡' then 1 else 0 end as mth9_scard_notoverdue_count,\n",
    "\t\tcase when months>=12 and biztype='准贷记卡' then 1 else 0 end as mth12_scard_notoverdue_count,\n",
    "\t\tcase when months>=18 and biztype='准贷记卡' then 1 else 0 end as mth18_scard_notoverdue_count,\n",
    "\t\tcase when months>=24 and biztype='准贷记卡' then 1 else 0 end as mth24_scard_notoverdue_count,\n",
    "\t\t\n",
    "\t\tcase when months>=3 and biztype='贷记卡' then 1 else 0 end as mth3_card_notoverdue_count,\n",
    "\t\tcase when months>=6 and biztype='贷记卡' then 1 else 0 end as mth6_card_notoverdue_count,\n",
    "\t\tcase when months>=9 and biztype='贷记卡' then 1 else 0 end as mth9_card_notoverdue_count,\n",
    "\t\tcase when months>=12 and biztype='贷记卡' then 1 else 0 end as mth12_card_notoverdue_count,\n",
    "\t\tcase when months>=18 and biztype='贷记卡' then 1 else 0 end as mth18_card_notoverdue_count,\n",
    "\t\tcase when months>=24 and biztype='贷记卡' then 1 else 0 end as mth24_card_notoverdue_count,\n",
    "\n",
    "\t\tcase when months>=3 and biztype='贷款' then 1 else 0 end as mth3_loan_notoverdue_count,\n",
    "\t\tcase when months>=6 and biztype='贷款' then 1 else 0 end as mth6_loan_notoverdue_count,\n",
    "\t\tcase when months>=9 and biztype='贷款' then 1 else 0 end as mth9_loan_notoverdue_count,\n",
    "\t\tcase when months>=12 and biztype='贷款' then 1 else 0 end as mth12_loan_notoverdue_count,\n",
    "\t\tcase when months>=18 and biztype='贷款' then 1 else 0 end as mth18_loan_notoverdue_count,\n",
    "\t\tcase when months>=24 and biztype='贷款' then 1 else 0 end as mth24_loan_notoverdue_count\n",
    "\n",
    "\tfrom temp_notoverdue_account\n",
    "\twhere is_ovd=0 and state!='销户' \n",
    "\t)t\n",
    "group by reportno\n",
    "\n",
    "\"\"\").createOrReplaceTempView(\"temp_month_all2\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "select head.reportno,\n",
    "\tcase when nvl(is_ex.mth3_scard,0) =0 then -9999999 \n",
    "\t\telse nvl(cardl.mth3_scard_notoverdue_count,0) end as mth3_scard_notoverdue_count,\n",
    "\tcase when nvl(is_ex.mth6_scard,0) =0 then -9999999 \n",
    "\t\telse nvl(cardl.mth6_scard_notoverdue_count,0) end as mth6_scard_notoverdue_count,\n",
    "\tcase when nvl(is_ex.mth9_scard,0)=0 then -9999999 \n",
    "\t\telse nvl(cardl.mth9_scard_notoverdue_count,0) end as mth9_scard_notoverdue_count,\t\n",
    "\tcase when nvl(is_ex.mth12_scard,0) =0 then -9999999 \n",
    "\t\telse nvl(cardl.mth12_scard_notoverdue_count,0) end as mth12_scard_notoverdue_count,\n",
    "\tcase when nvl(is_ex.mth18_scard,0) =0 then -9999999 \n",
    "\t\telse nvl(cardl.mth18_scard_notoverdue_count,0) end as mth18_scard_notoverdue_count,\n",
    "\tcase when nvl(is_ex.mth24_scard,0) =0 then -9999999 \n",
    "\t\telse nvl(cardl.mth24_scard_notoverdue_count,0) end as mth24_scard_notoverdue_count,\n",
    "\n",
    "\tcase when nvl(is_ex.mth3_card,0) =0 then -9999999 \n",
    "\t\telse nvl(cardl.mth3_card_notoverdue_count,0) end as mth3_card_notoverdue_count,\n",
    "\tcase when nvl(is_ex.mth6_card,0) =0 then -9999999 \n",
    "\t\telse nvl(cardl.mth6_card_notoverdue_count,0) end as mth6_card_notoverdue_count,\n",
    "\tcase when nvl(is_ex.mth9_card,0) =0 then -9999999 \n",
    "\t\telse nvl(cardl.mth9_card_notoverdue_count,0) end as mth9_card_notoverdue_count,\t\n",
    "\tcase when nvl(is_ex.mth12_card,0) =0 then -9999999 \n",
    "\t\telse nvl(cardl.mth12_card_notoverdue_count,0) end as mth12_card_notoverdue_count,\n",
    "\tcase when nvl(is_ex.mth18_card,0) =0 then -9999999 \n",
    "\t\telse nvl(cardl.mth18_card_notoverdue_count,0) end as mth18_card_notoverdue_count,\n",
    "\tcase when nvl(is_ex.mth24_card,0) =0 then -9999999 \n",
    "\t\telse nvl(cardl.mth24_card_notoverdue_count,0) end as mth24_card_notoverdue_count,\n",
    "\t\t\n",
    "\tcase when nvl(is_ex.mth3_loan,0) =0 then -9999999 \n",
    "\t\telse nvl(cardl.mth3_loan_notoverdue_count,0) end as mth3_loan_notoverdue_count,\n",
    "\tcase when nvl(is_ex.mth6_loan,0) =0 then -9999999 \n",
    "\t\telse nvl(cardl.mth6_loan_notoverdue_count,0) end as mth6_loan_notoverdue_count,\n",
    "\tcase when nvl(is_ex.mth9_loan,0) =0 then -9999999 \n",
    "\t\telse nvl(cardl.mth9_loan_notoverdue_count,0) end as mth9_loan_notoverdue_count,\t\n",
    "\tcase when nvl(is_ex.mth12_loan,0) =0 then -9999999 \n",
    "\t\telse nvl(cardl.mth12_loan_notoverdue_count,0) end as mth12_loan_notoverdue_count,\n",
    "\tcase when nvl(is_ex.mth18_loan,0) =0 then -9999999 \n",
    "\t\telse nvl(cardl.mth18_loan_notoverdue_count,0) end as mth18_loan_notoverdue_count,\n",
    "\tcase when nvl(is_ex.mth24_loan,0) =0 then -9999999 \n",
    "\t\telse nvl(cardl.mth24_loan_notoverdue_count,0) end as mth24_loan_notoverdue_count\n",
    "\t\t\n",
    "from ICRMESSAGEHEADER head\n",
    "left join temp_month_all2 cardl on head.reportno=cardl.reportno\n",
    "left join temp_month_all is_ex on head.reportno=is_ex.reportno\n",
    "\n",
    "\"\"\").createOrReplaceTempView(\"dw_notoverdue_account_summary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_name = \"dw_notoverdue_account_summary\"\n",
    "\n",
    "spark.sql(\"drop table if EXISTS renhang_user_profile.%s \"%table_name)\n",
    "spark.sql(\"create table renhang_user_profile.%s as select * from %s\"%(table_name,table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SQLContext,SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "\n",
    "# 初始化数据库连接，使用pymysql模块\n",
    "# MySQL的用户：dt, 密码:Usd&212%wePO2, 端口：3306,数据库：\n",
    "spark=SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.eventLog.enabled\", \"false\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\")\\\n",
    "        .config(\"spark.driver.memory\", \"8g\")\\\n",
    "        .config(\"spark.cores.max\", \"10\")\\\n",
    "        .config(\"spark.task.maxFailures\", \"1000\")\\\n",
    "        .config(\"spark.default.parallelism\", \"500\")\\\n",
    "        .config(\"spark.sql.shuffle.partitions\",100)\\\n",
    "        .appName('renhang_etl') \\\n",
    "        .master('yarn')\\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "engine = create_engine('mysql+pymysql://dt:Usd&212%wePO2@58.59.11.87:3306/DM')\n",
    "\n",
    "\n",
    "\n",
    "def time_format(ts):\n",
    "    try:\n",
    "        res = ts[0:10].replace('.','')\n",
    "        return res\n",
    "    except:\n",
    "        return 'None'\n",
    "    else:\n",
    "        return 'None'    \n",
    "spark.udf.register('time_format',lambda x:time_format(x))\n",
    "\n",
    "    \n",
    "def get_time(ts):\n",
    "    try:\n",
    "        res = ts.replace('.','-')\n",
    "        return res\n",
    "    except:\n",
    "        return 'None'\n",
    "    else:\n",
    "        return 'None'\n",
    "\n",
    "\n",
    "spark.udf.register('get_time',lambda x:get_time(x))\n",
    "# time_udfs = udf(get_time, StringType())\n",
    "\n",
    "\n",
    "def timestampdiff(flag,t1,t2):\n",
    "    try:\n",
    "        date1 = t1[0:10].replace('.','-')\n",
    "        date2 = t2[0:10].replace('.','-')\n",
    "        year = (int(date1[0:4])-int(date2[0:4]))\n",
    "        month = (int(date1[5:7])-int(date2[5:7]))\n",
    "        day = int(date1[8:10])-int(date2[8:10])\n",
    "        if(flag == 'year'):\n",
    "            if(month<0 or (month==0 and day<0)):\n",
    "                result = year -1\n",
    "                return result\n",
    "            return year\n",
    "        if(flag == 'month'):\n",
    "            res = year*12+month+int((day-30)/31)\n",
    "            return res\n",
    "    \n",
    "    except:\n",
    "        return 'None'\n",
    "    \n",
    "                                                                                      \n",
    "spark.udf.register('timestampdiff',lambda x1,x2,x3:timestampdiff(x1,x2,x3))\n",
    "# timestampdiff = udf(get_time, StringType())   \n",
    "\n",
    "\n",
    "def get_city(address):\n",
    "    try:\n",
    "        province_loc = address.find(\"省\")\n",
    "        city_loc = address.find(\"市\")\n",
    "        if(province_loc < 0 or (province_loc>=0 and province_loc>=city_loc) ): #江西赣州市兴国县江西省赣州市兴国县潋江镇罗廖村26号\n",
    "            city = address[0:city_loc+1]\n",
    "        if(province_loc>=0 and province_loc<city_loc):\n",
    "            city = address[province_loc+1:city_loc+1]\n",
    "        return city\n",
    "    except:\n",
    "        return 'None'\n",
    "\n",
    "\n",
    "\n",
    "spark.udf.register('get_city',lambda x:get_city(x))\n",
    "# time_udfs = udf(get_time, StringType())\n",
    "                                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建DW.dw_baseinfo_detail\n",
    "table1 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRMESSAGEHEADER\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table1.createOrReplaceTempView(\"ICRMESSAGEHEADER\")\n",
    "\n",
    "table2 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRCREDITCUE\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table2.createOrReplaceTempView(\"ICRCREDITCUE\")\n",
    "\n",
    "table3 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRLOANCARDINFO\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table3.createOrReplaceTempView(\"ICRLOANCARDINFO\")\n",
    "\n",
    "table4 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRCREDITCUE\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table4.createOrReplaceTempView(\"ICRCREDITCUE\")\n",
    "\n",
    "table5 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRLATEST2YEAROVERDUECARD\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table5.createOrReplaceTempView(\"ICRLATEST2YEAROVERDUECARD\")\n",
    "\n",
    "table6 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRLATEST5YEAROVERDUEDETAIL\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table6.createOrReplaceTempView(\"ICRLATEST5YEAROVERDUEDETAIL\")\n",
    "\n",
    "table7 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRLATEST2YEAROVERDUE\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table7.createOrReplaceTempView(\"ICRLATEST2YEAROVERDUE\")\n",
    "\n",
    "table8 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRLOANINFO\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table8.createOrReplaceTempView(\"ICRLOANINFO\")\n",
    "\n",
    "table9 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRUNDESTORYLOANCARD\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table9.createOrReplaceTempView(\"ICRUNDESTORYLOANCARD\")\n",
    "\n",
    "table10 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRUNDESTORYSTANDARDLOANCARD\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table10.createOrReplaceTempView(\"ICRUNDESTORYSTANDARDLOANCARD\")\n",
    "\n",
    "table11 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRUNPAIDLOAN\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table11.createOrReplaceTempView(\"ICRUNPAIDLOAN\")\n",
    "\n",
    "\n",
    "table12 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICROVERDUESUMMARY\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table12.createOrReplaceTempView(\"ICROVERDUESUMMARY\")\n",
    "\n",
    "\n",
    "table13 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRFELLBACKSUMMARY\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table13.createOrReplaceTempView(\"ICRFELLBACKSUMMARY\")\n",
    "\n",
    "\n",
    "table14 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRGUARANTEE\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table14.createOrReplaceTempView(\"ICRGUARANTEE\")\n",
    "\n",
    "\n",
    "table15 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRGUARANTEESUMMARY\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table15.createOrReplaceTempView(\"ICRGUARANTEESUMMARY\")\n",
    "\n",
    "\n",
    "#######\n",
    "\n",
    "table16 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRCIVILJUDGEMENT\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table16.createOrReplaceTempView(\"ICRCIVILJUDGEMENT\")\n",
    "\n",
    "\n",
    "\n",
    "table17 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRFORCEEXECUTION\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table17.createOrReplaceTempView(\"ICRFORCEEXECUTION\")\n",
    "\n",
    "\n",
    "table18 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRADMINPUNISHMENT\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table18.createOrReplaceTempView(\"ICRADMINPUNISHMENT\")\n",
    "\n",
    "\n",
    "table19 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRTAXARREAR\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table19.createOrReplaceTempView(\"ICRTAXARREAR\")\n",
    "\n",
    "\n",
    "table20 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRACCFUND\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table20.createOrReplaceTempView(\"ICRACCFUND\")\n",
    "\n",
    "\n",
    "\n",
    "table21 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRENDOWMENTINSURANCEDEPOSIT\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table21.createOrReplaceTempView(\"ICRENDOWMENTINSURANCEDEPOSIT\")\n",
    "\n",
    "\n",
    "table22 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRRECORDDETAIL\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table22.createOrReplaceTempView(\"ICRRECORDDETAIL\")\n",
    "\n",
    "\n",
    "\n",
    "table23 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRRECORDDETAIL\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table23.createOrReplaceTempView(\"ICRRECORDDETAIL\")\n",
    "\n",
    "\n",
    "table24 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.QUERY_HISTORY\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table24.createOrReplaceTempView(\"QUERY_HISTORY\")\n",
    "\n",
    "table25 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRRECORDSUMMARY\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table25.createOrReplaceTempView(\"ICRRECORDSUMMARY\")\n",
    "\n",
    "\n",
    "table25 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRQUERYREQ\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table25.createOrReplaceTempView(\"ICRQUERYREQ\")\n",
    "\n",
    "\n",
    "\n",
    "table25 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"ICR_PROD.ICRIDENTITY\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table25.createOrReplaceTempView(\"ICRIDENTITY\")\n",
    "###########################################################\n",
    "##########################################################\n",
    "table1 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"WKL.temp_standcard_2yearoverdue\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table1.createOrReplaceTempView(\"temp_standcard_2yearoverdue\")\n",
    "\n",
    "table2 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"WKL.temp_creditcard_2yearoverdue\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table2.createOrReplaceTempView(\"temp_creditcard_2yearoverdue\")\n",
    "\n",
    "table1 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"WKL.temp_creditcard_overdue\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table1.createOrReplaceTempView(\"temp_creditcard_overdue\")\n",
    "\n",
    "table2 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"WKL.temp_creditcard_5yearoverdue\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table2.createOrReplaceTempView(\"temp_creditcard_5yearoverdue\")\n",
    "\n",
    "table1 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"WKL.temp_loan_2yearoverdue\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table1.createOrReplaceTempView(\"temp_loan_2yearoverdue\")\n",
    "\n",
    "table2 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"WKL.temp_loan_overdue\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table2.createOrReplaceTempView(\"temp_loan_overdue\")\n",
    "\n",
    "table2 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"WKL.temp_loan_5yearoverdue\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table2.createOrReplaceTempView(\"temp_loan_5yearoverdue\")\n",
    "\n",
    "table2 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"WKL.temp_address_change\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table2.createOrReplaceTempView(\"temp_address_change\")\n",
    "\n",
    "table2 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"WKL.temp_emp_change\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table2.createOrReplaceTempView(\"temp_emp_change\")\n",
    "#####\n",
    "\n",
    "table2 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"WKL.temp_loan_cont_1\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table2.createOrReplaceTempView(\"temp_loan_cont_1\")\n",
    "\n",
    "table2 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"WKL.temp_loan_cont_2\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table2.createOrReplaceTempView(\"temp_loan_cont_2\")\n",
    "###\n",
    "\n",
    "table2 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"WKL.temp_notoverdue_account\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table2.createOrReplaceTempView(\"temp_notoverdue_account\")\n",
    "\n",
    "table2 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"WKL.temp_month_all\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table2.createOrReplaceTempView(\"temp_month_all\")\n",
    "\n",
    "table2 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"WKL.temp_month_all2\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table2.createOrReplaceTempView(\"temp_month_all2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`is_ex.mth24_loan`' given input columns: [mth3_scard_notoverdue_count, mth12_scard, reportno, reportno, mth3_scard, mth18_scard, mth12_scard_notoverdue_count, mth6_scard, mth6_scard_notoverdue_count, REPORTCREATETIME, mth9_scard_notoverdue_count, mth24_scard_notoverdue_count, mth9_scard, mth18_scard_notoverdue_count, QUERYTIME, REPORTNO, mth24_scard]; line 5 pos 15;\\n'Project [reportno#1810, CASE WHEN ('nvl('is_ex.mth24_loan, 0) = 0) THEN -9999999 ELSE 'nvl('cardl.mth24_loan_notoverdue_count, 0) END AS mth24_loan_notoverdue_count#2742]\\n+- Join LeftOuter, (reportno#1810 = reportno#2688)\\n   :- Join LeftOuter, (reportno#1810 = reportno#2704)\\n   :  :- SubqueryAlias head\\n   :  :  +- SubqueryAlias icrmessageheader\\n   :  :     +- Relation[REPORTNO#1810,QUERYTIME#1811,REPORTCREATETIME#1812] JDBCRelation(ICR_PROD.ICRMESSAGEHEADER) [numPartitions=1]\\n   :  +- SubqueryAlias cardl\\n   :     +- SubqueryAlias temp_month_all2\\n   :        +- Relation[reportno#2704,mth3_scard_notoverdue_count#2705,mth6_scard_notoverdue_count#2706,mth9_scard_notoverdue_count#2707,mth12_scard_notoverdue_count#2708,mth18_scard_notoverdue_count#2709,mth24_scard_notoverdue_count#2710] JDBCRelation(WKL.temp_month_all2) [numPartitions=1]\\n   +- SubqueryAlias is_ex\\n      +- SubqueryAlias temp_month_all\\n         +- Relation[reportno#2688,mth3_scard#2689L,mth6_scard#2690L,mth9_scard#2691L,mth12_scard#2692L,mth18_scard#2693L,mth24_scard#2694L] JDBCRelation(WKL.temp_month_all) [numPartitions=1]\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.0.1-1.cdh6.0.1.p0.590678/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.0.1-1.cdh6.0.1.p0.590678/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o78.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`is_ex.mth24_loan`' given input columns: [mth3_scard_notoverdue_count, mth12_scard, reportno, reportno, mth3_scard, mth18_scard, mth12_scard_notoverdue_count, mth6_scard, mth6_scard_notoverdue_count, REPORTCREATETIME, mth9_scard_notoverdue_count, mth24_scard_notoverdue_count, mth9_scard, mth18_scard_notoverdue_count, QUERYTIME, REPORTNO, mth24_scard]; line 5 pos 15;\n'Project [reportno#1810, CASE WHEN ('nvl('is_ex.mth24_loan, 0) = 0) THEN -9999999 ELSE 'nvl('cardl.mth24_loan_notoverdue_count, 0) END AS mth24_loan_notoverdue_count#2742]\n+- Join LeftOuter, (reportno#1810 = reportno#2688)\n   :- Join LeftOuter, (reportno#1810 = reportno#2704)\n   :  :- SubqueryAlias head\n   :  :  +- SubqueryAlias icrmessageheader\n   :  :     +- Relation[REPORTNO#1810,QUERYTIME#1811,REPORTCREATETIME#1812] JDBCRelation(ICR_PROD.ICRMESSAGEHEADER) [numPartitions=1]\n   :  +- SubqueryAlias cardl\n   :     +- SubqueryAlias temp_month_all2\n   :        +- Relation[reportno#2704,mth3_scard_notoverdue_count#2705,mth6_scard_notoverdue_count#2706,mth9_scard_notoverdue_count#2707,mth12_scard_notoverdue_count#2708,mth18_scard_notoverdue_count#2709,mth24_scard_notoverdue_count#2710] JDBCRelation(WKL.temp_month_all2) [numPartitions=1]\n   +- SubqueryAlias is_ex\n      +- SubqueryAlias temp_month_all\n         +- Relation[reportno#2688,mth3_scard#2689L,mth6_scard#2690L,mth9_scard#2691L,mth12_scard#2692L,mth18_scard#2693L,mth24_scard#2694L] JDBCRelation(WKL.temp_month_all) [numPartitions=1]\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$11.apply(TreeNode.scala:335)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:333)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$11.apply(TreeNode.scala:344)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:333)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:285)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:632)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4b74b31ab8dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mleft\u001b[0m \u001b[0mjoin\u001b[0m \u001b[0mtemp_month_all\u001b[0m \u001b[0mis_ex\u001b[0m \u001b[0mon\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreportno\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_ex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreportno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \"\"\").createOrReplaceTempView(\"dw_notoverdue_account_summary\")\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.0.1-1.cdh6.0.1.p0.590678/lib/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \"\"\"\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.0.1-1.cdh6.0.1.p0.590678/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.0.1-1.cdh6.0.1.p0.590678/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`is_ex.mth24_loan`' given input columns: [mth3_scard_notoverdue_count, mth12_scard, reportno, reportno, mth3_scard, mth18_scard, mth12_scard_notoverdue_count, mth6_scard, mth6_scard_notoverdue_count, REPORTCREATETIME, mth9_scard_notoverdue_count, mth24_scard_notoverdue_count, mth9_scard, mth18_scard_notoverdue_count, QUERYTIME, REPORTNO, mth24_scard]; line 5 pos 15;\\n'Project [reportno#1810, CASE WHEN ('nvl('is_ex.mth24_loan, 0) = 0) THEN -9999999 ELSE 'nvl('cardl.mth24_loan_notoverdue_count, 0) END AS mth24_loan_notoverdue_count#2742]\\n+- Join LeftOuter, (reportno#1810 = reportno#2688)\\n   :- Join LeftOuter, (reportno#1810 = reportno#2704)\\n   :  :- SubqueryAlias head\\n   :  :  +- SubqueryAlias icrmessageheader\\n   :  :     +- Relation[REPORTNO#1810,QUERYTIME#1811,REPORTCREATETIME#1812] JDBCRelation(ICR_PROD.ICRMESSAGEHEADER) [numPartitions=1]\\n   :  +- SubqueryAlias cardl\\n   :     +- SubqueryAlias temp_month_all2\\n   :        +- Relation[reportno#2704,mth3_scard_notoverdue_count#2705,mth6_scard_notoverdue_count#2706,mth9_scard_notoverdue_count#2707,mth12_scard_notoverdue_count#2708,mth18_scard_notoverdue_count#2709,mth24_scard_notoverdue_count#2710] JDBCRelation(WKL.temp_month_all2) [numPartitions=1]\\n   +- SubqueryAlias is_ex\\n      +- SubqueryAlias temp_month_all\\n         +- Relation[reportno#2688,mth3_scard#2689L,mth6_scard#2690L,mth9_scard#2691L,mth12_scard#2692L,mth18_scard#2693L,mth24_scard#2694L] JDBCRelation(WKL.temp_month_all) [numPartitions=1]\\n\""
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "select head.reportno,\n",
    "\n",
    "\tcase when nvl(is_ex.mth24_loan,0) =0 then -9999999 \n",
    "\t\telse nvl(cardl.mth24_loan_notoverdue_count,0) end as mth24_loan_notoverdue_count\n",
    "\t\t\n",
    "from ICRMESSAGEHEADER head\n",
    "left join temp_month_all2 cardl on head.reportno=cardl.reportno\n",
    "left join temp_month_all is_ex on head.reportno=is_ex.reportno\n",
    "\n",
    "\"\"\").createOrReplaceTempView(\"dw_notoverdue_account_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
