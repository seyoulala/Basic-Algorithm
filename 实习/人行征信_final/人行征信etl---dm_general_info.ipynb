{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在运行dw相关表之前，首先运行下面部分以创建中间表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/thd/人行征信\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark import SparkContext,SQLContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "import os\n",
    "import sklearn\n",
    "print(os.path.abspath(os.curdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.eventLog.enabled\", \"false\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\")\\\n",
    "        .config(\"spark.driver.memory\", \"16g\")\\\n",
    "        .config(\"spark.cores.max\", \"10\")\\\n",
    "        .config(\"spark.task.maxFailures\", \"1000\")\\\n",
    "        .config(\"spark.default.parallelism\", \"500\")\\\n",
    "        .config(\"spark.sql.shuffle.partitions\",500)\\\n",
    "        .appName('renhang_etl') \\\n",
    "        .master('yarn')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建DW.dw_baseinfo_detail\n",
    "table1 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"hyxf.ICRMESSAGEHEADER\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table1.createOrReplaceTempView(\"ICRMESSAGEHEADER\")\n",
    "\n",
    "table2 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"hyxf.ICRCREDITCUE\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table2.createOrReplaceTempView(\"ICRCREDITCUE\")\n",
    "\n",
    "table3 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"hyxf.ICRLOANCARDINFO\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table3.createOrReplaceTempView(\"ICRLOANCARDINFO\")\n",
    "\n",
    "table4 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"hyxf.ICRCREDITCUE\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table4.createOrReplaceTempView(\"ICRCREDITCUE\")\n",
    "\n",
    "table5 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"hyxf.ICRLATEST2YEAROVERDUECARD\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table5.createOrReplaceTempView(\"ICRLATEST2YEAROVERDUECARD\")\n",
    "\n",
    "table6 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"hyxf.ICRLATEST5YEAROVERDUEDETAIL\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table6.createOrReplaceTempView(\"ICRLATEST5YEAROVERDUEDETAIL\")\n",
    "\n",
    "table7 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"hyxf.ICRLATEST2YEAROVERDUE\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table7.createOrReplaceTempView(\"ICRLATEST2YEAROVERDUE\")\n",
    "\n",
    "table8 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"hyxf.ICRLOANINFO\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table8.createOrReplaceTempView(\"ICRLOANINFO\")\n",
    "\n",
    "table9 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"hyxf.ICRUNDESTORYLOANCARD\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table9.createOrReplaceTempView(\"ICRUNDESTORYLOANCARD\")\n",
    "\n",
    "table10 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"hyxf.ICRUNDESTORYSTANDARDLOANCARD\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table10.createOrReplaceTempView(\"ICRUNDESTORYSTANDARDLOANCARD\")\n",
    "\n",
    "table11 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"hyxf.ICRUNPAIDLOAN\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table11.createOrReplaceTempView(\"ICRUNPAIDLOAN\")\n",
    "\n",
    "\n",
    "table12 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"hyxf.ICROVERDUESUMMARY\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table12.createOrReplaceTempView(\"ICROVERDUESUMMARY\")\n",
    "\n",
    "\n",
    "table13 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"hyxf.ICRFELLBACKSUMMARY\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table13.createOrReplaceTempView(\"ICRFELLBACKSUMMARY\")\n",
    "\n",
    "\n",
    "table14 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"hyxf.ICRGUARANTEE\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table14.createOrReplaceTempView(\"ICRGUARANTEE\")\n",
    "\n",
    "\n",
    "table15 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"hyxf.ICRGUARANTEESUMMARY\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table15.createOrReplaceTempView(\"ICRGUARANTEESUMMARY\")\n",
    "\n",
    "\n",
    "#######\n",
    "\n",
    "table16 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"hyxf.ICRCIVILJUDGEMENT\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table16.createOrReplaceTempView(\"ICRCIVILJUDGEMENT\")\n",
    "\n",
    "\n",
    "\n",
    "table17 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"hyxf.ICRFORCEEXECUTION\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table17.createOrReplaceTempView(\"ICRFORCEEXECUTION\")\n",
    "\n",
    "\n",
    "table18 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"hyxf.ICRADMINPUNISHMENT\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table18.createOrReplaceTempView(\"ICRADMINPUNISHMENT\")\n",
    "\n",
    "\n",
    "table19 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"hyxf.ICRTAXARREAR\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table19.createOrReplaceTempView(\"ICRTAXARREAR\")\n",
    "\n",
    "\n",
    "table20 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"hyxf.ICRACCFUND\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table20.createOrReplaceTempView(\"ICRACCFUND\")\n",
    "\n",
    "\n",
    "\n",
    "table21 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"hyxf.ICRENDOWMENTINSURANCEDEPOSIT\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table21.createOrReplaceTempView(\"ICRENDOWMENTINSURANCEDEPOSIT\")\n",
    "\n",
    "\n",
    "table22 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"hyxf.ICRRECORDDETAIL\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table22.createOrReplaceTempView(\"ICRRECORDDETAIL\")\n",
    "\n",
    "\n",
    "\n",
    "table23 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"hyxf.ICRRECORDDETAIL\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table23.createOrReplaceTempView(\"ICRRECORDDETAIL\")\n",
    "\n",
    "\n",
    "table24 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"hyxf.QUERY_HISTORY\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table24.createOrReplaceTempView(\"QUERY_HISTORY\")\n",
    "\n",
    "table25 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"hyxf.ICRRECORDSUMMARY\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table25.createOrReplaceTempView(\"ICRRECORDSUMMARY\")\n",
    "\n",
    "\n",
    "##############\n",
    "\n",
    "table26 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"hyxf.ICRQUERYREQ\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table26.createOrReplaceTempView(\"ICRQUERYREQ\")\n",
    "\n",
    "\n",
    "table27 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"hyxf.ICRIDENTITY\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table27.createOrReplaceTempView(\"ICRIDENTITY\")\n",
    "\n",
    "\n",
    "table28 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"hyxf.ICRRESIDENCE\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table28.createOrReplaceTempView(\"ICRRESIDENCE\")\n",
    "\n",
    "table29 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://58.59.11.87:3306\") \\\n",
    "    .option(\"dbtable\", \"hyxf.ICRPROFESSIONAL\") \\\n",
    "    .option(\"user\", \"dt\") \\\n",
    "    .option(\"password\", \"Usd&212%wePO2\") \\\n",
    "    .load()\n",
    "table29.createOrReplaceTempView(\"ICRPROFESSIONAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select \n",
    "\t\tordue.reportno,\n",
    "\t\tordue.ACCOUNT,\n",
    "\t\thead.QUERYTIME,\n",
    "\t\tordue.MONTH,\n",
    "\t\tmonth(date_format(head.QUERYTIME,'%Y-%m-%d')) - month(date_format(concat(ordue.month,'.01'),'%Y-%m-%d')) as gap_months,\n",
    "\t\tcase \n",
    "\t\t\twhen ordue.LASTMONTHS in ('G','Z','D') then 8 \n",
    "\t\t\telse ordue.LASTMONTHS \n",
    "\t\tend as LASTMONTHS ,\n",
    "\t\tloanfo.GUARANTEETYPE,STATE,FINANCEORG\n",
    "from ICRLATEST2YEAROVERDUECARD ordue\n",
    "-- 只取贷记卡的信息\n",
    "inner join ICRLOANCARDINFO loanfo on ordue.reportno=loanfo.reportno and ordue.ACCOUNT=loanfo.ACCOUNT \n",
    "left join ICRMESSAGEHEADER head on ordue.reportno=head.reportno\n",
    "where BIZTYPE='准贷记卡' and ordue.LASTMONTHS!='C'\n",
    "\"\"\").createOrReplaceTempView(\"temp_standcard_2yearoverdue\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select \n",
    "\t\tordue.reportno,\n",
    "\t\tordue.ACCOUNT,\n",
    "\t\thead.QUERYTIME,\n",
    "\t\tordue.MONTH,\n",
    "\t\tmonth(date_format(head.QUERYTIME,'%Y-%m-%d')) - month(date_format(concat(ordue.month,'.01'),'%Y-%m-%d')) as gap_months,\n",
    "\t\tcase \n",
    "\t\t\twhen ordue.LASTMONTHS in ('G','Z','D') then 8 \n",
    "\t\t\telse ordue.LASTMONTHS \n",
    "\t\tend as LASTMONTHS ,\n",
    "\t\tloanfo.GUARANTEETYPE,STATE,FINANCEORG\n",
    "from ICRLATEST2YEAROVERDUECARD ordue\n",
    "-- 只取贷记卡的信息\n",
    "inner join ICRLOANCARDINFO loanfo on ordue.reportno=loanfo.reportno and ordue.ACCOUNT=loanfo.ACCOUNT \n",
    "left join ICRMESSAGEHEADER head on ordue.reportno=head.reportno\n",
    "where BIZTYPE='贷记卡' and ordue.LASTMONTHS!='C'\n",
    "\"\"\").createOrReplaceTempView(\"temp_creditcard_2yearoverdue\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select \n",
    "\t\tordue.reportno,\n",
    "\t\tordue.ACCOUNT,\n",
    "\t\thead.QUERYTIME,\n",
    "\t\tordue.MONTH,\n",
    "\t\tmonth(date_format(head.QUERYTIME,'%Y-%m-%d')) - month(date_format(concat(ordue.month,'.01'),'%Y-%m-%d')) as gap_months,\n",
    "\t\tordue.LASTMONTHS,ordue.AMOUNT,loanfo.GUARANTEETYPE,STATE,FINANCEORG\n",
    "from ICRLATEST5YEAROVERDUEDETAIL ordue\n",
    "-- 只取贷记卡的信息\n",
    "inner join ICRLOANCARDINFO loanfo on ordue.reportno=loanfo.reportno and ordue.ACCOUNT=loanfo.ACCOUNT \n",
    "left join ICRMESSAGEHEADER head on ordue.reportno=head.reportno\n",
    "where month!='--'  and BIZTYPE='贷记卡'\n",
    "\"\"\").createOrReplaceTempView(\"temp_creditcard_overdue\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select \n",
    "\t\treportno,\n",
    "\t\tACCOUNT,\n",
    "\t\tQUERYTIME,\n",
    "\t\tMONTH,\n",
    "\t\tgap_months,\n",
    "\t\tLASTMONTHS,\n",
    "\t\tGUARANTEETYPE,\n",
    "\t\tSTATE,\n",
    "\t\tFINANCEORG \n",
    "from temp_creditcard_2yearoverdue\n",
    "union \n",
    "select \n",
    "\t\treportno,\n",
    "\t\tACCOUNT,\n",
    "\t\tQUERYTIME,\n",
    "\t\tdate_format(concat(month,'.01'),'%Y-%m'),\n",
    "        gap_months,\n",
    "\t\tLASTMONTHS,\n",
    "\t\tGUARANTEETYPE,\n",
    "\t\tSTATE,\n",
    "\t\tFINANCEORG \n",
    "from temp_creditcard_overdue\n",
    "\"\"\").createOrReplaceTempView(\"temp_creditcard_5yearoverdue\")\n",
    "\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select \n",
    "\t\tordue.reportno,\n",
    "\t\tordue.ACCOUNT,\n",
    "\t\thead.QUERYTIME,\n",
    "\t\tordue.MONTH,\n",
    "\t\tmonth(date_format(head.QUERYTIME,'%Y-%m-%d')) - month(date_format(concat(ordue.month,'.01'),'%Y-%m-%d')) as gap_months,\n",
    "\t\tcase \n",
    "\t\t\twhen ordue.LASTMONTHS in ('G','Z','D') then 8 \n",
    "\t\t\telse ordue.LASTMONTHS \n",
    "\t\tend as LASTMONTHS ,\n",
    "\t\tloanfo.GUARANTEETYPE,\n",
    "\t\tSTATE,\n",
    "\t\tFINANCEORG\n",
    "from ICRLATEST2YEAROVERDUE ordue\n",
    "-- 只取贷款的信息\n",
    "inner join ICRLOANINFO loanfo on ordue.reportno=loanfo.reportno and ordue.ACCOUNT=loanfo.ACCOUNT \n",
    "left join ICRMESSAGEHEADER head on ordue.reportno=head.reportno\n",
    "where ordue.LASTMONTHS!='C'\n",
    "\"\"\").createOrReplaceTempView(\"temp_loan_2yearoverdue\")\n",
    "\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select \n",
    "\t\tordue.reportno,\n",
    "\t\tordue.ACCOUNT,\n",
    "\t\thead.QUERYTIME,\n",
    "\t\tordue.MONTH,\n",
    "\t\tmonth(date_format(head.QUERYTIME,'%Y-%m-%d')) - month(date_format(concat(ordue.month,'.01'),'%Y-%m-%d')) as gap_months,\n",
    "\t\tordue.LASTMONTHS,\n",
    "\t\tordue.AMOUNT,\n",
    "\t\tloanfo.GUARANTEETYPE,\n",
    "\t\tSTATE,FINANCEORG\n",
    "from ICRLATEST5YEAROVERDUEDETAIL ordue\n",
    "-- 只取贷款的信息\n",
    "inner join ICRLOANINFO loanfo on ordue.reportno=loanfo.reportno and ordue.ACCOUNT=loanfo.ACCOUNT \n",
    "left join ICRMESSAGEHEADER head on ordue.reportno=head.reportno\n",
    "where month!='--'\n",
    "\"\"\").createOrReplaceTempView(\"temp_loan_overdue\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select \n",
    "\t\treportno,\n",
    "\t\tACCOUNT,\n",
    "\t\tQUERYTIME,\n",
    "        MONTH,\n",
    "\t\tgap_months,\n",
    "\t\tLASTMONTHS,\n",
    "\t\tGUARANTEETYPE,\n",
    "\t\tSTATE,\n",
    "\t\tFINANCEORG \n",
    "from temp_loan_2yearoverdue\n",
    "union \n",
    "select \n",
    "\t\treportno,\n",
    "\t\tACCOUNT,\n",
    "\t\tQUERYTIME,\n",
    "\t\tdate_format(concat(month,'.01'),'%Y-%m'),gap_months,LASTMONTHS,\n",
    "\t\tGUARANTEETYPE,\n",
    "\t\tSTATE,\n",
    "\t\tFINANCEORG \n",
    "from temp_loan_overdue\n",
    "\"\"\").createOrReplaceTempView(\"temp_loan_5yearoverdue\")\n",
    "\n",
    "\n",
    "############\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "select a.reportno,a.ADDRESS_changenumber,a.addr_changetime,b.ADDRESS,b.RESIDENCETYPE\n",
    "\t\t\t\tfrom (\n",
    "\t\t\t\t\tselect reportno,\n",
    "\t\t\t\t\t\t\tcount(distinct ADDRESS) as ADDRESS_changenumber,\n",
    "\t\t\t\t\t\t\tmax(GETTIME) as addr_changetime\n",
    "\t\t\t\t\t\tfrom ICRRESIDENCE \n",
    "\t\t\t\t\t\tgroup by reportno\n",
    "\t\t\t\t\t\t) a\n",
    "\t\t\t\tleft join ICRRESIDENCE  b on a.reportno=b.REPORTNO \n",
    "\t\t\t\t\t\tand a.addr_changetime=b.GETTIME\n",
    "group by a.reportno,a.ADDRESS_changenumber,a.addr_changetime,b.ADDRESS,b.RESIDENCETYPE\n",
    "\n",
    "\"\"\").createOrReplaceTempView(\"temp_address_change\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "\n",
    "select a.reportno,a.EMPLOYER_changenumber,a.emp_changetime,b.EMPLOYER,b.EMPLOYERADDRESS,\n",
    "\t\t\t\t\tb.OCCUPATION,b.INDUSTRY,b.DUTY,b.TITLE,b.STARTYEAR\n",
    "\t\t\t\tfrom (\n",
    "\t\t\t\t\tselect reportno,count(distinct EMPLOYER) as EMPLOYER_changenumber,max(GETTIME) as emp_changetime\n",
    "\t\t\t\t\t\tfrom ICRPROFESSIONAL \n",
    "\t\t\t\t\t\tgroup by reportno\n",
    "\t\t\t\t\t\t) a\n",
    "\t\t\t\tleft join ICRPROFESSIONAL  b on a.reportno=b.REPORTNO \n",
    "\t\t\t\t\t\tand a.emp_changetime=b.GETTIME \n",
    "group by a.reportno,a.EMPLOYER_changenumber,a.emp_changetime,b.EMPLOYER,b.EMPLOYERADDRESS,b.OCCUPATION,b.INDUSTRY,b.DUTY,b.TITLE,b.STARTYEAR\n",
    "\n",
    "\"\"\").createOrReplaceTempView(\"temp_emp_change\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中间表已运行完成，spark sql 创建所需要的表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select \t distinct head.reportno,\n",
    "\t  \t qry.CERTTYPE,qry.CERTNO,\n",
    "\t   \t'' as is_existreport,\n",
    "\t  \thead.REPORTCREATETIME,\n",
    "\t\tcase \n",
    "\t\t\twhen idty.GENDER='男性' then 1 \n",
    "\t\t\twhen idty.GENDER='女性' then 2 else 9 \n",
    "\t\tend as GENDER,\n",
    "\t\tcase \n",
    "\t\t\twhen idty.BIRTHDAY='--' then substr(qry.CERTNO,7,8) \n",
    "\t\t\telse date_format(idty.BIRTHDAY,'%Y%m%d')\n",
    "\t\tend as BIRTHDAY,\n",
    "\t\tcase\n",
    "\t\t\t when idty.BIRTHDAY ='--' then '-9999999'  \n",
    "\t\t\t else year(head.QUERYTIME)-year(idty.BIRTHDAY)\n",
    "\t\tend as age,\n",
    "\t\tcase \n",
    "\t\t\twhen idty.MARITALSTATE='未婚' then 1 \n",
    "\t\t\twhen idty.MARITALSTATE='已婚' then 2\n",
    "\t\t\twhen idty.MARITALSTATE='丧偶' then 3 \n",
    "\t\t\twhen idty.MARITALSTATE='离婚' then 4 \n",
    "\t\t\telse 9 \n",
    "\t\tend as MARITALSTATE,\n",
    "\t\t'' as is_marriage,idty.mobile,\n",
    "\t\t'' as is_mobile,idty.OFFICETELEPHONENO,\n",
    "\t\t'' as is_OFFICETELEPHONENO,\n",
    "\t\tcase \n",
    "\t\t\twhen idty.EDULEVEL='研究生' then 1\n",
    "\t\t\twhen idty.EDULEVEL='大学本科（简称\"大学\"）' then 2 \n",
    "\t\t\twhen idty.EDULEVEL='大学专科和专科学校（简称\"大专\"）' then 3\n",
    "\t\t\twhen idty.EDULEVEL='中等专业学校或中等技术学校' then 4\n",
    "\t\t\twhen idty.EDULEVEL='技术学校' then 5\n",
    "\t\t\twhen idty.EDULEVEL='高中' then 6\n",
    "\t\t\twhen idty.EDULEVEL='初中' then 7\n",
    "\t\t\twhen idty.EDULEVEL='小学' then 8\n",
    "\t\t\twhen idty.EDULEVEL='文盲或半文盲' then 9 \n",
    "\t\t\telse 0 \n",
    "\t\tend as EDULEVEL,\n",
    "\t\tcase \n",
    "\t\t\twhen idty.EDUDEGREE='名誉博士' then 1  \n",
    "\t\t\twhen idty.EDUDEGREE='博士' then 2\n",
    "\t\t\twhen idty.EDUDEGREE='硕士' then 3 \n",
    "\t\t\twhen idty.EDUDEGREE='学士' then 4\n",
    "\t\t\twhen idty.EDUDEGREE='其他' then 5\n",
    "\t\t\telse 9 \n",
    "\t\tend as EDUDEGREE,\n",
    "\t\tcdt.score,\n",
    "\t\tidty.POSTADDRESS,\n",
    "\t\tidty.REGISTEREDADDRESS as registered_address,\n",
    "\t\tsubstr(idty.REGISTEREDADDRESS,1,locate('省' , idty.REGISTEREDADDRESS,0)) as registered_province,\n",
    "\t\tsubstr(substr(idty.REGISTEREDADDRESS,locate('市' , idty.REGISTEREDADDRESS)), locate('省' , substr(idty.REGISTEREDADDRESS,0,locate('市' , idty.REGISTEREDADDRESS) ) )+1) as registered_city,\n",
    "        \n",
    "        addr.ADDRESS_changenumber,\n",
    "\t\tmonth(head.QUERYTIME)-month(addr.addr_changetime) as ADDRESS_changemonth,\n",
    "\t\taddr.ADDRESS,\n",
    "\t\tcase \n",
    "\t\t\twhen addr.ADDRESS='--' or idty.POSTADDRESS='--' then 0\n",
    "\t\t\twhen addr.ADDRESS!='--' and idty.POSTADDRESS!='--' and addr.ADDRESS=idty.POSTADDRESS then 1 \n",
    "\t\t\telse 0 \n",
    "\t\tend as is_address,\n",
    "\t\tcase \n",
    "\t\t\twhen addr.RESIDENCETYPE='自置' then 1 when addr.RESIDENCETYPE='按揭' then 2 \n",
    "\t\t\twhen addr.RESIDENCETYPE='亲属楼宇' then 3 when addr.RESIDENCETYPE='集体宿舍' then 4 \n",
    "\t\t\twhen addr.RESIDENCETYPE='租房' then 5 when addr.RESIDENCETYPE='共有住宅' then 6 \n",
    "\t\t\twhen addr.RESIDENCETYPE='其他' then 7 else 9 \n",
    "\t\tend as RESIDENCE_TYPE,\n",
    "\t\tpro.EMPLOYER_changenumber,\n",
    "\t\tmonth(head.QUERYTIME)-month(pro.emp_changetime) as EMPLOYER_changemonth,\n",
    "\t\tpro.EMPLOYER,pro.EMPLOYERADDRESS as EMPLOYER_ADDRESS,\n",
    "\t\tsubstr(pro.EMPLOYERADDRESS,1,locate('省' , pro.EMPLOYERADDRESS)) as EMPLOYER_province,\n",
    "\t\tsubstr(substr(pro.EMPLOYERADDRESS,0,locate('市' , pro.EMPLOYERADDRESS)),locate('省' , substr(pro.EMPLOYERADDRESS,0,locate('市' , pro.EMPLOYERADDRESS)))+1) as EMPLOYER_city,\n",
    "\t\tpro.OCCUPATION,\n",
    "\t\tpro.INDUSTRY,\n",
    "\t\tpro.DUTY,\n",
    "\t\tpro.TITLE,\n",
    "\t\tmonth(head.QUERYTIME)-month(pro.emp_changetime) as EMPLOYER_changemonth2,\n",
    "\t\tcdt.ANNOUNCECOUNT,cdt.DISSENTCOUNT,\n",
    "        (date_format(head.QUERYTIME,'%Y')-pro.STARTYEAR)*12+date_format(head.QUERYTIME,'%m') as work_year,\n",
    "\t\tnow(),\n",
    "\t\tnow(),\n",
    "\t\tnow()\n",
    "from ICRMESSAGEHEADER head\n",
    "left join ICRQUERYREQ qry on head.reportno=qry.reportno\n",
    "left join ICRIDENTITY idty on qry.reportno=idty.reportno\n",
    "left join ICRCREDITCUE cdt on qry.reportno=cdt.reportno\n",
    "left join temp_address_change addr on qry.reportno=addr.reportno\n",
    "left join temp_emp_change pro on  qry.reportno=pro.reportno\n",
    "\"\"\").createOrReplaceTempView(\"dw_baseinfo_detail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "select \n",
    "\t\thead.reportno,\n",
    "\t\tnvl(t1.SUITOBJECT_num,0) as SUITOBJECT_num,\n",
    "\t\tnvl(t1.SUITOBJECTMONEY_amount,0) as SUITOBJECTMONEY_amount,\n",
    "\t\tnvl(t2.already_enforce_object_num,0) as already_enforce_object_num,\n",
    "\t\tnvl(t2.enforce_object_amount,0) as enforce_object_amount,\n",
    "\t\tnvl(t2.already_enforce_object_amount,0) as already_enforce_object_amount,\n",
    "\t\tnvl(t3.punish_num,0) as punish_num,\n",
    "\t\tnvl(t3.punish_money,0) as punish_money,\n",
    "\t\tnvl(t4.tax_num,0) as tax_num,\n",
    "\t\tnvl(t4.tax_amount,0) as tax_amount,\n",
    "\t\tnvl(t5.accfund_state,-9999999) as accfund_state,\n",
    "\t\tnvl(t5.accfund_pay,-9999999) as accfund_pay,\n",
    "\t\tnvl(t5.accfund_ownpercent,-9999999) as accfund_ownpercent,\n",
    "\t\tnvl(t5.accfund_cimpercent,-9999999) as accfund_cimpercent,\n",
    "\t\tnvl(t6.ownbasicmoney,-9999999) as ownbasicmoney ,\n",
    "\t\tnow(),\n",
    "\t\tnow(),\n",
    "\t\tnow()\n",
    "from ICRMESSAGEHEADER head\n",
    "left join \n",
    "(\n",
    "\tselect \n",
    "\t\t\treportno,\n",
    "\t\t\tcount(distinct SUITOBJECT) as SUITOBJECT_num,\n",
    "\t\t\tsum(nvl(SUITOBJECTMONEY,0)) as SUITOBJECTMONEY_amount\n",
    "\tfrom ICRCIVILJUDGEMENT\n",
    "\tgroup by reportno\n",
    "\n",
    ")t1 on head.reportno=t1.reportno\n",
    "\n",
    "left join \n",
    "(\n",
    "\tselect \n",
    "\t\treportno,\n",
    "\t\tcount(distinct ALREADYENFORCEOBJECT) as already_enforce_object_num,\n",
    "\t\tsum(nvl(ENFORCEOBJECTMONEY,0)) as enforce_object_amount,\n",
    "\t\tsum(nvl(ALREADYENFORCEOBJECTMONEY,0)) as already_enforce_object_amount\n",
    "\tfrom ICRFORCEEXECUTION\n",
    "\tgroup by reportno\n",
    "\n",
    ")t2 on head.reportno=t2.reportno\n",
    "\n",
    "left join \n",
    "(\n",
    "\tselect \n",
    "\t\treportno,\n",
    "\t\tcount(*) as  punish_num, \n",
    "\t\tsum(nvl(MONEY,0)) as punish_money\n",
    "\tfrom ICRADMINPUNISHMENT\n",
    "\tgroup by reportno\n",
    "\n",
    ")t3 on head.reportno=t3.reportno\n",
    "\n",
    "left join \n",
    "(\n",
    "\tselect \n",
    "\t\treportno,\n",
    "\t\tcount(*) as  tax_num, \n",
    "\t\tsum(nvl(TAXARREAAMOUNT,0)) as tax_amount\n",
    "\n",
    "\tfrom ICRTAXARREAR\n",
    "\tgroup by reportno\n",
    "\n",
    ")t4 on head.reportno=t4.reportno\n",
    "\n",
    "left join \n",
    "(\t\n",
    "\t\tselect \n",
    "\t\t\tt1.reportno,\n",
    "\t\t\tt1.STATE as accfund_state,\n",
    "\t\t\tt1.PAY as accfund_pay,\n",
    "\t\t\tt1.OWNPERCENT as accfund_ownpercent,\n",
    "\t\t\tt1.COMPERCENT as accfund_cimpercent\n",
    "\n",
    "\tfrom ICRACCFUND t1\n",
    "\n",
    "\tjoin (\n",
    "\t\t\tselect \n",
    "\t\t\t\ta.reportno,\n",
    "\t\t\t\ta.tomonth,\n",
    "\t\t\t\ta.gettime,\n",
    "\t\t\t\tmax(registerdate) as registerdate\n",
    "\t\t\tfrom ICRACCFUND a\n",
    "\t\t \tjoin \n",
    "\t\t\t\t(\n",
    "\t\t\t\t\tselect \n",
    "\t\t\t\t\t\ta.reportno,\n",
    "\t\t\t\t\t\ta.tomonth,\n",
    "\t\t\t\t\t\tmax(gettime) as gettime\n",
    "\t\t\t\t\tfrom ICRACCFUND a\n",
    "\t\t\t \t\tjoin \n",
    "\t\t\t\t\t(\n",
    "\t\t\t\t\t\tselect \n",
    "\t\t\t\t\t\t\treportno,\n",
    "\t\t\t\t\t\t\tmax(tomonth) as tomonth\n",
    "\t\t\t\t\t\tfrom ICRACCFUND\n",
    "\t\t\t\t\tgroup by reportno\n",
    "\t\t\t)b on a.reportno=b.reportno and a.tomonth=b.tomonth group by a.reportno,a.tomonth\n",
    "\n",
    "\t\t)b on a.reportno=b.reportno and a.tomonth=b.tomonth and a.gettime=b.gettime group by a.reportno,a.tomonth,a.gettime\n",
    "\n",
    "\t)t2 on t1.reportno=t2.reportno and t1.tomonth=t2.tomonth and t1.gettime=t2.gettime and t1.registerdate=t2.registerdate group by t1.reportno,t1.STATE,t1.PAY,t1.OWNPERCENT,t1.COMPERCENT ---group by 添加了t1.STATE,t1.PAY,t1.OWNPERCENT,t1.COMPERCENT\n",
    "\n",
    ")t5 on head.reportno=t5.reportno\n",
    "\n",
    "left join \n",
    "(\t\n",
    "\tselect \n",
    "\t\tt1.reportno,\n",
    "\t\tt1.OWNBASICMONEY as ownbasicmoney\n",
    "\n",
    "\tfrom ICRENDOWMENTINSURANCEDEPOSIT t1\n",
    "\tjoin (\n",
    "\t\tselect \n",
    "\t\t\ta.reportno,\n",
    "\t\t\ta.gettime,\n",
    "\t\t\tmax(registerdate) as registerdate\n",
    "\t\tfrom ICRENDOWMENTINSURANCEDEPOSIT a\n",
    "\t\t join \n",
    "\t\t\t(\n",
    "\t\t\t\tselect \n",
    "\t\t\t\t\treportno,\n",
    "\t\t\t\t\tmax(gettime) as gettime\n",
    "\t\t\t\tfrom ICRENDOWMENTINSURANCEDEPOSIT\n",
    "\t\t\t\tgroup by reportno\n",
    "\t\t\t)b on a.reportno=b.reportno and a.gettime=b.gettime group by a.reportno,a.gettime ---group by 修改了\n",
    "\n",
    "\t\t)t2 on t1.reportno=t2.reportno and t1.registerdate=t2.registerdate and t1.gettime=t2.gettime group by t1.reportno,t1.OWNBASICMONEY ---group by 添加了\n",
    "\n",
    ")t6 on head.reportno=t6.reportno\n",
    "\n",
    "\"\"\").createOrReplaceTempView(\"dw_other_info\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "select reportno from ICRMESSAGEHEADER\n",
    "where reportno not in(select distinct reportno from temp_creditcard_5yearoverdue)\n",
    "and reportno not in(select distinct reportno from temp_loan_5yearoverdue)\n",
    "and reportno not in(select distinct reportno from temp_standcard_2yearoverdue)\n",
    "\n",
    "\"\"\").createOrReplaceTempView(\"temp_never_overdue\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "select reportno\n",
    "from ICRMESSAGEHEADER\n",
    "where reportno not in (select reportno from ICRLOANINFO)\n",
    "and reportno not in (select reportno from ICRLOANCARDINFO)\n",
    "\n",
    "\"\"\").createOrReplaceTempView(\"temp_without_account\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "\n",
    "select \n",
    "\t\thead.reportno,\n",
    "\t\tnvl(mth.max_gap_months,-9999999) as max_gap_months,\n",
    "\t\tnvl(mth.min_gap_months,-9999999) as min_gap_months,\n",
    "\t\tcase \n",
    "\t\t\twhen wthacc.reportno is not null then -9999999\n",
    "\t\t\twhen wthacc.reportno is null and nrod.reportno is not null then -9999998\n",
    "\t\t\telse all_mth1.mth1_gap_months \n",
    "\t\tend as mth1_gap_months,\n",
    "\t\tcase \n",
    "\t\t\twhen wthacc.reportno is not null then -9999999\n",
    "\t\t\twhen wthacc.reportno is null and nrod.reportno is not null then -9999998\n",
    "\t\t\telse all_mth2.mth2_gap_months \n",
    "\t\tend as mth2_gap_months,\t\n",
    "\t\tcase \n",
    "\t\t\twhen wthacc.reportno is not null then -9999999\n",
    "\t\t\twhen wthacc.reportno is null and nrod.reportno is not null then -9999998\n",
    "\t\t\telse all_mth3.mth3_gap_months \n",
    "\t\tend as mth3_gap_months,\t\t\n",
    "\t\tcase \n",
    "\t\t\twhen wthacc.reportno is not null then -9999999\n",
    "\t\t\twhen wthacc.reportno is null and nrod.reportno is not null then -9999998\n",
    "\t\t\telse all_mth4.mth4_gap_months \n",
    "\t\tend as mth4_gap_months,\t\n",
    "\t\tcase \n",
    "\t\t\twhen wthacc.reportno is not null then -9999999\n",
    "\t\t\twhen wthacc.reportno is null and nrod.reportno is not null then -9999998\n",
    "\t\t\telse all_mth5.mth5_gap_months \n",
    "\t\tend as mth5_gap_months,\t\n",
    "\t\t\n",
    "\t\tmonth(head.QUERYTIME)-month(max_qtime) as max_query_mth,\t\n",
    "\t\tday(head.QUERYTIME) - day(max_qtime) as max_query_day,\n",
    "\t\tnvl(loan.CREDITLIMITAMOUNT,-9999999) as max_loan,\n",
    "\t\tnvl(cloan.CREDITLIMITAMOUNT,-9999999) as max_cloan,\n",
    "\t\tnvl(sloan.CREDITLIMITAMOUNT,-9999999) as max_sloan,\n",
    "\t\tcase \n",
    "\t\t\twhen loan.CREDITLIMITAMOUNT is null then -9999999 \n",
    "\t\t\telse nvl(outl.out_num,0) \n",
    "\t\tend as out_loan_num,now(),now(),now()\n",
    "\t\n",
    "from ICRMESSAGEHEADER head\n",
    "\n",
    "left join(\n",
    "\n",
    "\t\tselect \n",
    "\t\t\treportno,\n",
    "\t\t\tmax(gap_months) as max_gap_months,\n",
    "\t\t\tmin(gap_months) as min_gap_months\n",
    "\t\tfrom \n",
    "\t\t(\n",
    "\t\t\tselect \n",
    "\t\t\t\tinfo.reportno,\n",
    "\t\t\t\tcase \n",
    "\t\t\t\t\twhen info.ENDDATE='-' then substr(PAYMENTCYC,1,length(PAYMENTCYC)-3)+0\n",
    "\t\t\t\t\twhen info.ENDDATE!='-' and day(date_format(info.ENDDATE,'%Y-%m-%d'))-day(date_format(head.QUERYTIME,'%Y-%m-%d'))<0 then month(info.ENDDATE )-month(info.OPENDATE)\n",
    "\t\t\t\t\telse month(head.QUERYTIME) - month(info.OPENDATE)\n",
    "\t\t\t\tend as gap_months\n",
    "\t\t\tfrom ICRLOANINFO info\n",
    "\n",
    "\t\t\tleft join ICRMESSAGEHEADER head on info.reportno=head.reportno  where PAYMENTCYC!='-' or state!='结清' or ENDDATE!='-' --去掉了and\n",
    "\t\t\n",
    "\t\t)t group by reportno\n",
    "\n",
    "\t)mth on head.reportno=mth.reportno\n",
    "\n",
    "left join temp_never_overdue nrod on nrod.reportno=head.reportno\n",
    "\n",
    "left join temp_without_account wthacc on wthacc.reportno=head.reportno\n",
    "\n",
    "left join \n",
    "(\n",
    "\t\tselect \n",
    "\t\t\tt1.reportno,\n",
    "\t\t\tmonth(date_format(head.QUERYTIME,'%Y-%m-%d'))-month(date_format(concat(t1.month,'.01'),'%Y-%m-%d')) as mth1_gap_months\n",
    "\t\tfrom (\n",
    "\t\t\tselect \n",
    "\t\t\t\treportno,\n",
    "\t\t\t\tmax(month) as month\n",
    "\t\t\tfrom \n",
    "\t\t\t(\n",
    "\t\t\t\tselect reportno,month,LASTMONTHS from temp_standcard_2yearoverdue\n",
    "\t\t\t\tunion all\n",
    "\t\t\t\tselect reportno,month,LASTMONTHS from temp_creditcard_5yearoverdue\n",
    "\t\t\t\tunion all\n",
    "\t\t\t\tselect reportno,month,LASTMONTHS from temp_loan_5yearoverdue\n",
    "\t\t\t\n",
    "\t\t\t)t   where LASTMONTHS>0 group by reportno\n",
    "\t\t)t1 \n",
    "\n",
    "\t\tleft join ICRMESSAGEHEADER head on t1.reportno=head.reportno\n",
    "\n",
    ") all_mth1 on all_mth1.reportno=head.reportno\n",
    "\n",
    "\n",
    "left join \n",
    "(\n",
    "select \n",
    "\t\tt1.reportno,\n",
    "\t\tmonth(date_format(head.QUERYTIME,'%Y-%m-%d')) - month(date_format(concat(t1.month,'.01'),'%Y-%m-%d')) as mth2_gap_months\n",
    "from (\n",
    "\tselect \n",
    "\t\treportno,\n",
    "\t\tmax(month) as month\n",
    "\tfrom \n",
    "\t\t(\n",
    "\t\t\tselect reportno,month,LASTMONTHS from temp_standcard_2yearoverdue\n",
    "\t\t\tunion all\n",
    "\t\t\tselect reportno,month,LASTMONTHS from temp_creditcard_5yearoverdue\n",
    "\t\t\tunion all\n",
    "\t\t\tselect reportno,month,LASTMONTHS from temp_loan_5yearoverdue\n",
    "\n",
    "\t\t)t where LASTMONTHS>1 group by reportno\n",
    "\t)t1 \n",
    "\n",
    "\tleft join ICRMESSAGEHEADER head on t1.reportno=head.reportno\n",
    "\n",
    ") all_mth2 on all_mth2.reportno=head.reportno\n",
    "\n",
    "left join \n",
    "(\n",
    "\tselect \n",
    "\t\tt1.reportno,\n",
    "\t\tmonth(date_format(head.QUERYTIME,'%Y-%m-%d'))- month(date_format(concat(t1.month,'.01'),'%Y-%m-%d'))  as mth3_gap_months\n",
    "\tfrom (\n",
    "\t\tselect \n",
    "\t\t\treportno,\n",
    "\t\t\tmax(month) as month\n",
    "\t\tfrom \n",
    "\t\t\t(\n",
    "\t\t\t\tselect reportno,month,LASTMONTHS from temp_standcard_2yearoverdue\n",
    "\t\t\t\tunion all\n",
    "\t\t\t\tselect reportno,month,LASTMONTHS from temp_creditcard_5yearoverdue\n",
    "\t\t\t\tunion all\n",
    "\t\t\t\tselect reportno,month,LASTMONTHS from temp_loan_5yearoverdue\n",
    "\t\t\n",
    "\t\t\t)t where LASTMONTHS>2 group by reportno\n",
    "\t\t)t1 \n",
    "\t\tleft join ICRMESSAGEHEADER head on t1.reportno=head.reportno\n",
    ") all_mth3 on all_mth3.reportno=head.reportno\n",
    "\n",
    "left join \n",
    "(\n",
    "\tselect \n",
    "\t\t\tt1.reportno,\n",
    "\t\t\tmonth(date_format(head.QUERYTIME,'%Y-%m-%d'))-month(date_format(concat(t1.month,'.01'),'%Y-%m-%d')) as mth4_gap_months\n",
    "\tfrom (\n",
    "\t\t\tselect \n",
    "\t\t\t\treportno,\n",
    "\t\t\t\tmax(month) as month\n",
    "\t\t\tfrom \n",
    "\t\t\t(\n",
    "\t\t\t\tselect reportno,month,LASTMONTHS from temp_standcard_2yearoverdue\n",
    "\t\t\t\tunion all\n",
    "\t\t\t\tselect reportno,month,LASTMONTHS from temp_creditcard_5yearoverdue\n",
    "\t\t\t\tunion all\n",
    "\t\t\t\tselect reportno,month,LASTMONTHS from temp_loan_5yearoverdue\n",
    "\n",
    "\t\t\t)t where LASTMONTHS>3 group by reportno\n",
    "\t\t)t1 \n",
    "\t\tleft join ICRMESSAGEHEADER head on t1.reportno=head.reportno\n",
    ") all_mth4 on all_mth4.reportno=head.reportno\n",
    "\n",
    "left join \n",
    "(\n",
    "\tselect \n",
    "\t\tt1.reportno,\n",
    "\t\tmonth(date_format(head.QUERYTIME,'%Y-%m-%d'))-month(date_format(concat(t1.month,'.01'),'%Y-%m-%d')) as mth5_gap_months\n",
    "\tfrom (\n",
    "\t\tselect \n",
    "\t\t\treportno,max(month) as month\n",
    "\t\t\tfrom \n",
    "\t\t\t(\n",
    "\t\t\t\tselect reportno,month,LASTMONTHS from temp_standcard_2yearoverdue\n",
    "\t\t\t\tunion all\n",
    "\t\t\t\tselect reportno,month,LASTMONTHS from temp_creditcard_5yearoverdue\n",
    "\t\t\t\tunion all\n",
    "\t\t\t\tselect reportno,month,LASTMONTHS from temp_loan_5yearoverdue\n",
    "\n",
    "\t\t\t)t where LASTMONTHS>4 group by reportno\n",
    "\t\t)t1 \n",
    "\t\tleft join ICRMESSAGEHEADER head on t1.reportno=head.reportno\n",
    ") all_mth5 on all_mth5.reportno=head.reportno\n",
    "\n",
    "left join \n",
    "\n",
    "(\n",
    "\tselect \n",
    "\t\t\treportno,\n",
    "\t\t\tmax(querytime) as max_qtime\n",
    "\tfrom QUERY_HISTORY\n",
    " \twhere reportno is not null \n",
    "\tgroup by reportno\n",
    ") max_q on max_q.reportno=head.reportno\n",
    "\n",
    "left join \n",
    "\n",
    "(\tselect \n",
    "\t\treportno,\n",
    "\t\tmax(CREDITLIMITAMOUNT) as CREDITLIMITAMOUNT \n",
    "\tfrom ICRLOANINFO\n",
    "\tgroup by reportno\n",
    "\n",
    ") loan on loan.reportno=head.reportno\n",
    "\n",
    "left join \n",
    "\n",
    "(\tselect \n",
    "\t\treportno,\n",
    "\t\tmax(CREDITLIMITAMOUNT) as CREDITLIMITAMOUNT \n",
    "\tfrom ICRLOANINFO\n",
    "\twhere GUARANTEETYPE='信用/免担保'\n",
    "\tgroup by reportno\n",
    ")cloan on cloan.reportno=head.reportno\n",
    "\n",
    "left join \n",
    "\n",
    "(\tselect \n",
    "\t\treportno,\n",
    "\t\tmax(CREDITLIMITAMOUNT) as CREDITLIMITAMOUNT \n",
    "\tfrom ICRLOANINFO\n",
    "\twhere GUARANTEETYPE='信用/免担保' and state!='结清'\n",
    "\tgroup by reportno\n",
    ") sloan on sloan.reportno=head.reportno\n",
    "\n",
    "left join\n",
    "\n",
    "(\tselect \n",
    "\t\treportno,\n",
    "\t\tcount(*)  as out_num \n",
    "\tfrom ICRLOANINFO\n",
    "\twhere CLASS5STATE='转出'\n",
    "\tgroup by reportno\n",
    ")outl on outl.reportno=head.reportno\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\").createOrReplaceTempView(\"dw_other_info2\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "\n",
    "select reportno,\n",
    "\tsum(mth1_card_query_count) as mth1_card_query_count,\n",
    "\tcount(distinct mth1_card_query_org) as mth1_card_query_org,\n",
    "\tsum(mth1_loan_query_count) as mth1_loan_query_count,\n",
    "\tcount(distinct mth1_loan_query_org) as mth1_loan_query_org,\n",
    "\t0 as mth1_person_query_count,\n",
    "\tcount(distinct mth1_loan_card_query_org) as mth1_loan_card_query_org,\n",
    "\n",
    "\tsum(mth3_card_query_count) as mth3_card_query_count,\n",
    "\tcount(distinct mth3_card_query_org) as mth3_card_query_org,\n",
    "\tsum(mth3_loan_query_count) as mth3_loan_query_count,\n",
    "\tcount(distinct mth3_loan_query_org) as mth3_loan_query_org,\n",
    "\t0 as mth3_person_query_count,\n",
    "\tcount(distinct mth3_loan_card_query_org) as mth3_loan_card_query_org,\t\n",
    "\t\n",
    "\tsum(mth6_card_query_count) as mth6_card_query_count,\n",
    "\tcount(distinct mth6_card_query_org) as mth6_card_query_org,\n",
    "\tsum(mth6_loan_query_count) as mth6_loan_query_count,\n",
    "\tcount(distinct mth6_loan_query_org) as mth6_loan_query_org,\n",
    "\t0 as mth6_person_query_count,\n",
    "\tcount(distinct mth6_loan_card_query_org) as mth6_loan_card_query_org,\n",
    "\t\n",
    "\tsum(mth9_card_query_count) as mth9_card_query_count,\n",
    "\tcount(distinct mth9_card_query_org) as mth9_card_query_org,\n",
    "\tsum(mth9_loan_query_count) as mth9_loan_query_count,\n",
    "\tcount(distinct mth9_loan_query_org) as mth9_loan_query_org,\n",
    "\t0 as mth9_person_query_count,\n",
    "\tcount(distinct mth9_loan_card_query_org) as mth9_loan_card_query_org,\n",
    "\t\n",
    "\tsum(mth12_card_query_count) as mth12_card_query_count,\n",
    "\tcount(distinct mth12_card_query_org) as mth12_card_query_org,\n",
    "\tsum(mth12_loan_query_count) as mth12_loan_query_count,\n",
    "\tcount(distinct mth12_loan_query_org) as mth12_loan_query_org,\n",
    "\t0 as mth12_person_query_count,\n",
    "\tcount(distinct mth12_loan_card_query_org) as mth12_loan_card_query_org,\n",
    "\t\n",
    "\tnow(),now(),now()\n",
    "\n",
    "from (\n",
    "select head.reportno,\n",
    "\tcase when (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))<=30 \n",
    "\t\t\tand (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))>=0\n",
    "\t\t\tand his.queryreason='信用卡审批'\n",
    "\t\tthen 1 else 0 end as mth1_card_query_count,\n",
    "\tcase when (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))<=30 \n",
    "\t\t\tand (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))>=0\n",
    "\t\t\tand his.queryreason='信用卡审批'\n",
    "\t\tthen QUERIER else null end as mth1_card_query_org,\t\t\n",
    "\tcase when (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))<=30 \n",
    "\t\t\tand (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))>=0\n",
    "\t\t\tand his.queryreason='贷款审批'\n",
    "\t\tthen 1 else 0 end as mth1_loan_query_count,\t\n",
    "\tcase when (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))<=30 \n",
    "\t\t\tand (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))>=0\n",
    "\t\t\tand his.queryreason='贷款审批'\n",
    "\t\tthen QUERIER else null end as mth1_loan_query_org,\t\n",
    "\tsumm.recordSum3 as mth1_person_query_count,\n",
    "\tcase when (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))<=30 \n",
    "\t\t\tand (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))>=0\n",
    "\t\t\t\tand (his.queryreason='信用卡审批' or his.queryreason='贷款审批')\n",
    "\t\tthen QUERIER else null end as mth1_loan_card_query_org,\n",
    "\t\t\n",
    "\tcase when (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))<=90 \n",
    "\t\t\tand (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))>=0\n",
    "\t\t\tand his.queryreason='信用卡审批'\n",
    "\t\tthen 1 else 0 end as mth3_card_query_count,\n",
    "\tcase when (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))<=90 \n",
    "\t\t\tand (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))>=0\n",
    "\t\t\tand his.queryreason='信用卡审批'\n",
    "\t\tthen QUERIER else null end as mth3_card_query_org,\t\t\n",
    "\tcase when (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))<=90 \n",
    "\t\t\tand (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))>=0\n",
    "\t\t\tand his.queryreason='贷款审批'\n",
    "\t\tthen 1 else 0 end as mth3_loan_query_count,\t\n",
    "\tcase when (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))<=90 \n",
    "\t\t\tand (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))>=0\n",
    "\t\t\tand his.queryreason='贷款审批'\n",
    "\t\tthen QUERIER else null end as mth3_loan_query_org,\t\n",
    "\t0 as mth3_person_query_count,\n",
    "\tcase when (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))<=90 \n",
    "\t\t\tand (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))>=0\n",
    "\t\t\tand (his.queryreason='信用卡审批' or his.queryreason='贷款审批')\n",
    "\t\tthen QUERIER else null end as mth3_loan_card_query_org,\t\n",
    "\n",
    "\tcase when (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))<=180 \n",
    "\t\t\tand (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))>=0\n",
    "\t\t\tand his.queryreason='信用卡审批'\n",
    "\t\tthen 1 else 0 end as mth6_card_query_count,\n",
    "\tcase when (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))<=180 \n",
    "\t\t\tand (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))>=0\n",
    "\t\t\tand his.queryreason='信用卡审批'\n",
    "\t\tthen QUERIER else null end as mth6_card_query_org,\t\t\n",
    "\tcase when (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))<=180 \n",
    "\t\t\tand (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))>=0\n",
    "\t\t\tand his.queryreason='贷款审批'\n",
    "\t\tthen 1 else 0 end as mth6_loan_query_count,\t\n",
    "\tcase when (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))<=180 \n",
    "\t\t\tand (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))>=0\n",
    "\t\t\tand his.queryreason='贷款审批'\n",
    "\t\tthen QUERIER else null end as mth6_loan_query_org,\t\n",
    "\t0 as mth6_person_query_count,\n",
    "\tcase when (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))<=180 \n",
    "\t\t\tand (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))>=0\n",
    "\t\t\t\tand (his.queryreason='信用卡审批' or his.queryreason='贷款审批')\n",
    "\t\tthen QUERIER else null end as mth6_loan_card_query_org,\n",
    "\n",
    "\tcase when (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))<=270 \n",
    "\t\t\tand (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))>=0\n",
    "\t\t\tand his.queryreason='信用卡审批'\n",
    "\t\tthen 1 else 0 end as mth9_card_query_count,\n",
    "\tcase when (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))<=270 \n",
    "\t\t\tand (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))>=0\n",
    "\t\t\tand his.queryreason='信用卡审批'\n",
    "\t\tthen QUERIER else null end as mth9_card_query_org,\t\t\n",
    "\tcase when (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))<=270 \n",
    "\t\t\tand (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))>=0\n",
    "\t\t\tand his.queryreason='贷款审批'\n",
    "\t\tthen 1 else 0 end as mth9_loan_query_count,\t\n",
    "\tcase when (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))<=270 \n",
    "\t\t\tand (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))>=0\n",
    "\t\t\tand his.queryreason='贷款审批'\n",
    "\t\tthen QUERIER else null end as mth9_loan_query_org,\t\n",
    "\t0 as mth9_person_query_count,\n",
    "\tcase when (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))<=270 \n",
    "\t\t\tand (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))>=0 \n",
    "\t\t\tand (his.queryreason='信用卡审批' or his.queryreason='贷款审批')\n",
    "\t\tthen QUERIER else null end as mth9_loan_card_query_org,\t\t\t\n",
    "\n",
    "\tcase when (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))<=360 \n",
    "\t\t\tand (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))>=0\n",
    "\t\t\tand his.queryreason='信用卡审批'\n",
    "\t\tthen 1 else 0 end as mth12_card_query_count,\n",
    "\tcase when (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))<=360 \n",
    "\t\t\tand (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))>=0\n",
    "\t\t\tand his.queryreason='信用卡审批'\n",
    "\t\tthen QUERIER else null end as mth12_card_query_org,\t\t\n",
    "\tcase when (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))<=360 \n",
    "\t\t\tand (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))>=0\n",
    "\t\t\tand his.queryreason='贷款审批'\n",
    "\t\tthen 1 else 0 end as mth12_loan_query_count,\t\n",
    "\tcase when (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))<=360 \n",
    "\t\t\tand (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))>=0\n",
    "\t\t\tand his.queryreason='贷款审批'\n",
    "\t\tthen QUERIER else null end as mth12_loan_query_org,\t\n",
    "\t0 as mth12_person_query_count,\n",
    "\tcase when (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))<=360 \n",
    "\t\t\tand (day(DATE_FORMAT(head.QUERYTIME,'%Y-%m-%d'))-day(date_format(his.QUERYDATE,'%Y-%m-%d')))>=0\n",
    "\t\t\tand (his.queryreason='信用卡审批' or his.queryreason='贷款审批')\n",
    "\t\tthen QUERIER else null end as mth12_loan_card_query_org\n",
    "from ICRMESSAGEHEADER head \n",
    "left join ICRRECORDDETAIL his on his.reportno=head.reportno\n",
    "left join ICRRECORDSUMMARY summ on summ.reportno=head.reportno\n",
    ")t\n",
    "group by reportno\n",
    "\n",
    "\"\"\").createOrReplaceTempView(\"dw_query_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "select \n",
    "\t\treportno,\n",
    "\t\tmax(baddebt_count) as baddebt_count,\n",
    "\t\tmax(baddebt_amount) as baddebt_amount,\n",
    "\t\tmax(asset_num) as asset_num,\n",
    "\t\tmax(asset_amount) as asset_amount,\n",
    "\t\tmax(guarantor_num) as guarantor_num,\n",
    "\t\tmax(guarantor_amount) as guarantor_amount,\n",
    "\t\tmax(unnormal_cardguarante_num) as unnormal_cardguarante_num,\n",
    "\t\tmax(unnormal_cardguarante_amount) as unnormal_cardguarante_amount,\n",
    "\t\tmax(guarante_num) as guarante_num,\n",
    "\t\tmax(guarante_amount) as guarante_amount,\n",
    "\t\tmax(guarante_balance) as guarante_balance,\n",
    "\t\tnow() as create_time,\n",
    "\t\tnow() as update_time,\n",
    "\t\tnow() as etl_date\n",
    "from(\n",
    "\t\tselect \n",
    "\t\t\t\tfell.reportno,\n",
    "\t\t\t\tfell.COUNT as baddebt_count,\n",
    "\t\t\t\tfell.BALANCE as baddebt_amount,\n",
    "\t\t\t\tfell.COUNT2 as asset_num,\n",
    "\t\t\t\tfell.BALANCE2 as asset_amount,\n",
    "\t\t\t\tfell.COUNT3 as guarantor_num,\n",
    "\t\t\t\tfell.BALANCE3 as guarantor_amount,\n",
    "\t\t\t\tnull as unnormal_cardguarante_num,\n",
    "\t\t\t\tnull as unnormal_cardguarante_amount,\n",
    "\t\t\t\tnull as guarante_num,\n",
    "\t\t\t\tnull as guarante_amount,\n",
    "\t\t\t\tnull as guarante_balance\n",
    "\t\tfrom ICRFELLBACKSUMMARY fell\n",
    "union all\n",
    "\n",
    "\t\tselect \n",
    "\t\t\t\treportno,\n",
    "\t\t\t\tnull as baddebt_count,\n",
    "\t\t\t\tnull as baddebt_amount,\n",
    "\t\t\t\tnull as asset_num,\n",
    "\t\t\t\tnull as asset_amount,\n",
    "\t\t\t\tnull as guarantor_num,\n",
    "\t\t\t\tnull as guarantor_amount,\n",
    "\t\t\t\tcount(*) as unnormal_cardguarante_num,\n",
    "\t\t\t\tsum(ifnull(GUARANTEEBALANCE,0)) as unnormal_cardguarante_amount,\n",
    "\t\t\t\tnull as guarante_num,\n",
    "\t\t\t\tnull as guarante_amount,\n",
    "\t\t\t\tnull as guarante_balance\n",
    "\t\tfrom ICRGUARANTEE\n",
    "\t\twhere CLASS5STATE!='正常'\n",
    "\t\tgroup by reportno\n",
    "\n",
    "union all\n",
    "\t\tselect \n",
    "\t\t\t\treportno,\n",
    "\t\t\t\tnull as baddebt_count,\n",
    "\t\t\t\tnull as baddebt_amount,\n",
    "\t\t\t\tnull as asset_num,\n",
    "\t\t\t\tnull as asset_amount,\n",
    "\t\t\t\tnull as guarantor_num,\n",
    "\t\t\t\tnull as guarantor_amount,\n",
    "\t\t\t\tnull as unnormal_cardguarante_num,\n",
    "\t\t\t\tnull as unnormal_cardguarante_amount,\n",
    "\t\t\t\tCOUNT as guarante_num,\n",
    "\t\t\t\tAMOUNT as guarante_amount,\n",
    "\t\t\t\tBALANCE as guarante_balance\n",
    "\t\tfrom ICRGUARANTEESUMMARY\n",
    ")t\n",
    "group by reportno\n",
    "\n",
    "\n",
    "\n",
    "\"\"\").createOrReplaceTempView(\"dw_other_creditcue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "select \n",
    "\t\tt1.REPORTNO,\n",
    "\t\tt1.CERTTYPE,\n",
    "\t\tt1.CERTNO,\n",
    "        t1.is_existreport,\n",
    "\t\tt1.REPORTCREATETIME,\n",
    "        t1.GENDER,\n",
    "\t\tt1.BIRTHDAY,\n",
    "\t\tt1.age,\n",
    "\t\tt1.MARITALSTATE,\n",
    "\t\tt1.is_marriage,\n",
    "\t\tt1.MOBILE,\n",
    "\t\tt1.is_mobile,\n",
    "\t\tt1.OFFICETELEPHONENO,\n",
    "\t\tt1.is_OFFICETELEPHONENO,\n",
    "\t\tt1.EDULEVEL,\n",
    "\t\tt1.EDUDEGREE,\n",
    "\t\tt1.SCORE,\n",
    "\t\tt1.POSTADDRESS,\n",
    "\t\tt1.REGISTERED_ADDRESS,\n",
    "\t\tt1.registered_province,\n",
    "\t\tt1.registered_city,\n",
    "\t\tt1.ADDRESS_changenumber,\n",
    "\t\tt1.ADDRESS_changemonth,\n",
    "\t\tt1.ADDRESS,\n",
    "\t\tt1.is_address,\n",
    "\t\tt1.RESIDENCE_TYPE,\n",
    "\t\tt1.EMPLOYER_changenumber,\n",
    "\t\tt1.EMPLOYER_changemonth,\n",
    "\t\tt1.EMPLOYER,\n",
    "\t\tt1.EMPLOYER_ADDRESS,\n",
    "\t\tt1.EMPLOYER_province,\n",
    "\t\tt1.EMPLOYER_city,\n",
    "\t\tt1.OCCUPATION,\n",
    "\t\tt1.INDUSTRY,\n",
    "\t\tt1.DUTY,\n",
    "\t\tt1.TITLE,\n",
    "\t\tt1.work_year,\n",
    "\t\tt1.ANNOUNCECOUNT,\n",
    "\t\tt1.DISSENTCOUNT,\n",
    "\t\tnvl(t2.baddebt_count,0) as baddebt_count,\n",
    "\t\tnvl(t2.baddebt_amount,0) as baddebt_amount,nvl(t2.asset_num,0) as asset_num,\n",
    "\t\tnvl(t2.asset_amount,0) as asset_amount,nvl(t2.guarantor_num,0) as guarantor_num,\n",
    "\t\tnvl(t2.guarantor_amount,0) as guarantor_amount,\n",
    "\t\tnvl(t2.unnormal_cardguarante_num,0) as unnormal_cardguarante_num,\n",
    "\t\tnvl(t2.unnormal_cardguarante_amount,0) as unnormal_cardguarante_amount,\n",
    "\t\tnvl(t2.guarante_num,0) as guarante_num,nvl(t2.guarante_amount,0) as guarante_amount,\n",
    "\t\tnvl(t2.guarante_balance,0) as guarante_balance,\n",
    "\t\tt3.SUITOBJECT_num,\n",
    "\t\tt3.SUITOBJECTMONEY_amount,\n",
    "\t\tt3.already_enforce_object_num,\n",
    "\t\tt3.enforce_object_amount,\n",
    "\t\tt3.already_enforce_object_amount,\n",
    "\t\tt3.punish_num,\n",
    "        t3.punish_money,\n",
    "        t3.tax_num,\n",
    "        t3.tax_amount,\n",
    "        t3.accfund_state,\n",
    "        t3.accfund_pay,\n",
    "        t3.accfund_ownpercent,\n",
    "        t3.accfund_cimpercent,\n",
    "        t3.ownbasicmoney,\n",
    "        t4.max_gap_months,\n",
    "        t4.min_gap_months,\n",
    "        t4.mth1_gap_months,\n",
    "        t4.mth2_gap_months,\n",
    "        t4.mth3_gap_months,\n",
    "        t4.mth4_gap_months,\n",
    "        t4.mth5_gap_months,\n",
    "        t4.max_query_mth,\n",
    "        t4.max_query_day,\n",
    "        t4.out_loan_num,\n",
    "        t5.mth1_card_query_count,\n",
    "        t5.mth1_card_query_org,\n",
    "        t5.mth1_loan_query_count,\n",
    "        t5.mth1_loan_query_org,\n",
    "        t5.mth1_person_query_count,\n",
    "        t5.mth1_loan_card_query_org,\n",
    "        t5.mth3_card_query_count,\n",
    "        t5.mth3_card_query_org,\n",
    "        t5.mth3_loan_query_count,\n",
    "        t5.mth3_loan_query_org,\n",
    "        t5.mth3_person_query_count,\n",
    "        t5.mth3_loan_card_query_org,\n",
    "        t5.mth6_card_query_count,\n",
    "        t5.mth6_card_query_org,\n",
    "        t5.mth6_loan_query_count,\n",
    "        t5.mth6_loan_query_org,\n",
    "        t5.mth6_person_query_count,\n",
    "        t5.mth6_loan_card_query_org,\n",
    "        t5.mth9_card_query_count,\n",
    "        t5.mth9_card_query_org,\n",
    "        t5.mth9_loan_query_count,\n",
    "        t5.mth9_loan_query_org,\n",
    "        t5.mth9_person_query_count,\n",
    "        t5.mth9_loan_card_query_org,\n",
    "        t5.mth12_card_query_count,\n",
    "        t5.mth12_card_query_org,\n",
    "        t5.mth12_loan_query_count,\n",
    "        t5.mth12_loan_query_org,\n",
    "        t5.mth12_person_query_count,\n",
    "        t5.mth12_loan_card_query_org,\n",
    "\tnow() as create_time,now() as update_time,now() as etl_date\n",
    "\n",
    "from dw_baseinfo_detail as t1\n",
    "left join  dw_other_creditcue t2 on t1.reportno=t2.reportno\n",
    "left join  dw_other_info  t3 on t1.reportno=t3.reportno\n",
    "left join  dw_other_info2 t4 on t1.reportno=t4.reportno\n",
    "left join  dw_query_summary t5 on t1.reportno=t5.reportno\n",
    "\n",
    "\n",
    "\n",
    "\"\"\").createOrReplaceTempView(\"dm_general_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o78.sql.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:211)\n\tat org.apache.spark.sql.hive.execution.InsertIntoHiveTable.run(InsertIntoHiveTable.scala:337)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:66)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:61)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:84)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.hive.execution.CreateHiveTableAsSelectCommand.run(CreateHiveTableAsSelectCommand.scala:81)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:64)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:61)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:77)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:183)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:183)\n\tat org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:2841)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2840)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:183)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:68)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:632)\n\tat sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(reportno#3163, 500)\n+- *Project [REPORTNO#3163]\n   +- BroadcastNestedLoopJoin BuildLeft, LeftAnti, ((REPORTNO#3163 = reportno#2206) || isnull((REPORTNO#3163 = reportno#2206)))\n      :- BroadcastExchange IdentityBroadcastMode\n      :  +- BroadcastNestedLoopJoin BuildLeft, LeftAnti, ((REPORTNO#3163 = reportno#2230) || isnull((REPORTNO#3163 = reportno#2230)))\n      :     :- BroadcastExchange IdentityBroadcastMode\n      :     :  +- BroadcastNestedLoopJoin BuildLeft, LeftAnti, ((REPORTNO#3163 = reportno#2206) || isnull((REPORTNO#3163 = reportno#2206)))\n      :     :     :- BroadcastExchange IdentityBroadcastMode\n      :     :     :  +- *Scan JDBCRelation(hyxf.ICRMESSAGEHEADER) [numPartitions=1] [REPORTNO#3163,QUERYTIME#3164,REPORTCREATETIME#3165] ReadSchema: struct<REPORTNO:string,QUERYTIME:string,REPORTCREATETIME:string>\n      :     :     +- *HashAggregate(keys=[reportno#2206], functions=[], output=[reportno#2206])\n      :     :        +- Exchange hashpartitioning(reportno#2206, 500)\n      :     :           +- *HashAggregate(keys=[reportno#2206], functions=[], output=[reportno#2206])\n      :     :              +- *HashAggregate(keys=[reportno#2206, ACCOUNT#2207, QUERYTIME#2083, MONTH#2208, gap_months#2787, LASTMONTHS#2788, GUARANTEETYPE#2137, STATE#2138, FINANCEORG#2133], functions=[], output=[reportno#2206])\n      :     :                 +- Exchange hashpartitioning(reportno#2206, ACCOUNT#2207, QUERYTIME#2083, MONTH#2208, gap_months#2787, LASTMONTHS#2788, GUARANTEETYPE#2137, STATE#2138, FINANCEORG#2133, 500)\n      :     :                    +- *HashAggregate(keys=[reportno#2206, ACCOUNT#2207, QUERYTIME#2083, MONTH#2208, gap_months#2787, LASTMONTHS#2788, GUARANTEETYPE#2137, STATE#2138, FINANCEORG#2133], functions=[], output=[reportno#2206, ACCOUNT#2207, QUERYTIME#2083, MONTH#2208, gap_months#2787, LASTMONTHS#2788, GUARANTEETYPE#2137, STATE#2138, FINANCEORG#2133])\n      :     :                       +- Union\n      :     :                          :- *Project [reportno#2206, ACCOUNT#2207, QUERYTIME#2083, MONTH#2208, (month(cast(date_format(cast(QUERYTIME#2083 as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date)) - month(cast(date_format(cast(concat(month#2208, .01) as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date))) AS gap_months#2787, CASE WHEN LASTMONTHS#2209 IN (G,Z,D) THEN 8 ELSE LASTMONTHS#2209 END AS LASTMONTHS#2788, GUARANTEETYPE#2137, STATE#2138, FINANCEORG#2133]\n      :     :                          :  +- SortMergeJoin [reportno#2206], [reportno#2082], LeftOuter\n      :     :                          :     :- *Sort [reportno#2206 ASC NULLS FIRST], false, 0\n      :     :                          :     :  +- Exchange hashpartitioning(reportno#2206, 500)\n      :     :                          :     :     +- *Project [reportno#2206, account#2207, month#2208, lastmonths#2209, FINANCEORG#2133, GUARANTEETYPE#2137, STATE#2138]\n      :     :                          :     :        +- *SortMergeJoin [reportno#2206, ACCOUNT#2207], [reportno#2128, ACCOUNT#2131], Inner\n      :     :                          :     :           :- *Sort [reportno#2206 ASC NULLS FIRST, ACCOUNT#2207 ASC NULLS FIRST], false, 0\n      :     :                          :     :           :  +- Exchange hashpartitioning(reportno#2206, ACCOUNT#2207, 500)\n      :     :                          :     :           :     +- *Scan JDBCRelation(hyxf.ICRLATEST2YEAROVERDUECARD) [numPartitions=1] [reportno#2206,account#2207,month#2208,lastmonths#2209] PushedFilters: [*IsNotNull(lastmonths), *Not(EqualTo(lastmonths,C)), *IsNotNull(account)], ReadSchema: struct<reportno:string,account:string,month:string,lastmonths:string>\n      :     :                          :     :           +- *Sort [reportno#2128 ASC NULLS FIRST, ACCOUNT#2131 ASC NULLS FIRST], false, 0\n      :     :                          :     :              +- Exchange hashpartitioning(reportno#2128, ACCOUNT#2131, 500)\n      :     :                          :     :                 +- *Project [REPORTNO#2128, ACCOUNT#2131, FINANCEORG#2133, GUARANTEETYPE#2137, STATE#2138]\n      :     :                          :     :                    +- *Scan JDBCRelation(hyxf.ICRLOANCARDINFO) [numPartitions=1] [REPORTNO#2128,GUARANTEETYPE#2137,ACCOUNT#2131,FINANCEORG#2133,STATE#2138] PushedFilters: [*IsNotNull(BIZTYPE), *EqualTo(BIZTYPE,贷记卡)], ReadSchema: struct<REPORTNO:string,ACCOUNT:string,FINANCEORG:string,GUARANTEETYPE:string,STATE:string>\n      :     :                          :     +- *Sort [reportno#2082 ASC NULLS FIRST], false, 0\n      :     :                          :        +- ReusedExchange [REPORTNO#2082, QUERYTIME#2083], Exchange hashpartitioning(REPORTNO#3604, 500)\n      :     :                          +- *Project [reportno#2216, ACCOUNT#2217, QUERYTIME#2083, date_format(cast(concat(month#2219, .01) as timestamp), %Y-%m, Some(Asia/Shanghai)) AS date_format(CAST(concat(month, .01) AS TIMESTAMP), %Y-%m)#2813, (month(cast(date_format(cast(QUERYTIME#2083 as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date)) - month(cast(date_format(cast(concat(month#2219, .01) as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date))) AS gap_months#2800, cast(LASTMONTHS#2220 as string) AS LASTMONTHS#2814, GUARANTEETYPE#2137, STATE#2138, FINANCEORG#2133]\n      :     :                             +- SortMergeJoin [reportno#2216], [reportno#2082], LeftOuter\n      :     :                                :- *Sort [reportno#2216 ASC NULLS FIRST], false, 0\n      :     :                                :  +- Exchange hashpartitioning(reportno#2216, 500)\n      :     :                                :     +- *Project [REPORTNO#2216, ACCOUNT#2217, MONTH#2219, LASTMONTHS#2220, FINANCEORG#2133, GUARANTEETYPE#2137, STATE#2138]\n      :     :                                :        +- *SortMergeJoin [reportno#2216, ACCOUNT#2217], [reportno#2128, ACCOUNT#2131], Inner\n      :     :                                :           :- *Sort [reportno#2216 ASC NULLS FIRST, ACCOUNT#2217 ASC NULLS FIRST], false, 0\n      :     :                                :           :  +- Exchange hashpartitioning(reportno#2216, ACCOUNT#2217, 500)\n      :     :                                :           :     +- *Scan JDBCRelation(hyxf.ICRLATEST5YEAROVERDUEDETAIL) [numPartitions=1] [REPORTNO#2216,ACCOUNT#2217,MONTH#2219,LASTMONTHS#2220] PushedFilters: [*IsNotNull(MONTH), *Not(EqualTo(MONTH,--)), *IsNotNull(REPORTNO), *IsNotNull(ACCOUNT)], ReadSchema: struct<REPORTNO:string,ACCOUNT:string,MONTH:string,LASTMONTHS:decimal(20,0)>\n      :     :                                :           +- *Sort [reportno#2128 ASC NULLS FIRST, ACCOUNT#2131 ASC NULLS FIRST], false, 0\n      :     :                                :              +- ReusedExchange [REPORTNO#2128, ACCOUNT#2131, FINANCEORG#2133, GUARANTEETYPE#2137, STATE#2138], Exchange hashpartitioning(reportno#2128, ACCOUNT#2131, 500)\n      :     :                                +- *Sort [reportno#2082 ASC NULLS FIRST], false, 0\n      :     :                                   +- ReusedExchange [REPORTNO#2082, QUERYTIME#2083], Exchange hashpartitioning(REPORTNO#3604, 500)\n      :     +- *HashAggregate(keys=[reportno#2230], functions=[], output=[reportno#2230])\n      :        +- Exchange hashpartitioning(reportno#2230, 500)\n      :           +- *HashAggregate(keys=[reportno#2230], functions=[], output=[reportno#2230])\n      :              +- *HashAggregate(keys=[reportno#2230, ACCOUNT#2231, QUERYTIME#2083, MONTH#2232, gap_months#2826, LASTMONTHS#2827, GUARANTEETYPE#2263, STATE#2266, FINANCEORG#2257], functions=[], output=[reportno#2230])\n      :                 +- Exchange hashpartitioning(reportno#2230, ACCOUNT#2231, QUERYTIME#2083, MONTH#2232, gap_months#2826, LASTMONTHS#2827, GUARANTEETYPE#2263, STATE#2266, FINANCEORG#2257, 500)\n      :                    +- *HashAggregate(keys=[reportno#2230, ACCOUNT#2231, QUERYTIME#2083, MONTH#2232, gap_months#2826, LASTMONTHS#2827, GUARANTEETYPE#2263, STATE#2266, FINANCEORG#2257], functions=[], output=[reportno#2230, ACCOUNT#2231, QUERYTIME#2083, MONTH#2232, gap_months#2826, LASTMONTHS#2827, GUARANTEETYPE#2263, STATE#2266, FINANCEORG#2257])\n      :                       +- Union\n      :                          :- *Project [reportno#2230, ACCOUNT#2231, QUERYTIME#2083, MONTH#2232, (month(cast(date_format(cast(QUERYTIME#2083 as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date)) - month(cast(date_format(cast(concat(month#2232, .01) as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date))) AS gap_months#2826, CASE WHEN LASTMONTHS#2233 IN (G,Z,D) THEN 8 ELSE LASTMONTHS#2233 END AS LASTMONTHS#2827, GUARANTEETYPE#2263, STATE#2266, FINANCEORG#2257]\n      :                          :  +- SortMergeJoin [reportno#2230], [reportno#2082], LeftOuter\n      :                          :     :- *Sort [reportno#2230 ASC NULLS FIRST], false, 0\n      :                          :     :  +- Exchange hashpartitioning(reportno#2230, 500)\n      :                          :     :     +- *Project [reportno#2230, account#2231, month#2232, lastmonths#2233, FINANCEORG#2257, GUARANTEETYPE#2263, STATE#2266]\n      :                          :     :        +- *SortMergeJoin [reportno#2230, ACCOUNT#2231], [reportno#2253, ACCOUNT#2258], Inner\n      :                          :     :           :- *Sort [reportno#2230 ASC NULLS FIRST, ACCOUNT#2231 ASC NULLS FIRST], false, 0\n      :                          :     :           :  +- Exchange hashpartitioning(reportno#2230, ACCOUNT#2231, 500)\n      :                          :     :           :     +- *Scan JDBCRelation(hyxf.ICRLATEST2YEAROVERDUE) [numPartitions=1] [reportno#2230,account#2231,month#2232,lastmonths#2233] PushedFilters: [*IsNotNull(lastmonths), *Not(EqualTo(lastmonths,C)), *IsNotNull(account)], ReadSchema: struct<reportno:string,account:string,month:string,lastmonths:string>\n      :                          :     :           +- *Sort [reportno#2253 ASC NULLS FIRST, ACCOUNT#2258 ASC NULLS FIRST], false, 0\n      :                          :     :              +- Exchange hashpartitioning(reportno#2253, ACCOUNT#2258, 500)\n      :                          :     :                 +- *Scan JDBCRelation(hyxf.ICRLOANINFO) [numPartitions=1] [REPORTNO#2253,FINANCEORG#2257,ACCOUNT#2258,GUARANTEETYPE#2263,STATE#2266] ReadSchema: struct<REPORTNO:string,FINANCEORG:string,ACCOUNT:string,GUARANTEETYPE:string,STATE:string>\n      :                          :     +- *Sort [reportno#2082 ASC NULLS FIRST], false, 0\n      :                          :        +- ReusedExchange [REPORTNO#2082, QUERYTIME#2083], Exchange hashpartitioning(REPORTNO#3604, 500)\n      :                          +- *Project [reportno#2216, ACCOUNT#2217, QUERYTIME#2083, date_format(cast(concat(month#2219, .01) as timestamp), %Y-%m, Some(Asia/Shanghai)) AS date_format(CAST(concat(month, .01) AS TIMESTAMP), %Y-%m)#2852, (month(cast(date_format(cast(QUERYTIME#2083 as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date)) - month(cast(date_format(cast(concat(month#2219, .01) as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date))) AS gap_months#2839, cast(LASTMONTHS#2220 as string) AS LASTMONTHS#2853, GUARANTEETYPE#2263, STATE#2266, FINANCEORG#2257]\n      :                             +- SortMergeJoin [reportno#2216], [reportno#2082], LeftOuter\n      :                                :- *Sort [reportno#2216 ASC NULLS FIRST], false, 0\n      :                                :  +- Exchange hashpartitioning(reportno#2216, 500)\n      :                                :     +- *Project [REPORTNO#2216, ACCOUNT#2217, MONTH#2219, LASTMONTHS#2220, FINANCEORG#2257, GUARANTEETYPE#2263, STATE#2266]\n      :                                :        +- *SortMergeJoin [reportno#2216, ACCOUNT#2217], [reportno#2253, ACCOUNT#2258], Inner\n      :                                :           :- *Sort [reportno#2216 ASC NULLS FIRST, ACCOUNT#2217 ASC NULLS FIRST], false, 0\n      :                                :           :  +- ReusedExchange [REPORTNO#2216, ACCOUNT#2217, MONTH#2219, LASTMONTHS#2220], Exchange hashpartitioning(reportno#2216, ACCOUNT#2217, 500)\n      :                                :           +- *Sort [reportno#2253 ASC NULLS FIRST, ACCOUNT#2258 ASC NULLS FIRST], false, 0\n      :                                :              +- ReusedExchange [REPORTNO#2253, FINANCEORG#2257, ACCOUNT#2258, GUARANTEETYPE#2263, STATE#2266], Exchange hashpartitioning(reportno#2253, ACCOUNT#2258, 500)\n      :                                +- *Sort [reportno#2082 ASC NULLS FIRST], false, 0\n      :                                   +- ReusedExchange [REPORTNO#2082, QUERYTIME#2083], Exchange hashpartitioning(REPORTNO#3604, 500)\n      +- *HashAggregate(keys=[reportno#2206], functions=[], output=[reportno#2206])\n         +- *HashAggregate(keys=[reportno#2206], functions=[], output=[reportno#2206])\n            +- *Project [reportno#2206]\n               +- SortMergeJoin [reportno#2206], [reportno#2082], LeftOuter\n                  :- *Sort [reportno#2206 ASC NULLS FIRST], false, 0\n                  :  +- Exchange hashpartitioning(reportno#2206, 500)\n                  :     +- *Project [reportno#2206]\n                  :        +- *SortMergeJoin [reportno#2206, ACCOUNT#2207], [reportno#2128, ACCOUNT#2131], Inner\n                  :           :- *Sort [reportno#2206 ASC NULLS FIRST, ACCOUNT#2207 ASC NULLS FIRST], false, 0\n                  :           :  +- Exchange hashpartitioning(reportno#2206, ACCOUNT#2207, 500)\n                  :           :     +- *Project [reportno#2206, account#2207]\n                  :           :        +- *Scan JDBCRelation(hyxf.ICRLATEST2YEAROVERDUECARD) [numPartitions=1] [reportno#2206,account#2207] PushedFilters: [*IsNotNull(lastmonths), *Not(EqualTo(lastmonths,C)), *IsNotNull(account)], ReadSchema: struct<reportno:string,account:string>\n                  :           +- *Sort [reportno#2128 ASC NULLS FIRST, ACCOUNT#2131 ASC NULLS FIRST], false, 0\n                  :              +- Exchange hashpartitioning(reportno#2128, ACCOUNT#2131, 500)\n                  :                 +- *Project [REPORTNO#2128, ACCOUNT#2131]\n                  :                    +- *Scan JDBCRelation(hyxf.ICRLOANCARDINFO) [numPartitions=1] [REPORTNO#2128,ACCOUNT#2131] PushedFilters: [*IsNotNull(BIZTYPE), *EqualTo(BIZTYPE,准贷记卡)], ReadSchema: struct<REPORTNO:string,ACCOUNT:string>\n                  +- *Sort [reportno#2082 ASC NULLS FIRST], false, 0\n                     +- ReusedExchange [REPORTNO#2082], Exchange hashpartitioning(REPORTNO#3601, 500)\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.ProjectExec.doExecute(basicPhysicalOperators.scala:73)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:173)\n\t... 34 more\nCaused by: java.util.concurrent.TimeoutException: Futures timed out after [300 seconds]\n\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:123)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:126)\n\tat org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec.doExecute(BroadcastNestedLoopJoinExec.scala:343)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:228)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:76)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:73)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:97)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d8951934df25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drop table if EXISTS renhang_user_profile.%s \"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"create table renhang_user_profile.%s as select * from %s\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.0.1-1.cdh6.0.1.p0.590678/lib/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \"\"\"\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.0.1-1.cdh6.0.1.p0.590678/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.0.1-1.cdh6.0.1.p0.590678/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.0.1-1.cdh6.0.1.p0.590678/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o78.sql.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:211)\n\tat org.apache.spark.sql.hive.execution.InsertIntoHiveTable.run(InsertIntoHiveTable.scala:337)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:66)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:61)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:84)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.hive.execution.CreateHiveTableAsSelectCommand.run(CreateHiveTableAsSelectCommand.scala:81)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:64)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:61)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:77)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:183)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:183)\n\tat org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:2841)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2840)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:183)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:68)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:632)\n\tat sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(reportno#3163, 500)\n+- *Project [REPORTNO#3163]\n   +- BroadcastNestedLoopJoin BuildLeft, LeftAnti, ((REPORTNO#3163 = reportno#2206) || isnull((REPORTNO#3163 = reportno#2206)))\n      :- BroadcastExchange IdentityBroadcastMode\n      :  +- BroadcastNestedLoopJoin BuildLeft, LeftAnti, ((REPORTNO#3163 = reportno#2230) || isnull((REPORTNO#3163 = reportno#2230)))\n      :     :- BroadcastExchange IdentityBroadcastMode\n      :     :  +- BroadcastNestedLoopJoin BuildLeft, LeftAnti, ((REPORTNO#3163 = reportno#2206) || isnull((REPORTNO#3163 = reportno#2206)))\n      :     :     :- BroadcastExchange IdentityBroadcastMode\n      :     :     :  +- *Scan JDBCRelation(hyxf.ICRMESSAGEHEADER) [numPartitions=1] [REPORTNO#3163,QUERYTIME#3164,REPORTCREATETIME#3165] ReadSchema: struct<REPORTNO:string,QUERYTIME:string,REPORTCREATETIME:string>\n      :     :     +- *HashAggregate(keys=[reportno#2206], functions=[], output=[reportno#2206])\n      :     :        +- Exchange hashpartitioning(reportno#2206, 500)\n      :     :           +- *HashAggregate(keys=[reportno#2206], functions=[], output=[reportno#2206])\n      :     :              +- *HashAggregate(keys=[reportno#2206, ACCOUNT#2207, QUERYTIME#2083, MONTH#2208, gap_months#2787, LASTMONTHS#2788, GUARANTEETYPE#2137, STATE#2138, FINANCEORG#2133], functions=[], output=[reportno#2206])\n      :     :                 +- Exchange hashpartitioning(reportno#2206, ACCOUNT#2207, QUERYTIME#2083, MONTH#2208, gap_months#2787, LASTMONTHS#2788, GUARANTEETYPE#2137, STATE#2138, FINANCEORG#2133, 500)\n      :     :                    +- *HashAggregate(keys=[reportno#2206, ACCOUNT#2207, QUERYTIME#2083, MONTH#2208, gap_months#2787, LASTMONTHS#2788, GUARANTEETYPE#2137, STATE#2138, FINANCEORG#2133], functions=[], output=[reportno#2206, ACCOUNT#2207, QUERYTIME#2083, MONTH#2208, gap_months#2787, LASTMONTHS#2788, GUARANTEETYPE#2137, STATE#2138, FINANCEORG#2133])\n      :     :                       +- Union\n      :     :                          :- *Project [reportno#2206, ACCOUNT#2207, QUERYTIME#2083, MONTH#2208, (month(cast(date_format(cast(QUERYTIME#2083 as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date)) - month(cast(date_format(cast(concat(month#2208, .01) as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date))) AS gap_months#2787, CASE WHEN LASTMONTHS#2209 IN (G,Z,D) THEN 8 ELSE LASTMONTHS#2209 END AS LASTMONTHS#2788, GUARANTEETYPE#2137, STATE#2138, FINANCEORG#2133]\n      :     :                          :  +- SortMergeJoin [reportno#2206], [reportno#2082], LeftOuter\n      :     :                          :     :- *Sort [reportno#2206 ASC NULLS FIRST], false, 0\n      :     :                          :     :  +- Exchange hashpartitioning(reportno#2206, 500)\n      :     :                          :     :     +- *Project [reportno#2206, account#2207, month#2208, lastmonths#2209, FINANCEORG#2133, GUARANTEETYPE#2137, STATE#2138]\n      :     :                          :     :        +- *SortMergeJoin [reportno#2206, ACCOUNT#2207], [reportno#2128, ACCOUNT#2131], Inner\n      :     :                          :     :           :- *Sort [reportno#2206 ASC NULLS FIRST, ACCOUNT#2207 ASC NULLS FIRST], false, 0\n      :     :                          :     :           :  +- Exchange hashpartitioning(reportno#2206, ACCOUNT#2207, 500)\n      :     :                          :     :           :     +- *Scan JDBCRelation(hyxf.ICRLATEST2YEAROVERDUECARD) [numPartitions=1] [reportno#2206,account#2207,month#2208,lastmonths#2209] PushedFilters: [*IsNotNull(lastmonths), *Not(EqualTo(lastmonths,C)), *IsNotNull(account)], ReadSchema: struct<reportno:string,account:string,month:string,lastmonths:string>\n      :     :                          :     :           +- *Sort [reportno#2128 ASC NULLS FIRST, ACCOUNT#2131 ASC NULLS FIRST], false, 0\n      :     :                          :     :              +- Exchange hashpartitioning(reportno#2128, ACCOUNT#2131, 500)\n      :     :                          :     :                 +- *Project [REPORTNO#2128, ACCOUNT#2131, FINANCEORG#2133, GUARANTEETYPE#2137, STATE#2138]\n      :     :                          :     :                    +- *Scan JDBCRelation(hyxf.ICRLOANCARDINFO) [numPartitions=1] [REPORTNO#2128,GUARANTEETYPE#2137,ACCOUNT#2131,FINANCEORG#2133,STATE#2138] PushedFilters: [*IsNotNull(BIZTYPE), *EqualTo(BIZTYPE,贷记卡)], ReadSchema: struct<REPORTNO:string,ACCOUNT:string,FINANCEORG:string,GUARANTEETYPE:string,STATE:string>\n      :     :                          :     +- *Sort [reportno#2082 ASC NULLS FIRST], false, 0\n      :     :                          :        +- ReusedExchange [REPORTNO#2082, QUERYTIME#2083], Exchange hashpartitioning(REPORTNO#3604, 500)\n      :     :                          +- *Project [reportno#2216, ACCOUNT#2217, QUERYTIME#2083, date_format(cast(concat(month#2219, .01) as timestamp), %Y-%m, Some(Asia/Shanghai)) AS date_format(CAST(concat(month, .01) AS TIMESTAMP), %Y-%m)#2813, (month(cast(date_format(cast(QUERYTIME#2083 as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date)) - month(cast(date_format(cast(concat(month#2219, .01) as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date))) AS gap_months#2800, cast(LASTMONTHS#2220 as string) AS LASTMONTHS#2814, GUARANTEETYPE#2137, STATE#2138, FINANCEORG#2133]\n      :     :                             +- SortMergeJoin [reportno#2216], [reportno#2082], LeftOuter\n      :     :                                :- *Sort [reportno#2216 ASC NULLS FIRST], false, 0\n      :     :                                :  +- Exchange hashpartitioning(reportno#2216, 500)\n      :     :                                :     +- *Project [REPORTNO#2216, ACCOUNT#2217, MONTH#2219, LASTMONTHS#2220, FINANCEORG#2133, GUARANTEETYPE#2137, STATE#2138]\n      :     :                                :        +- *SortMergeJoin [reportno#2216, ACCOUNT#2217], [reportno#2128, ACCOUNT#2131], Inner\n      :     :                                :           :- *Sort [reportno#2216 ASC NULLS FIRST, ACCOUNT#2217 ASC NULLS FIRST], false, 0\n      :     :                                :           :  +- Exchange hashpartitioning(reportno#2216, ACCOUNT#2217, 500)\n      :     :                                :           :     +- *Scan JDBCRelation(hyxf.ICRLATEST5YEAROVERDUEDETAIL) [numPartitions=1] [REPORTNO#2216,ACCOUNT#2217,MONTH#2219,LASTMONTHS#2220] PushedFilters: [*IsNotNull(MONTH), *Not(EqualTo(MONTH,--)), *IsNotNull(REPORTNO), *IsNotNull(ACCOUNT)], ReadSchema: struct<REPORTNO:string,ACCOUNT:string,MONTH:string,LASTMONTHS:decimal(20,0)>\n      :     :                                :           +- *Sort [reportno#2128 ASC NULLS FIRST, ACCOUNT#2131 ASC NULLS FIRST], false, 0\n      :     :                                :              +- ReusedExchange [REPORTNO#2128, ACCOUNT#2131, FINANCEORG#2133, GUARANTEETYPE#2137, STATE#2138], Exchange hashpartitioning(reportno#2128, ACCOUNT#2131, 500)\n      :     :                                +- *Sort [reportno#2082 ASC NULLS FIRST], false, 0\n      :     :                                   +- ReusedExchange [REPORTNO#2082, QUERYTIME#2083], Exchange hashpartitioning(REPORTNO#3604, 500)\n      :     +- *HashAggregate(keys=[reportno#2230], functions=[], output=[reportno#2230])\n      :        +- Exchange hashpartitioning(reportno#2230, 500)\n      :           +- *HashAggregate(keys=[reportno#2230], functions=[], output=[reportno#2230])\n      :              +- *HashAggregate(keys=[reportno#2230, ACCOUNT#2231, QUERYTIME#2083, MONTH#2232, gap_months#2826, LASTMONTHS#2827, GUARANTEETYPE#2263, STATE#2266, FINANCEORG#2257], functions=[], output=[reportno#2230])\n      :                 +- Exchange hashpartitioning(reportno#2230, ACCOUNT#2231, QUERYTIME#2083, MONTH#2232, gap_months#2826, LASTMONTHS#2827, GUARANTEETYPE#2263, STATE#2266, FINANCEORG#2257, 500)\n      :                    +- *HashAggregate(keys=[reportno#2230, ACCOUNT#2231, QUERYTIME#2083, MONTH#2232, gap_months#2826, LASTMONTHS#2827, GUARANTEETYPE#2263, STATE#2266, FINANCEORG#2257], functions=[], output=[reportno#2230, ACCOUNT#2231, QUERYTIME#2083, MONTH#2232, gap_months#2826, LASTMONTHS#2827, GUARANTEETYPE#2263, STATE#2266, FINANCEORG#2257])\n      :                       +- Union\n      :                          :- *Project [reportno#2230, ACCOUNT#2231, QUERYTIME#2083, MONTH#2232, (month(cast(date_format(cast(QUERYTIME#2083 as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date)) - month(cast(date_format(cast(concat(month#2232, .01) as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date))) AS gap_months#2826, CASE WHEN LASTMONTHS#2233 IN (G,Z,D) THEN 8 ELSE LASTMONTHS#2233 END AS LASTMONTHS#2827, GUARANTEETYPE#2263, STATE#2266, FINANCEORG#2257]\n      :                          :  +- SortMergeJoin [reportno#2230], [reportno#2082], LeftOuter\n      :                          :     :- *Sort [reportno#2230 ASC NULLS FIRST], false, 0\n      :                          :     :  +- Exchange hashpartitioning(reportno#2230, 500)\n      :                          :     :     +- *Project [reportno#2230, account#2231, month#2232, lastmonths#2233, FINANCEORG#2257, GUARANTEETYPE#2263, STATE#2266]\n      :                          :     :        +- *SortMergeJoin [reportno#2230, ACCOUNT#2231], [reportno#2253, ACCOUNT#2258], Inner\n      :                          :     :           :- *Sort [reportno#2230 ASC NULLS FIRST, ACCOUNT#2231 ASC NULLS FIRST], false, 0\n      :                          :     :           :  +- Exchange hashpartitioning(reportno#2230, ACCOUNT#2231, 500)\n      :                          :     :           :     +- *Scan JDBCRelation(hyxf.ICRLATEST2YEAROVERDUE) [numPartitions=1] [reportno#2230,account#2231,month#2232,lastmonths#2233] PushedFilters: [*IsNotNull(lastmonths), *Not(EqualTo(lastmonths,C)), *IsNotNull(account)], ReadSchema: struct<reportno:string,account:string,month:string,lastmonths:string>\n      :                          :     :           +- *Sort [reportno#2253 ASC NULLS FIRST, ACCOUNT#2258 ASC NULLS FIRST], false, 0\n      :                          :     :              +- Exchange hashpartitioning(reportno#2253, ACCOUNT#2258, 500)\n      :                          :     :                 +- *Scan JDBCRelation(hyxf.ICRLOANINFO) [numPartitions=1] [REPORTNO#2253,FINANCEORG#2257,ACCOUNT#2258,GUARANTEETYPE#2263,STATE#2266] ReadSchema: struct<REPORTNO:string,FINANCEORG:string,ACCOUNT:string,GUARANTEETYPE:string,STATE:string>\n      :                          :     +- *Sort [reportno#2082 ASC NULLS FIRST], false, 0\n      :                          :        +- ReusedExchange [REPORTNO#2082, QUERYTIME#2083], Exchange hashpartitioning(REPORTNO#3604, 500)\n      :                          +- *Project [reportno#2216, ACCOUNT#2217, QUERYTIME#2083, date_format(cast(concat(month#2219, .01) as timestamp), %Y-%m, Some(Asia/Shanghai)) AS date_format(CAST(concat(month, .01) AS TIMESTAMP), %Y-%m)#2852, (month(cast(date_format(cast(QUERYTIME#2083 as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date)) - month(cast(date_format(cast(concat(month#2219, .01) as timestamp), %Y-%m-%d, Some(Asia/Shanghai)) as date))) AS gap_months#2839, cast(LASTMONTHS#2220 as string) AS LASTMONTHS#2853, GUARANTEETYPE#2263, STATE#2266, FINANCEORG#2257]\n      :                             +- SortMergeJoin [reportno#2216], [reportno#2082], LeftOuter\n      :                                :- *Sort [reportno#2216 ASC NULLS FIRST], false, 0\n      :                                :  +- Exchange hashpartitioning(reportno#2216, 500)\n      :                                :     +- *Project [REPORTNO#2216, ACCOUNT#2217, MONTH#2219, LASTMONTHS#2220, FINANCEORG#2257, GUARANTEETYPE#2263, STATE#2266]\n      :                                :        +- *SortMergeJoin [reportno#2216, ACCOUNT#2217], [reportno#2253, ACCOUNT#2258], Inner\n      :                                :           :- *Sort [reportno#2216 ASC NULLS FIRST, ACCOUNT#2217 ASC NULLS FIRST], false, 0\n      :                                :           :  +- ReusedExchange [REPORTNO#2216, ACCOUNT#2217, MONTH#2219, LASTMONTHS#2220], Exchange hashpartitioning(reportno#2216, ACCOUNT#2217, 500)\n      :                                :           +- *Sort [reportno#2253 ASC NULLS FIRST, ACCOUNT#2258 ASC NULLS FIRST], false, 0\n      :                                :              +- ReusedExchange [REPORTNO#2253, FINANCEORG#2257, ACCOUNT#2258, GUARANTEETYPE#2263, STATE#2266], Exchange hashpartitioning(reportno#2253, ACCOUNT#2258, 500)\n      :                                +- *Sort [reportno#2082 ASC NULLS FIRST], false, 0\n      :                                   +- ReusedExchange [REPORTNO#2082, QUERYTIME#2083], Exchange hashpartitioning(REPORTNO#3604, 500)\n      +- *HashAggregate(keys=[reportno#2206], functions=[], output=[reportno#2206])\n         +- *HashAggregate(keys=[reportno#2206], functions=[], output=[reportno#2206])\n            +- *Project [reportno#2206]\n               +- SortMergeJoin [reportno#2206], [reportno#2082], LeftOuter\n                  :- *Sort [reportno#2206 ASC NULLS FIRST], false, 0\n                  :  +- Exchange hashpartitioning(reportno#2206, 500)\n                  :     +- *Project [reportno#2206]\n                  :        +- *SortMergeJoin [reportno#2206, ACCOUNT#2207], [reportno#2128, ACCOUNT#2131], Inner\n                  :           :- *Sort [reportno#2206 ASC NULLS FIRST, ACCOUNT#2207 ASC NULLS FIRST], false, 0\n                  :           :  +- Exchange hashpartitioning(reportno#2206, ACCOUNT#2207, 500)\n                  :           :     +- *Project [reportno#2206, account#2207]\n                  :           :        +- *Scan JDBCRelation(hyxf.ICRLATEST2YEAROVERDUECARD) [numPartitions=1] [reportno#2206,account#2207] PushedFilters: [*IsNotNull(lastmonths), *Not(EqualTo(lastmonths,C)), *IsNotNull(account)], ReadSchema: struct<reportno:string,account:string>\n                  :           +- *Sort [reportno#2128 ASC NULLS FIRST, ACCOUNT#2131 ASC NULLS FIRST], false, 0\n                  :              +- Exchange hashpartitioning(reportno#2128, ACCOUNT#2131, 500)\n                  :                 +- *Project [REPORTNO#2128, ACCOUNT#2131]\n                  :                    +- *Scan JDBCRelation(hyxf.ICRLOANCARDINFO) [numPartitions=1] [REPORTNO#2128,ACCOUNT#2131] PushedFilters: [*IsNotNull(BIZTYPE), *EqualTo(BIZTYPE,准贷记卡)], ReadSchema: struct<REPORTNO:string,ACCOUNT:string>\n                  +- *Sort [reportno#2082 ASC NULLS FIRST], false, 0\n                     +- ReusedExchange [REPORTNO#2082], Exchange hashpartitioning(REPORTNO#3601, 500)\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:141)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.ProjectExec.doExecute(basicPhysicalOperators.scala:73)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:173)\n\t... 34 more\nCaused by: java.util.concurrent.TimeoutException: Futures timed out after [300 seconds]\n\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:123)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:126)\n\tat org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec.doExecute(BroadcastNestedLoopJoinExec.scala:343)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:228)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:76)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:73)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:97)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "table_name = \"dm_general_info\"\n",
    "\n",
    "spark.sql(\"drop table if EXISTS renhang_user_profile.%s \"%table_name)\n",
    "spark.sql(\"create table renhang_user_profile.%s as select * from %s\"%(table_name,table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
